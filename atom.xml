<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>isKage`Blog</title>
  
  <subtitle>Welcome to isKage Blog :)</subtitle>
  <link href="https://blog.iskage.online/atom.xml" rel="self"/>
  
  <link href="https://blog.iskage.online/"/>
  <updated>2025-02-10T08:43:53.724Z</updated>
  <id>https://blog.iskage.online/</id>
  
  <author>
    <name>isKage</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习框架 Pytorch 深入学习（3）：利用 torch.nn 便捷搭建神经网络</title>
    <link href="https://blog.iskage.online/posts/e6c3766c.html"/>
    <id>https://blog.iskage.online/posts/e6c3766c.html</id>
    <published>2025-02-10T08:32:00.000Z</published>
    <updated>2025-02-10T08:43:53.724Z</updated>
    
    <content type="html"><![CDATA[<p>基于<a href="https://book.douban.com/subject/27624483/">《深度学习框架 Pytorch 入门与实践》陈云</a></p><p>参考 <a href="https://github.com/chenyuntc/pytorch-book">Github 的 pytorch-book 项目</a></p><p>参考 <a href="https://github.com/zergtant/pytorch-handbook">GitHub 的 pytorch-handbook 项目</a></p><hr><p>本章主要讲解如何使用 Pytorch 实现深度学习/神经网络里的结构和功能，关注实践，理论较少。</p><p><code>nn</code> 模块是 Pytorch 提供的神经网络模块，可以快速便捷地搭建神经网络或神经网络里的各个层（layer）。</p><h1>1 利用 nn.Module 实现全连接层和多层感知机</h1><p>在实际应用中，我们往往继承类 <code>torch.nn.Module</code> ，然后便携自己的网络层。下面以实现全连接层作为简单引入。</p><h2 id="1-1-全连接层">1.1 全连接层</h2><p>全连接层可以简单理解为一个线性层，它接受输入的张量 <code>x.shape = (?, in_features)</code> 并返回结果 <code>y.shape = (?, out_features)</code> ，利用的就是简单的线性组合。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = W x + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mtext>in_features</mtext><mo>×</mo><mtext>out_features</mtext></mrow></msup></mrow><annotation encoding="application/x-tex">W \in \R^{\text{in\_features}\times\text{out\_features}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">in_features</span></span><span class="mbin mtight">×</span><span class="mord text mtight"><span class="mord mtight">out_features</span></span></span></span></span></span></span></span></span></span></span></span></span> 而 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mtext>out_features</mtext></msup></mrow><annotation encoding="application/x-tex">b \in \R^{\text{out\_features}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">out_features</span></span></span></span></span></span></span></span></span></span></span></span></span>。</p><blockquote><p>注意：此处的乘是类似矩阵乘法，而【不是逐元素相乘】</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; 定义线性层 Linear 用来计算 y = W x + b &quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Linear</span>(nn.Module):  <span class="comment"># 继承 nn.Module</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, out_features</span>):</span><br><span class="line">        <span class="comment"># in_features 输入的形状，out_features 输出的形状</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        <span class="comment"># nn.Parameter 指定需要网络学习的参数</span></span><br><span class="line">        <span class="variable language_">self</span>.W = nn.Parameter(torch.randn(in_features, out_features))</span><br><span class="line">        <span class="variable language_">self</span>.b = nn.Parameter(torch.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 计算 y = xW + b : 利用了广播机制，b 会复制成 y 一般大小，即 (out_features,)</span></span><br><span class="line">        y = x @ <span class="variable language_">self</span>.W + <span class="variable language_">self</span>.b  <span class="comment"># @ 代表矩阵乘法</span></span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><ul><li>需要使用 <code>super()</code> 方法调用父类的 <code>__init__()</code> 方法。或者直接使用 <code>nn.Module.__init__(self)</code></li><li>注意在定义自己的 <code>__init__()</code> 时，需要声明参数。例如这里的 <code>in_features</code> 和 <code>out_features</code></li><li><code>in_features</code> 和 <code>out_features</code> 指定输入输出的形状</li><li><code>nn.Parameter()</code> 指定网络需要学习的参数，用来告诉网络之后需要更新的对象</li><li>注意参数的形状，需要满足 <code>(?, in_features) @ (in_features, out_features) -&gt; (?, out_features)</code> 这类似矩阵乘法，不过此处是张量</li></ul><p>调用上述定义的线性层/全连接层，检查维度是否正确</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用上述定义的线性层/全连接层，检查维度</span></span><br><span class="line">linear_layer = Linear(in_features=<span class="number">4</span>, out_features=<span class="number">3</span>)</span><br><span class="line">inputs = torch.randn(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">outputs = linear_layer(inputs)</span><br><span class="line"><span class="built_in">print</span>(outputs.shape)</span><br><span class="line"><span class="comment"># torch.Size([2, 3]) : (2, 4) @ (4, 3) -&gt; (2, 3)</span></span><br></pre></td></tr></table></figure><p>使用 <code>.named_parameters()</code> 方法检查参数 <code>W, b</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> linear_layer.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;1. It is parameter: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(name))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;2.&quot;</span>, parameter)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;3. The shape is: &#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(parameter.shape))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上述检查参数的返回结果</span></span><br><span class="line">[Out]:  <span class="number">1.</span> It <span class="keyword">is</span> parameter: W</span><br><span class="line">        <span class="number">2.</span> Parameter containing:</span><br><span class="line">        tensor([[ <span class="number">1.1711</span>,  <span class="number">0.4335</span>, -<span class="number">1.7343</span>],</span><br><span class="line">                [-<span class="number">1.3360</span>,  <span class="number">0.8871</span>,  <span class="number">0.7680</span>],</span><br><span class="line">                [ <span class="number">0.0571</span>,  <span class="number">0.2240</span>,  <span class="number">0.5520</span>],</span><br><span class="line">                [-<span class="number">0.5788</span>,  <span class="number">0.0177</span>,  <span class="number">0.1318</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">        <span class="number">3.</span> The shape <span class="keyword">is</span>: torch.Size([<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">        <span class="number">1.</span> It <span class="keyword">is</span> parameter: b</span><br><span class="line">        <span class="number">2.</span> Parameter containing:</span><br><span class="line">        tensor([ <span class="number">1.0198</span>, -<span class="number">0.4468</span>,  <span class="number">0.4520</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">        <span class="number">3.</span> The shape <span class="keyword">is</span>: torch.Size([<span class="number">3</span>])</span><br></pre></td></tr></table></figure><h2 id="1-2-多层感知机">1.2 多层感知机</h2><p>由多个线性层/全连接层通过某些激活函数构成的网络，称为多层感知机。</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1739156093597.png" alt=""></p><p>根据上图的网络结构搭建多层感知机：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiPerceptron</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features, out_features</span>):</span><br><span class="line">        <span class="comment"># 新增参数：隐藏层神经元个数（形状）</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 直接使用之前定义的线性层/全连接层 Linear</span></span><br><span class="line">        <span class="variable language_">self</span>.layer1 = Linear(in_features, hidden_features) </span><br><span class="line">        <span class="variable language_">self</span>.layer2 = Linear(hidden_features, out_features)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.layer1(x)</span><br><span class="line">        x = torch.sigmoid(x)  <span class="comment"># 使用激活函数，增加非线性因素（此处是逐个元素计算）</span></span><br><span class="line">        y = <span class="variable language_">self</span>.layer2(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><ul><li>之前定义的层 Layer 可以在后续重复使用</li><li>注意传入参数，用以确认形状</li></ul><p>调用上述定义的多层感知机，检查维度是否正确</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查维度</span></span><br><span class="line">mlp = MultiPerceptron(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">inputs = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">outputs = mlp(inputs)</span><br><span class="line"><span class="built_in">print</span>(outputs.shape)</span><br><span class="line"><span class="comment"># torch.Size([2, 1]) ： (2, 3) @ (3, 4) @ (4, 1) -&gt; (2, 1)</span></span><br></pre></td></tr></table></figure><p>检查参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查参数</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> mlp.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># layer1.W torch.Size([3, 4])</span></span><br><span class="line"><span class="comment"># layer1.b torch.Size([4])</span></span><br><span class="line"><span class="comment"># layer2.W torch.Size([4, 1])</span></span><br><span class="line"><span class="comment"># layer2.b torch.Size([1])</span></span><br></pre></td></tr></table></figure><blockquote><p>【注意输入形状】输入的形状一般为 <code>(?, in_features)</code> 其中 <code>?</code> 一般为 <code>batch_size</code> 即样本集个数。</p><p>当输入单一数据时，即只输入一个样本时，需要扩展维度，利用<a href="https://blog.iskage.online/posts/652f5539.html#8-3-%E7%BB%B4%E5%BA%A6%E5%8E%8B%E7%BC%A9%E3%80%81%E6%89%A9%E5%B1%95%E3%80%81%E6%8B%BC%E6%8E%A5-squeeze-unsqueeze-cat">第一章</a>介绍的 <code>unsqueeze()</code> 函数。向前扩展一个维度 <code>tensor.unsqueeze(0)</code> ，例如：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># batch_size = 1</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>)</span><br><span class="line">x.unsqueeze_(<span class="number">0</span>)  <span class="comment"># 需要向前扩展 1 个维度 （`_` 表示 inplace 操作，直接替换 x）</span></span><br><span class="line">y = mlp(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)  <span class="comment"># 正确 torch.Size([1, 1])</span></span><br></pre></td></tr></table></figure><p>总结：Pytorch 的 nn 封装了非常多网络层，可以直接前往<a href="https://pytorch.org/docs/stable/nn.html">官方文档</a>查看。下面介绍常见的网络层。</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1739157306136.png" alt=""></p><h1>2 常见神经网络层</h1><h2 id="2-1-以图像处理为例">2.1 以图像处理为例</h2><p>图像相关层主要包括：卷积层 <code>Conv</code>、池化层 <code>Pool</code> 等。往往还有不同维度图像处理的分类，同时池化方法也有最大池化、均值池化等。</p><blockquote><p>建议先学习卷积的原理，参考课程</p><p>中文，更专业：b站 <a href="https://www.bilibili.com/video/BV1L64y1m7Nh/?share_source=copy_web&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2">【19 卷积层【动手学深度学习v2】】</a></p><p>中文，更易懂：b站 <a href="https://www.bilibili.com/video/BV1K7411W7So/?p=5&amp;share_source=copy_web&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2">【【子豪兄】精讲CS231N斯坦福计算机视觉公开课（2020最新）】</a></p><p>英文，更专业：<a href="https://cs231n.stanford.edu/">cs231n</a></p></blockquote><h3 id="2-1-1-卷积层">2.1.1 卷积层</h3><p>图像处理相关的神经网络层，最最重要的就是卷积层。以 <code>Conv2d</code> 为例，介绍里面的参数和使用方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)</span><br></pre></td></tr></table></figure><p><strong>参数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in_channels: 输入</span></span><br><span class="line">- in_channels (<span class="built_in">int</span>) – 图片的通道数，例如RGB图片就是 <span class="number">3</span> 通道，灰度图只有 <span class="number">1</span> 通道</span><br><span class="line"></span><br><span class="line"><span class="comment"># out_channels: 输出</span></span><br><span class="line">- out_channels (<span class="built_in">int</span>) – 输出结果的通道数</span><br><span class="line"></span><br><span class="line"><span class="comment"># kernel_size: 卷积核的大小</span></span><br><span class="line">- kernel_size (<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>) – 卷积核的大小，只需输入T 则会自动生成一个 (T, T, channels) 大小的卷积核</span><br><span class="line"></span><br><span class="line"><span class="comment"># stride: 步数</span></span><br><span class="line">- stride (<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>, optional) – 卷积核每次移动的步数，默认为 <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># padding: 填充层数</span></span><br><span class="line">- padding (<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>, optional) – 填充层数，用以维持图片大小的参数，默认为 <span class="number">0</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># padding_mode: 填充方式</span></span><br><span class="line">- padding_mode (string, optional) – 填充方式，一般默认即可，有 <span class="string">&#x27;zeros&#x27;</span>, <span class="string">&#x27;reflect&#x27;</span>, <span class="string">&#x27;replicate&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;circular&#x27;</span> 多种选择，默认为 <span class="string">&#x27;zeros&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dilation: 卷积核中元素的对应位置</span></span><br><span class="line">- dilation (<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>, optional) – 默认为 <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># groups: 可选</span></span><br><span class="line">- groups (<span class="built_in">int</span>, optional) – Number of blocked connections <span class="keyword">from</span> <span class="built_in">input</span> channels to output channels. Default: <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bias: 可选</span></span><br><span class="line">- bias (<span class="built_in">bool</span>, optional) – 是否增加偏倚项，默认为 <span class="literal">True</span> : If <span class="literal">True</span>, adds a learnable bias to the output. Default: <span class="literal">True</span></span><br></pre></td></tr></table></figure><blockquote><p>如果希望卷积后，通道变多，但尺寸不变，则需要填充 <code>padding</code> ，公式</p></blockquote><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88110036714.png" alt=""></p><p>卷积过程的动画展示可参考 <a href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a></p><p>原理简单理解【卷积】</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88101819072.png" alt=""></p><p>原理简单理解【padding】</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88102514146.png" alt=""></p><h3 id="2-1-2-代码：使用卷积层">2.1.2 代码：使用卷积层</h3><p>导入库，进行图片处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage</span><br><span class="line"></span><br><span class="line">to_tensor = ToTensor()  <span class="comment"># img -&gt; Tensor</span></span><br><span class="line">to_pil = ToPILImage()  <span class="comment"># Tensor -&gt; PIL</span></span><br></pre></td></tr></table></figure><p>选择一张图片（图源网络）点此下载 <a href="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/lena.png">lena’s photo</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">example = Image.<span class="built_in">open</span>(<span class="string">&#x27;imgs/lena.png&#x27;</span>)</span><br><span class="line">example <span class="comment"># 可视化输出</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/lena.png" alt="lena"></p><p>查看输入图片形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">example = to_tensor(example).unsqueeze(<span class="number">0</span>)  <span class="comment"># 补充 batch_size</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Input Size:&quot;</span>,example.size()) <span class="comment"># 查看 input 维度</span></span><br><span class="line"><span class="comment"># Input Size: torch.Size([1, 1, 200, 200])</span></span><br></pre></td></tr></table></figure><p>查看卷积后输出图片形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conv = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">out = conv(example)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output Size:&quot;</span>,out.size())</span><br><span class="line"><span class="comment"># Output Size: torch.Size([1, 1, 198, 198])</span></span><br><span class="line"><span class="comment"># 198 = (200 + 2 * 0 - 3 )/1 + 1 = 198</span></span><br></pre></td></tr></table></figure><p>以图片形式输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))  <span class="comment"># 去除 batch_size 转换为图片输出</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/lena_conv.png" alt="lena_conv"></p><blockquote><p>拓展：指定卷积核</p></blockquote><p>指定卷积核可以达到不同的效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拓展：指定卷积核</span></span><br><span class="line">kernel = torch.tensor([</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">0.</span>, -<span class="number">1.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">0.</span>, -<span class="number">1.</span>],</span><br><span class="line">    [<span class="number">1.</span>, <span class="number">0.</span>, -<span class="number">1.</span>]</span><br><span class="line">], dtype=torch.float32)  <span class="comment"># 提取竖直边缘特征</span></span><br><span class="line"></span><br><span class="line">conv = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">conv.weight.data = kernel.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)  <span class="comment"># (batch_size, in_channels, height, width)</span></span><br><span class="line"></span><br><span class="line">out = conv(example)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output Size:&quot;</span>, out.size())  <span class="comment"># torch.Size([1, 1, 198, 198])</span></span><br><span class="line"></span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))  <span class="comment"># 去除 batch_size 转换为图片输出</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/lena_kernel.png" alt="lena_conv_with_certain_kernel"></p><h3 id="2-1-3-池化层">2.1.3 池化层</h3><p>池化层模糊选取某些特征，某些意义上可以防止过拟合。以最大池化为例，他选取范围内最大值替换整个范围。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MaxPool2d(kernel_size, stride=<span class="literal">None</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=<span class="literal">False</span>, ceil_mode=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p><strong>参数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取最大值的窗口</span></span><br><span class="line">- kernel_size – the size of the window to take a <span class="built_in">max</span> over</span><br><span class="line"></span><br><span class="line"><span class="comment"># 横向纵向的步长，default = kernel_size</span></span><br><span class="line">- stride – the stride of the window. Default value <span class="keyword">is</span> kernel_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 补充图像边缘</span></span><br><span class="line">- padding – implicit zero padding to be added on both sides</span><br><span class="line"></span><br><span class="line"><span class="comment"># 空洞</span></span><br><span class="line">- dilation – a parameter that controls the stride of elements <span class="keyword">in</span> the window</span><br><span class="line"></span><br><span class="line">- return_indices – <span class="keyword">if</span> <span class="literal">True</span>, will <span class="keyword">return</span> the <span class="built_in">max</span> indices along <span class="keyword">with</span> the outputs. Useful <span class="keyword">for</span> torch.nn.MaxUnpool2d later</span><br><span class="line"></span><br><span class="line"><span class="comment"># floor向下取整 ceil向上取整，例如ceil_mode = True，保留超出部分</span></span><br><span class="line">- ceil_mode – when <span class="literal">True</span>, will use ceil instead of floor to compute the output shape</span><br></pre></td></tr></table></figure><p>结合下图例理解最大池化原理</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240804%E4%B8%8B%E5%8D%8832249522.png" alt=""></p><p>代码实现上述案例，进行验证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"></span><br><span class="line">inputs = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, ],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, ],</span><br><span class="line">], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 batch_size，1 通道，5x5 大小，-1 表示自动计算</span></span><br><span class="line">inputs = torch.reshape(inputs, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 神经网络</span></span><br><span class="line">max_pool = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">output = max_pool(inputs)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Out]: tensor([[[[<span class="number">3.</span>, <span class="number">2.</span>],</span><br><span class="line">                 [<span class="number">5.</span>, <span class="number">1.</span>]]]])  <span class="comment"># 确实与手算结果相同</span></span><br></pre></td></tr></table></figure><blockquote><p>根据池化原理，只是做了简单的取值替换，故【没有可学习的参数】</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>(max_pool.parameters())</span><br><span class="line">[Out]: []</span><br></pre></td></tr></table></figure><h3 id="2-1-4-代码：使用池化层">2.1.4 代码：使用池化层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out = max_pool(example)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>)) <span class="comment"># 输出池化后的lena</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/lena_pool.png" alt="lena_max_pool"></p><p>容易发现，经过最大池化后，图片变小，变模糊。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out.shape</span><br><span class="line"><span class="comment"># torch.Size([1, 1, 67, 67])</span></span><br></pre></td></tr></table></figure><h2 id="2-2-其他常见层">2.2 其他常见层</h2><h3 id="2-2-1-线性层-全连接层">2.2.1 线性层/全连接层</h3><p><code>nn.Linear</code> 层提供了类似计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = Wx+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span></span></span></span> 的功能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性层</span></span><br><span class="line">inputs = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">linear_out = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">out = linear_out(inputs)</span><br><span class="line">out.shape</span><br><span class="line"><span class="comment"># torch.Size([2, 4]) : (2, 3) @ (3, 4) -&gt; (2, 4) where 2 is batch_size</span></span><br></pre></td></tr></table></figure><p>更多可参见 <a href="https://blog.iskage.online/posts/ae1c954d.html#1-Linear">Pytorch 搭建神经网络（2）网络搭建 - 线性层</a></p><h3 id="2-2-2-批量归一化层">2.2.2 批量归一化层</h3><p><code>nn.BatchNorm1d</code> 层提供对 1 维数据进行归一化，填入的参数为特征数（例如上一个输出的维度）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn = nn.BatchNorm1d(<span class="number">512</span>)  <span class="comment"># 全连接层后接BatchNorm1d</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bn(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>更多可参见 <a href="https://blog.iskage.online/posts/ae1c954d.html#7-%E6%AD%A3%E5%88%99%E5%8C%96%E5%B1%82">Pytorch 搭建神经网络（2）网络搭建 - 正则化层</a></p><h3 id="2-2-3-Dropout-层">2.2.3 Dropout 层</h3><p><code>nn.Dropout</code> 层用于防止过拟合，按照概率遗弃一些神经元</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">784</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(<span class="number">0.5</span>)  <span class="comment"># 以 0.5 的概率遗弃</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="2-3-循环神经网络">2.3 循环神经网络</h2><p>PyTorch 中提供了最常用的三种循环神经网络：<code>RNN</code>、<code>LSTM</code> 和 <code>GRU</code> 。</p><p>推荐学习 <a href="https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/index.html">《动手学深度学习》</a> 中关于循环神经网络的知识，十分详细。也可结合李沐老师的讲解[b站连接](【54 循环神经网络 RNN【动手学深度学习v2】】 <a href="https://www.bilibili.com/video/BV1D64y1z7CA/?share_source=copy_web&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2">https://www.bilibili.com/video/BV1D64y1z7CA/?share_source=copy_web&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2</a>)</p><h2 id="3-激活函数">3 激活函数</h2><p>激活函数可以为模型加入非线性性。</p><p>这部分可以参见 <a href="https://blog.iskage.online/posts/ae1c954d.html#6-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%EF%BC%88%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%89">Pytorch 搭建神经网络（2）网络搭建 - 激活函数</a></p><h2 id="4-前馈传播网络的便捷构建">4 前馈传播网络的便捷构建</h2><p>上述的网络结构均为：前一层的输出是下一层的输入。这样的网络结构称为<strong>前馈传播网络</strong>（Feedforward Neural Network，FFN）。</p><p>针对这样的网络结构，可以使用 <code>ModuleList</code> 和 <code>Sequential</code> 来组合各个层。</p><h3 id="4-1-Sequential">4.1 Sequential</h3><p>使用 <code>Sequential</code> 的三种方法：将卷积层、归一化层和激活函数层组合成一个网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 法一</span></span><br><span class="line">net1 = nn.Sequential()</span><br><span class="line">net1.add_module(<span class="string">&#x27;conv&#x27;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">&#x27;batchnorm&#x27;</span>, nn.BatchNorm2d(<span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">&#x27;relu&#x27;</span>, nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;net1:&#x27;</span>, net1)</span><br><span class="line"><span class="comment"># net1: Sequential(</span></span><br><span class="line"><span class="comment">#   (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"><span class="comment">#   (relu): ReLU()</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 法二</span></span><br><span class="line">net2 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">3</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;net2:&#x27;</span>, net2)</span><br><span class="line"><span class="comment"># net2: Sequential(</span></span><br><span class="line"><span class="comment">#   (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"><span class="comment">#   (2): ReLU()</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 法三</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line">net3 = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;conv&#x27;</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)),</span><br><span class="line">    (<span class="string">&#x27;batchnorm&#x27;</span>, nn.BatchNorm2d(<span class="number">3</span>)),</span><br><span class="line">    (<span class="string">&#x27;relu&#x27;</span>, nn.ReLU())</span><br><span class="line">]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;net3:&#x27;</span>, net3)</span><br><span class="line"><span class="comment"># net3: Sequential(</span></span><br><span class="line"><span class="comment">#   (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"><span class="comment">#   (relu): ReLU()</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure><ul><li>可以根据名字和序号取出对应的层</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net1.conv</span><br><span class="line"><span class="comment"># Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line"></span><br><span class="line">net2[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span></span><br><span class="line"></span><br><span class="line">net3.relu</span><br><span class="line"><span class="comment"># ReLU()</span></span><br></pre></td></tr></table></figure><h3 id="4-2-ModuleList">4.2 ModuleList</h3><p>使用 <code>nn.ModuleList</code> 连接三个层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model_list = nn.ModuleList([nn.Linear(<span class="number">3</span>,<span class="number">4</span>), nn.ReLU(), nn.Linear(<span class="number">4</span>,<span class="number">2</span>)])</span><br><span class="line">inputs = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> model_list:</span><br><span class="line">    inputs = model(inputs)  <span class="comment"># 一步一步执行，相当于前向传播 forward</span></span><br><span class="line">inputs.shape</span><br><span class="line"><span class="comment"># torch.Size([1, 2])</span></span><br></pre></td></tr></table></figure><blockquote><p>不可以直接调用 <code>modellist(inputs)</code> ，因为没有定义前向传播</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">output = modellist(inputs)  <span class="comment"># 报错，没有定义 forward 函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># NotImplementedError: Module [ModuleList] is missing the required &quot;forward&quot; function</span></span><br></pre></td></tr></table></figure><blockquote><p>【不能直接使用 list 类型】必须使用 <code>nn.ModuleList</code> 连接各个层，直接使用 <code>list</code> 类型是无法继承 <code>nn.Module</code> 从而无法被识别</p></blockquote><h2 id="5-损失函数">5 损失函数</h2><p>Pytorch 提供简单计算损失的函数，例如均方误差、交叉熵损失等。</p><ul><li>均方误差损失 <code>nn.MSELoss()</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成预测值和真实值</span></span><br><span class="line">y_pred = torch.randn(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">y_real = torch.randn(<span class="number">4</span>).squeeze(-<span class="number">1</span>)  <span class="comment"># 将 y_real 的形状调整为 (4, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 MSE 损失函数</span></span><br><span class="line">mse = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = mse(y_pred, y_real)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss)  <span class="comment"># tensor(1.2719)</span></span><br></pre></td></tr></table></figure><ul><li>交叉熵损失 <code>nn.CrossEntropyLoss()</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># batch_size=4，即这一组共 4 个样本，类别为 2</span></span><br><span class="line">score = torch.randn(<span class="number">4</span>, <span class="number">2</span>)  <span class="comment"># 4 个样本，每个样本对应 2 个数值，代表属于第 0 or 1 类的概率</span></span><br><span class="line"><span class="comment"># 假设 4 个样本的真实类为：1, 0, 1, 1 </span></span><br><span class="line">label = torch.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]).long()  <span class="comment"># label 必须为 LongTensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵损失 CrossEntropyLoss （常用与计算分类问题的损失）</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">loss = criterion(score, label)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss)  <span class="comment"># tensor(0.5944)</span></span><br></pre></td></tr></table></figure><h2 id="6-nn-functional-模块">6 nn.functional 模块</h2><p>使用 <code>nn.Module</code> 实现的层是一个特殊的类，其由 <code>class layer(nn.Module)</code> 定义，会自动提取可学习的参数；使用<code>nn.functional</code>实现的层更像是纯函数，由<code>def function(input)</code>定义。</p><p>也就是说，当这一层无需学习参数时，使用 <code>nn.functional</code> 是合理的。</p><h3 id="6-1-使用-nn-functional-的函数">6.1 使用 nn.functional 的函数</h3><p>以 <code>nn.functional.linear()</code> 为例，其他函数可参考官网 <a href="https://pytorch.org/docs/stable/nn.functional.html">https://pytorch.org/docs/stable/nn.functional.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.functional.linear(<span class="built_in">input</span>, weight, bias=<span class="literal">None</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure><p><strong>参数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- <span class="built_in">input</span>: (batch_size, in_features)</span><br><span class="line">输入值，需要为 tensor</span><br><span class="line"></span><br><span class="line">- weight: (in_features, out_features)</span><br><span class="line">权重，需要为 tensor</span><br><span class="line"></span><br><span class="line">- bias: (out_features) <span class="keyword">or</span> <span class="literal">None</span></span><br><span class="line">偏倚，需要为 tensor，或者为空</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 使用 nn.Module</span></span><br><span class="line">model = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">output1 = model(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 使用 nn.functional</span></span><br><span class="line">output2 = nn.functional.linear(inputs, model.weight, model.bias)  <span class="comment"># 这里使用与 1 相同的参数</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output1)</span><br><span class="line"><span class="built_in">print</span>(output2)</span><br><span class="line"><span class="comment"># 二者值完全一样</span></span><br></pre></td></tr></table></figure><h3 id="6-2-nn-Module-和-nn-functional-结合使用">6.2 nn.Module 和 nn.functional 结合使用</h3><ul><li>如果模型具有可学习的参数，最好用 <code>nn.Module</code></li><li>否则既可以使用 <code>nn.functional</code>，也可以使用 <code>nn.Module</code></li></ul><blockquote><p>例如：激活函数、池化层没有可学习参数，可以使用对应的 <code>functional</code> 函数代替。而卷积层、线性层/全连接层需要学习参数，所以使用 <code>nn.Module</code></p><p>【推荐】dropout 虽然无参数学习，但推荐使用 <code>nn.Module</code></p></blockquote><p>例：混合使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 混合使用</span></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        <span class="comment"># 不需要声明那些没有参数学习的层：池化等</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.max_pool2d(F.relu(<span class="variable language_">self</span>.conv1(x)), <span class="number">2</span>)  <span class="comment"># 池化直接写在前向传播里即可</span></span><br><span class="line">        x = F.max_pool2d(F.relu(<span class="variable language_">self</span>.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)  <span class="comment"># 计算池化后的大小</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">x = torch.randn(<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)  <span class="comment"># batch_size=64, channels=3, height=32, width=32</span></span><br><span class="line">model = myNet()</span><br><span class="line">out = model(x)</span><br><span class="line"><span class="built_in">print</span>(out.shape)  <span class="comment"># torch.Size([64, 10])</span></span><br></pre></td></tr></table></figure><h2 id="7-优化器">7 优化器</h2><p>PyTorch 将提供常用的优化方法，这些方法全部封装在 <code>torch.optim</code> 中</p><p>以 [1.2 多层感知机](#1.2 多层感知机) 为例，首先构建网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiPerceptron</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features, out_features</span>):</span><br><span class="line">        <span class="comment"># 新增参数：隐藏层神经元个数（形状）</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 直接使用之前定义的线性层/全连接层 Linear</span></span><br><span class="line">        <span class="variable language_">self</span>.layer1 = Linear(in_features, hidden_features)</span><br><span class="line">        <span class="variable language_">self</span>.layer2 = Linear(hidden_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.layer1(x)</span><br><span class="line">        x = torch.sigmoid(x)  <span class="comment"># 使用激活函数，增加非线性因素（此处是逐个元素计算）</span></span><br><span class="line">        y = <span class="variable language_">self</span>.layer2(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><p>然后实例化网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in_features=3, hidden_features=4, out_features=1</span></span><br><span class="line">mlp = MultiPerceptron(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>设置优化器和学习率（使用随机梯度下降优化器 SGD）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置优化器和学习率</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为网络设置学习率，使用随机梯度下降优化器 SGD</span></span><br><span class="line">optimizer = optim.SGD(params=mlp.parameters(), lr=learning_rate)  <span class="comment"># 【重点】</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面就是网络的训练过程，这里我们只模仿更新一次</span></span><br><span class="line">optimizer.zero_grad()  <span class="comment"># 梯度清零，因为梯度累计效应</span></span><br><span class="line"></span><br><span class="line">inputs = torch.randn(<span class="number">32</span>, <span class="number">3</span>)  <span class="comment"># batch_size=32, in_features=3</span></span><br><span class="line">output = mlp(inputs)</span><br><span class="line">output.backward(output)  <span class="comment"># fake backward</span></span><br><span class="line"></span><br><span class="line">optimizer.step()  <span class="comment"># 执行优化</span></span><br></pre></td></tr></table></figure><blockquote><p>如果想为不同参数设置不同学习率</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为不同的参数分别设置不同的学习率</span></span><br><span class="line">weight_params = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> mlp.named_parameters() <span class="keyword">if</span> name.endswith(<span class="string">&#x27;.W&#x27;</span>)]</span><br><span class="line">bias_params = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> mlp.named_parameters() <span class="keyword">if</span> name.endswith(<span class="string">&#x27;.b&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: bias_params&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: weight_params, <span class="string">&#x27;lr&#x27;</span>: <span class="number">1e-2</span>&#125;</span><br><span class="line">], lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本章主要讲解如何使用 Pytorch 实现深度学习/神经网络里的结构和功能，关注实践，理论较少。nn 模块是 Pytorch 提供的神经网络模块，可以快速便捷地搭建神经网络或神经网络里的各个层（layer）。</summary>
    
    
    
    <category term="深度学习 Pytorch 完整教程" scheme="https://blog.iskage.online/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch-%E5%AE%8C%E6%95%B4%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="AI" scheme="https://blog.iskage.online/tags/AI/"/>
    
    <category term="Python" scheme="https://blog.iskage.online/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>SQL 自学笔记（5）多表查询</title>
    <link href="https://blog.iskage.online/posts/b4103c0f.html"/>
    <id>https://blog.iskage.online/posts/b4103c0f.html</id>
    <published>2025-02-09T06:43:00.000Z</published>
    <updated>2025-02-09T09:21:42.030Z</updated>
    
    <content type="html"><![CDATA[<p>本文笔记根据<a href="https://www.bilibili.com/video/BV1iq4y1u7vj/?share_source=copy_web&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2">【b站 尚硅谷-宋红康 MySQL 课程】</a>整理</p><hr><p>多表查询，关联查询，指对多个表进行查询。</p><blockquote><p>前提：被查询的多表之间存在联系，即存在关键字段、相同字段（例如：外键）</p></blockquote><h1>1 笛卡尔积</h1><h2 id="1-1-错误的查询">1.1 错误的查询</h2><p>如果直接查询，则会出现<strong>笛卡尔积错误</strong>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees, departments;</span><br><span class="line"># <span class="number">2889</span> <span class="keyword">rows</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(last_name) <span class="keyword">FROM</span> employees;  <span class="comment">-- 107 rows</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(department_name) <span class="keyword">FROM</span> departments;  <span class="comment">-- 27 rows</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">107</span> <span class="operator">*</span> <span class="number">27</span>;  <span class="comment">-- 2889</span></span><br></pre></td></tr></table></figure><p>即将两个表的所有行进行了组合，将所有非重复组合一同输出。</p><h2 id="1-2-笛卡尔积">1.2 笛卡尔积</h2><p><strong>定义</strong>：对于集合 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo separator="true">,</mo><mtext> </mtext><mi>B</mi></mrow><annotation encoding="application/x-tex">A,\ B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 称由 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo separator="true">,</mo><mtext> </mtext><mi>B</mi></mrow><annotation encoding="application/x-tex">A,\ B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 则称 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 的笛卡尔积是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 的所有可能组合。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mo>×</mo><mi>B</mi><mo>=</mo><mrow><mo fence="true">{</mo><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mtext> </mtext><mi>b</mi><mo stretchy="false">)</mo><mtext> </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>a</mi><mo>∈</mo><mi>A</mi><mo separator="true">,</mo><mtext> </mtext><mi>b</mi><mo>∈</mo><mi>B</mi><mo fence="true">}</mo></mrow></mrow><annotation encoding="application/x-tex">A \times B = \left\{ (a,\ b)\ |\ a \in A,\ b \in B \right\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">{</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose delimcenter" style="top:0em;">}</span></span></span></span></span></span></p><p>则容易得知新的集合元素个数为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>A</mi><mi mathvariant="normal">∣</mi><mo>×</mo><mi mathvariant="normal">∣</mi><mi>B</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|A| \times |B|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal">A</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord">∣</span></span></span></span> ，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mo>⋅</mo><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|\cdot|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span></span></span></span> 表示元素个数。</p><h2 id="1-3-CROSS-JOIN-交叉连接">1.3 <code>CROSS JOIN</code> 交叉连接</h2><p>笛卡尔积也称为<strong>交叉连接</strong>，可以使用 <code>CROSS JOIN</code> <code>INNER JOIN</code> <code>JOIN</code>链接二表。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees, departments;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> departments;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees <span class="keyword">INNER</span> <span class="keyword">JOIN</span> departments;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees <span class="keyword">JOIN</span> departments;</span><br></pre></td></tr></table></figure><blockquote><p>不通过 <code>WHERE</code> 或 <code>ON</code> 添加条件，则会出现笛卡尔积错误</p></blockquote><h2 id="1-4-WHERE-加入链接条件">1.4 <code>WHERE</code> 加入链接条件</h2><p>为了避免笛卡尔积， 可以<strong>在</strong> <code>WHERE</code> 加入有效的连接条件。格式为</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 表<span class="number">1.</span>列名, 表<span class="number">1.</span>列名</span><br><span class="line"><span class="keyword">FROM</span> 表<span class="number">1</span>, 表<span class="number">2</span></span><br><span class="line"><span class="keyword">WHERE</span> 表<span class="number">1.</span>列名<span class="number">1</span> <span class="operator">=</span> 表<span class="number">2.</span>列名<span class="number">2</span>;  <span class="comment">-- 有效的连接条件</span></span><br></pre></td></tr></table></figure><ul><li>例：根据部门 id 筛选</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">WHERE</span> 加入条件</span><br><span class="line"><span class="keyword">SELECT</span> employees.last_name, departments.department_name, employees.department_id</span><br><span class="line"><span class="keyword">FROM</span> employees, departments</span><br><span class="line"><span class="keyword">WHERE</span> employees.department_id <span class="operator">=</span> departments.department_id;</span><br></pre></td></tr></table></figure><blockquote><p>在表中有相同列时，在列名之前加上表名前缀。【推荐使用，方便标注各表各列】</p></blockquote><h1>2 多表查询概念</h1><h2 id="2-1-等值连接-非等值连接">2.1 等值连接 &amp; 非等值连接</h2><h3 id="2-1-1-键的类型">2.1.1 键的类型</h3><blockquote><p>主键 <code>PRI</code> &amp; 唯一标识 <code>UNI</code> &amp; 外键 <code>MUL</code></p></blockquote><p>在SQL中，<code>key</code> 是用于标识表中列的属性，常见的类型包括 <code>PRI</code>、<code>MUL</code> 和 <code>UNI</code>。这些属性描述了列在表中的角色和约束。以下是它们的详细区别：</p><ul><li><code>PRI</code> (Primary Key)：表示该列为表的主键。</li></ul><p>值唯一，不能有重复；不为空；唯一标识；可作为唯一索引；</p><ul><li><code>UNI</code> (Unique Key)：表示该列具有唯一约束。</li></ul><p>值唯一，不能有重复；允许空值；也可作为唯一索引；</p><ul><li><code>MUL</code> (Multiple)：表示该列是外键或普通索引。</li></ul><p>非唯一性，值可以重复；如果该列是外键，它引用另一个表的主键或唯一键；</p><p>使用 <code>DESC</code> 查询表的属性</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DESC</span> employees;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1739081118663.png" alt=""></p><h3 id="2-1-2-等值连接">2.1.2 等值连接</h3><p>一般通过外键寻找另一个表的唯一索引（<code>PRI</code> 或 <code>UNI</code>）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 根据 job_id 外键连接 jobs 表的 job_title</span></span><br><span class="line"><span class="keyword">SELECT</span> employees.last_name, jobs.job_title, employees.job_id</span><br><span class="line"><span class="keyword">FROM</span> employees, jobs</span><br><span class="line"><span class="keyword">WHERE</span> employees.job_id <span class="operator">=</span> jobs.job_id;</span><br></pre></td></tr></table></figure><ul><li>多个条件 <code>AND</code> 连接</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> employees.last_name, departments.department_name, employees.department_id</span><br><span class="line"><span class="keyword">FROM</span> employees, departments</span><br><span class="line"><span class="keyword">WHERE</span> employees.department_id <span class="operator">=</span> departments.department_id <span class="keyword">AND</span> departments.department_name <span class="operator">=</span> <span class="string">&#x27;Shipping&#x27;</span>;</span><br></pre></td></tr></table></figure><ul><li>多个表中有相同列时，必须在列名之前加上表名前缀</li></ul><p>【推荐使用，方便标注各表各列，提高查询效率】</p><ul><li>使用别名可以简化查询</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.last_name, d.department_name, e.department_id</span><br><span class="line"><span class="keyword">FROM</span> employees <span class="keyword">AS</span> e, departments <span class="keyword">AS</span> d</span><br><span class="line"><span class="keyword">WHERE</span> e.department_id <span class="operator">=</span> d.department_id;</span><br></pre></td></tr></table></figure><blockquote><p>一但使用了别名，这一个语句中必须使用别名替代</p></blockquote><ul><li>连接多个表</li></ul><p>例如：展示员工 <code>last_name, department_name, city</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.last_name, d.department_name, lc.city</span><br><span class="line"><span class="keyword">FROM</span> employees <span class="keyword">AS</span> e,</span><br><span class="line">     departments <span class="keyword">as</span> d,</span><br><span class="line">     locations <span class="keyword">as</span> lc</span><br><span class="line"><span class="keyword">WHERE</span> e.department_id <span class="operator">=</span> d.department_id</span><br><span class="line">  <span class="keyword">AND</span> d.location_id <span class="operator">=</span> lc.location_id;</span><br></pre></td></tr></table></figure><blockquote><p>连接 N 个表，至少需要 N-1 个条件</p></blockquote><h3 id="2-1-3-非等值连接">2.1.3 非等值连接</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.last_name, e.salary, j.grade_level <span class="keyword">AS</span> GRADE</span><br><span class="line"><span class="keyword">FROM</span> employees <span class="keyword">AS</span> e,</span><br><span class="line">     job_grades <span class="keyword">AS</span> j</span><br><span class="line"><span class="keyword">WHERE</span> e.salary <span class="keyword">BETWEEN</span> j.lowest_sal <span class="keyword">AND</span> j.highest_sal  <span class="comment">-- 按工资分档</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> salary <span class="keyword">DESC</span>;  <span class="comment">-- 按照工资排序</span></span><br></pre></td></tr></table></figure><h2 id="2-2-自连接-非自连接">2.2 自连接 &amp; 非自连接</h2><p>使用别名的方式把 <strong>1</strong> 张表虚拟成 <strong>2</strong> 张表，而后两表可以进行内连接和外连接</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> CONCAT(worker.last_name, <span class="string">&#x27; works for &#x27;</span></span><br><span class="line">           , manager.last_name) <span class="keyword">AS</span> relationship <span class="comment">-- 连接为长字符串</span></span><br><span class="line"><span class="keyword">FROM</span> employees <span class="keyword">AS</span> worker,</span><br><span class="line">     employees <span class="keyword">AS</span> manager</span><br><span class="line"><span class="keyword">WHERE</span> worker.manager_id <span class="operator">=</span> manager.employee_id;</span><br></pre></td></tr></table></figure><blockquote><p>将 employees 表虚拟为 表 worker 和 manager，然后自连接</p></blockquote><h2 id="2-3-内连接-外连接">2.3 内连接 &amp; 外连接</h2><ul><li>内连接：合并的表存在联系，即存在相同列。结果中<strong>不包含一个表与另一个表不匹配的行</strong></li></ul><p>内连接使用 <code>INNER JOIN</code></p><ul><li>外连接：两个表在连接过程中除了返回满足连接条件的行以外<strong>还返回不满足条件的行</strong></li></ul><p>外连接使用 <code>LEFT JOIN</code> <code>RIGHT JOIN</code>，特别地，对 MySQL 而言全外连接需要使用 <code>UNION</code> 方法</p><blockquote><p>不满足条件的行 —— 相应的列为空 <code>NULL</code></p><p>当返回不满足条件的行来自<u>左表</u>时，则为<u>左外连接</u>，左边的表也称为<u>主表</u> ，右边的表称为<u>从表</u></p><p>当返回不满足条件的行来自<u>右表</u>时，则为<u>右外连接</u>，右边的表也称为<u>主表</u> ，左边的表称为<u>从表</u></p></blockquote><h1>3 多表查询代码</h1><p>使用 <code>SQL99</code> 语法实现</p><h2 id="3-1-基础语法-JOIN-ON">3.1 基础语法 <code>JOIN ON</code></h2><p>标准格式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> table1.column, </span><br><span class="line">   table2.column,</span><br><span class="line">   table3.column</span><br><span class="line"><span class="keyword">FROM</span> table1</span><br><span class="line"><span class="keyword">JOIN</span> table2 </span><br><span class="line"><span class="keyword">ON</span> table1 和 table2 的连接条件</span><br><span class="line">    <span class="keyword">JOIN</span> table3 </span><br><span class="line">        <span class="keyword">ON</span> table2 和 table3 的连接条件</span><br></pre></td></tr></table></figure><ul><li>各个条件相互独立</li><li>使用 <code>JOIN</code> <code>CROSS JOIN</code> <code>INNER JOIN</code> 含义相同，均代表内连接</li></ul><p>下面看具体内连接案例：</p><h2 id="3-2-内连接">3.2 内连接</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> table1.column,</span><br><span class="line">       table2.column,</span><br><span class="line"><span class="keyword">FROM</span> table1</span><br><span class="line"><span class="keyword">JOIN</span> table2</span><br><span class="line"><span class="keyword">ON</span> 条件</span><br><span class="line"><span class="keyword">WHERE</span> 其他;</span><br></pre></td></tr></table></figure><ul><li>例：查询 <code>last_name, department_name, city</code> 员工为 <code>'Chen'</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.last_name, d.department_name, lc.city</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">JOIN</span> departments d</span><br><span class="line">              <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id</span><br><span class="line">         <span class="keyword">JOIN</span> locations lc</span><br><span class="line">              <span class="keyword">ON</span> d.location_id <span class="operator">=</span> lc.location_id;</span><br><span class="line"><span class="keyword">WHERE</span> e.last_name <span class="operator">=</span> <span class="string">&#x27;Chen&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">&gt;&gt;</span><span class="operator">&gt;</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-----------------+---------+</span></span><br><span class="line"><span class="operator">|</span> last_name <span class="operator">|</span> department_name <span class="operator">|</span> city    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-----------------+---------+</span></span><br><span class="line"><span class="operator">|</span> Chen      <span class="operator">|</span> Finance         <span class="operator">|</span> Seattle <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+-----------------+---------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h2 id="3-3-外连接">3.3 外连接</h2><h3 id="3-3-1-左外连接-LEFT-JOIN-ON">3.3.1 左外连接 <code>LEFT JOIN ON</code></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- A 为主表，B 表向 A 表插入</span></span><br><span class="line"><span class="keyword">SELECT</span> 字段列表</span><br><span class="line"><span class="keyword">FROM</span> A表 </span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> B表</span><br><span class="line"><span class="keyword">ON</span> 关联条件</span><br><span class="line"><span class="keyword">WHERE</span> 其他;</span><br></pre></td></tr></table></figure><ul><li>例：<code>employees</code> 表中 <code>Grant</code> 对应 <code>department_id</code> 为 <code>NULL</code> 故在表 <code>departments</code> 检索不到</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.last_name, e.department_id, d.department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> departments d</span><br><span class="line">                         <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1739087518109.png" alt=""></p><h3 id="3-3-2-右外连接-RIGHT-JOIN-ON">3.3.2 右外连接 <code>RIGHT JOIN ON</code></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- B 为主表，A 表向 B 表插入</span></span><br><span class="line"><span class="keyword">SELECT</span> 字段列表</span><br><span class="line"><span class="keyword">FROM</span> A表 </span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> B表</span><br><span class="line"><span class="keyword">ON</span> 关联条件</span><br><span class="line"><span class="keyword">WHERE</span> 其他;</span><br></pre></td></tr></table></figure><ul><li>例：<code>employees</code> 表中没有处于 <code>departments</code> 表中 <code>Treasury</code> 部门的，故向 <code>departments</code> 插入时补空</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> d.department_name, e.last_name, e.department_id</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> departments d</span><br><span class="line">                          <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1739087735674.png" alt=""></p><h3 id="3-3-3-满外连接-FULL-JOIN-ON">3.3.3 满外连接 <code>FULL JOIN ON</code></h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">满外连接的结果 <span class="operator">=</span> 左右表匹配到的数据 <span class="operator">+</span> 左表没有匹配到的数据 <span class="operator">+</span> 右表没有匹配到的数据</span><br></pre></td></tr></table></figure><p>标准语法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 互相charity</span></span><br><span class="line"><span class="keyword">SELECT</span> 字段列表</span><br><span class="line"><span class="keyword">FROM</span> A表 </span><br><span class="line"><span class="keyword">FULL</span> <span class="keyword">JOIN</span> B表</span><br><span class="line"><span class="keyword">ON</span> 关联条件</span><br><span class="line"><span class="keyword">WHERE</span> 其他;</span><br></pre></td></tr></table></figure><blockquote><p>注：<code>MySQL</code> 不支持 <code>FULL JOIN</code> ，但可以通过<strong>合并左外连接和右外连接的方式</strong>实现。</p><p>可以简单理解：<code>FULL JOIN &lt;=&gt; LEFT JOIN UNION RIGHT JOIN</code></p></blockquote><h1>4 合并查询结果 <code>UNION</code></h1><p><code>UNION</code> 可以将多个 SELECT 语句得到的结果合并成一个表输出：</p><ul><li>匹配：多个 SELECT 语句得到的结果集列数和数据类型必须相同</li><li>去重与否：使用 <code>UNION</code> 会去除重复数据；使用 <code>UNION ALL</code> 不会去除重复数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">column</span>, ... <span class="keyword">FROM</span> table1</span><br><span class="line"><span class="keyword">UNION</span> [<span class="keyword">ALL</span>]  <span class="comment">-- UNION 或者 UNION ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">column</span>, ... <span class="keyword">FROM</span> table2</span><br></pre></td></tr></table></figure><ul><li>例：查询部门编号 <code>department_id &gt; 90</code> 或 邮箱 <code>email</code> 包含 <code>a</code> 的员工信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 法一：使用 WHERE</span></span><br><span class="line"><span class="keyword">SELECT</span> last_name, email, department_id</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">WHERE</span> email <span class="keyword">LIKE</span> <span class="string">&#x27;%a%&#x27;</span></span><br><span class="line">   <span class="keyword">OR</span> department_id <span class="operator">&gt;</span> <span class="number">90</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 法二：使用 UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> last_name, email, department_id</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">WHERE</span> email <span class="keyword">LIKE</span> <span class="string">&#x27;%a%&#x27;</span></span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> last_name, email, department_id</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">WHERE</span> department_id <span class="operator">&gt;</span> <span class="number">90</span>;</span><br></pre></td></tr></table></figure><h1>5 JOIN 实现的几种关系（总结）</h1><h2 id="5-1-内连接：A-交-B">5.1 内连接：A 交 B</h2><p>A 表与 B 表内连接，相当于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∩</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A \cap B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> A.column, B.column</span><br><span class="line"><span class="keyword">FROM</span> A</span><br><span class="line"><span class="keyword">JOIN</span> B</span><br><span class="line"><span class="keyword">ON</span> 条件;</span><br></pre></td></tr></table></figure><h2 id="5-2-左外连接：A-交-A-交-B">5.2 左外连接：A 交 (A 交 B)</h2><p>A 表作为主表，B 表作为从表插入 A ，相当于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>+</mo><mi>A</mi><mo>∩</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A + A \cap B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> A.column, B.column</span><br><span class="line"><span class="keyword">FROM</span> A</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> B</span><br><span class="line"><span class="keyword">ON</span> 条件;</span><br></pre></td></tr></table></figure><h2 id="5-3-右外连接：B-交-A-交-B">5.3 右外连接：B 交 (A 交 B)</h2><p>B 表作为主表，A 表作为从表插入 B ，相当于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>+</mo><mi>A</mi><mo>∩</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">B + A \cap B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> A.column, B.column</span><br><span class="line"><span class="keyword">FROM</span> A</span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> B</span><br><span class="line"><span class="keyword">ON</span> 条件;</span><br></pre></td></tr></table></figure><blockquote><p>左外连接和右外连接本质相同</p></blockquote><h2 id="5-4-WHERE-找出空值-IS-NULL：只属于-A">5.4 WHERE 找出空值 IS NULL：只属于 A</h2><p>A 表作为主表，B 表插入，但去除完美匹配的，即 A 表中与 B 表无关的部分。相当于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>−</mo><mi>A</mi><mo>∩</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A - A \cap B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> A.column, B.column</span><br><span class="line"><span class="keyword">FROM</span> A</span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> B</span><br><span class="line"><span class="keyword">ON</span> 条件</span><br><span class="line"><span class="keyword">WHERE</span> B.column <span class="keyword">IS</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure><ul><li>例：查询 部门表中无员工的部门</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> d.department_name, d.department_id, e.department_id</span><br><span class="line"><span class="keyword">FROM</span> departments d</span><br><span class="line">         <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> employees e</span><br><span class="line">                   <span class="keyword">ON</span> d.department_id <span class="operator">=</span> e.department_id</span><br><span class="line"><span class="keyword">WHERE</span> e.department_id <span class="keyword">IS</span> <span class="keyword">NULL</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1739090941337.png" alt=""></p><h2 id="5-5-UNION-合并：A-并-B">5.5 UNION 合并：A 并 B</h2><p>UNION 可以将多个 SELECT 语句得到的结果合并成一个表输出，相当于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∪</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A \cup B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">column</span>, ... </span><br><span class="line"><span class="keyword">FROM</span> A</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> B</span><br><span class="line"><span class="keyword">ON</span> A.col <span class="operator">=</span> B.col</span><br><span class="line"><span class="keyword">WHERE</span> B.col <span class="keyword">IS</span> <span class="keyword">NULL</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">column</span>, ... </span><br><span class="line"><span class="keyword">FROM</span> A</span><br><span class="line">    <span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> B</span><br><span class="line">    <span class="keyword">ON</span> A.col <span class="operator">=</span> B.col</span><br></pre></td></tr></table></figure><ul><li>例：两张方法，推荐第一种【先手动去重，再使用 UNION ALL 效率高】</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 法一：利用 WHERE d.department_id IS NULL 手动去重</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> departments d</span><br><span class="line">                   <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id</span><br><span class="line"><span class="keyword">WHERE</span> d.department_id <span class="keyword">IS</span> <span class="keyword">NULL</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> departments d</span><br><span class="line">                    <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 法二：直接使用 UNION 自动去重</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> departments d</span><br><span class="line">                   <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id</span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> departments d</span><br><span class="line">                    <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id;</span><br></pre></td></tr></table></figure><h2 id="5-6-找出空值后-UNION-合并：对称差">5.6 找出空值后 UNION 合并：对称差</h2><p>查询所有匹配失败的行数据，相当于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∪</mo><mi>B</mi><mo>−</mo><mi>A</mi><mo>∩</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A \cup B - A \cap B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">column</span>, ... </span><br><span class="line"><span class="keyword">FROM</span> A</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> B</span><br><span class="line"><span class="keyword">ON</span> A.col <span class="operator">=</span> B.col</span><br><span class="line"><span class="keyword">WHERE</span> B.col <span class="keyword">IS</span> <span class="keyword">NULL</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">column</span>, ... </span><br><span class="line"><span class="keyword">FROM</span> A</span><br><span class="line">    <span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> B</span><br><span class="line">    <span class="keyword">ON</span> A.col <span class="operator">=</span> B.col</span><br><span class="line"><span class="keyword">WHERE</span> A.col <span class="keyword">IS</span> <span class="keyword">NULL</span>  <span class="comment">-- 全部加上 IS NULL 判断</span></span><br></pre></td></tr></table></figure><ul><li>例：查询所有失败行数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> departments d</span><br><span class="line">                   <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id</span><br><span class="line"><span class="keyword">WHERE</span> d.department_id <span class="keyword">IS</span> <span class="keyword">NULL</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> departments d</span><br><span class="line">                    <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id</span><br><span class="line"><span class="keyword">WHERE</span> e.department_id <span class="keyword">IS</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1739091754343.png" alt=""></p><h1>6 特殊等值连接</h1><h2 id="6-1-自然连接">6.1 自然连接</h2><p><code>NATURAL JOIN</code> 会自动查询两张表中<strong>所有相同的字段</strong>，然后进行等值连接</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 这是 SQL99 新语法</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">NATURAL</span> <span class="keyword">JOIN</span> departments d;</span><br></pre></td></tr></table></figure><blockquote><p>注：自然连接使用 <code>AND</code> 连接各个条件。即只有当所有相同列名的值都相等时，才会放入结果集。</p><p>例如：当表 employees 和表 departments 有相同列 department_id 和 manager_id 时</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 上面自然连接等价于</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">JOIN</span> departments d</span><br><span class="line">              <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id</span><br><span class="line">                  <span class="keyword">AND</span> e.manager_id <span class="operator">=</span> d.manager_id;  <span class="comment">-- 是 AND 而不是 OR</span></span><br></pre></td></tr></table></figure><h2 id="6-2-USING-连接">6.2 USING 连接</h2><p>当有多个同名字段时，<code>USING</code> 可以指定同名字段进行连接</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e,</span><br><span class="line">     departments d</span><br><span class="line"><span class="keyword">WHERE</span> e.department_id <span class="operator">=</span> d.department_id;  <span class="comment">-- 利用 USING 简化</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 可以简化为</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">JOIN</span> departments d</span><br><span class="line">              <span class="keyword">USING</span> (department_id);  <span class="comment">-- 指定 department_id</span></span><br></pre></td></tr></table></figure><h2 id="6-3-总结：等值连接的三种写法">6.3 总结：等值连接的三种写法</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 法一：WHERE 限制条件</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e,</span><br><span class="line">     departments d</span><br><span class="line"><span class="keyword">WHERE</span> e.department_id <span class="operator">=</span> d.department_id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 法二：JOIN ... ON + 条件</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">JOIN</span> departments d</span><br><span class="line">              <span class="keyword">ON</span> e.department_id <span class="operator">=</span> d.department_id;</span><br><span class="line">                  </span><br><span class="line"><span class="comment">-- 法三：USING 指定同名字段</span></span><br><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, department_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line">         <span class="keyword">JOIN</span> departments d</span><br><span class="line">              <span class="keyword">USING</span> (department_id);  <span class="comment">-- 指定 department_id</span></span><br></pre></td></tr></table></figure><blockquote><ul><li><code>WHERE</code> 的使用没有限制，目的就是增加约束条件</li><li><code>ON</code> 只能和 <code>JOIN</code> 连用</li><li><code>USING</code> 只能和 <code>JOIN</code>，且要求字段必须同名</li></ul></blockquote>]]></content>
    
    
    <summary type="html">本文详细介绍 数据库查询语言 SQL 的多表查询知识，包括等值连接、自连接、外连接。重点讲解 JOIN ON 语法</summary>
    
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/tags/MySQL/"/>
    
    <category term="SQL" scheme="https://blog.iskage.online/tags/SQL/"/>
    
    <category term="数据库" scheme="https://blog.iskage.online/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>深度学习框架 Pytorch 深入学习（2）：自动求导 autograd 反向传播 backward 与计算图</title>
    <link href="https://blog.iskage.online/posts/421f3bd1.html"/>
    <id>https://blog.iskage.online/posts/421f3bd1.html</id>
    <published>2025-02-08T10:32:00.000Z</published>
    <updated>2025-02-08T11:06:07.762Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch搭建神经网络（2）自动求导autograd、反向传播backward与计算图"><a href="#Pytorch搭建神经网络（2）自动求导autograd、反向传播backward与计算图" class="headerlink" title="Pytorch搭建神经网络（2）自动求导autograd、反向传播backward与计算图"></a>Pytorch搭建神经网络（2）自动求导autograd、反向传播backward与计算图</h1><p>基于<a href="https://book.douban.com/subject/27624483/">《深度学习框架 Pytorch 入门与实践》陈云</a> </p><p>参考 <a href="https://github.com/chenyuntc/pytorch-book">Github 的 pytorch-book 项目</a></p><p>参考 <a href="https://github.com/zergtant/pytorch-handbook">GitHub 的 pytorch-handbook 项目</a></p><hr><p><code>torch.autograd</code> 提供了一套自动求导方式，它能够根据前向传播过程自动构建计算图，执行反向传播。</p><h2 id="1-autograd-的数学原理：计算图"><a href="#1-autograd-的数学原理：计算图" class="headerlink" title="1 autograd 的数学原理：计算图"></a>1 autograd 的数学原理：计算图</h2><p>计算图原理可以查看 <strong>cs231n</strong> 课程讲解：【计算图的原理非常重要！】或者见<a href="#3">后文分析</a></p><p>英文官网 <a href="https://cs231n.github.io/">https://cs231n.github.io/</a></p><p>b站 课程整理 <a href="https://www.bilibili.com/video/BV1nJ411z7fe?p=8&amp;spm_id_from=333.788.videopod.episodes">BV1nJ411z7fe</a> 【反向传播章节】</p><p>b站 中文讲解 <a href="https://www.bilibili.com/video/BV1K7411W7So?p=4&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2">【子豪兄】精讲CS231N斯坦福计算机视觉公开课（2020最新）</a></p><h2 id="2-autograd-的使用：requires-grad-amp-backward"><a href="#2-autograd-的使用：requires-grad-amp-backward" class="headerlink" title="2 autograd 的使用：requires_grad &amp; backward"></a>2 autograd 的使用：requires_grad &amp; backward</h2><h3 id="2-1-requires-grad-属性"><a href="#2-1-requires-grad-属性" class="headerlink" title="2.1 requires_grad 属性"></a>2.1 requires_grad 属性</h3><p>只需要对Tensor增加一个 <code>requires_grad=True</code> 属性，Pytorch就会自动计算 <code>requires_grad=True</code> 属性的 Tensor，并保留计算图，从而快速实现反向传播。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Method 1</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Method 2</span></span><br><span class="line">x = torch.rand(<span class="number">2</span>, <span class="number">3</span>).requires_grad_()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Method 3</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">x.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.requires_grad)  <span class="comment"># True</span></span><br></pre></td></tr></table></figure><h3 id="2-2-backward-反向传播"><a href="#2-2-backward-反向传播" class="headerlink" title="2.2 backward 反向传播"></a>2.2 backward 反向传播</h3><p>反向传播函数的使用：其中第一个参数 <code>tensors</code> 传入用于计算梯度的张量，格式和各个参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.backward(tensors, grad_tensors=<span class="literal">None</span>, retain_graph=<span class="literal">None</span>, create_graph=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><ul><li><p><code>tensors</code>：用于计算梯度的Tensor，如<code>torch.autograd.backward(y)</code>，等价于<code>y.backward()</code>。</p></li><li><p><code>grad_tensors</code>：形状与tensors一致，对于<code>y.backward(grad_tensors)</code>，grad_tensors相当于链式法则${\mathrm{d}z \over \mathrm{d}x}={\mathrm{d}z \over \mathrm{d}y} \times {\mathrm{d}y \over \mathrm{d}x}$中的${\mathrm{d}z} \over {\mathrm{d}y}$。【结合例子理解见后】</p></li><li><code>retain_graph</code>：计算计算图里每一个导数值时需要保留各个变量的值，retain_graph 为 True 时会保存。【结合例子理解见后】</li></ul><h4 id="2-2-1-requires-grad-属性的传递"><a href="#2-2-1-requires-grad-属性的传递" class="headerlink" title="2.2.1 requires_grad 属性的传递"></a>2.2.1 requires_grad 属性的传递</h4><ul><li>例：<code>a</code> 需要求导，<code>b</code> 不需要，<code>c</code> 定义为 <code>a + b</code> 的元素加和</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">c = (a + b).<span class="built_in">sum</span>()  <span class="comment"># c 受 a 的影响，c.requires_grad = True</span></span><br><span class="line"></span><br><span class="line">a.requires_grad, b.requires_grad, c.requires_grad</span><br><span class="line"><span class="comment"># (True, False, True)</span></span><br></pre></td></tr></table></figure><h4 id="2-2-2-is-leaf-叶子结点"><a href="#2-2-2-is-leaf-叶子结点" class="headerlink" title="2.2.2 is_leaf 叶子结点"></a>2.2.2 is_leaf 叶子结点</h4><p>对于计算图中的Tensor而言， <code>is_leaf=True</code> 的Tensor称为Leaf Tensor，也就是计算图中的叶子节点。</p><ul><li><code>requires_grad=False</code> 时，无需求导，故为叶子结点。</li><li>即使 <code>requires_grad=True</code> 但是由用户创建的时，此时它位于计算图的头部（叶子结点），它的梯度会被保留下来。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仍然是上面的例子</span></span><br><span class="line">a.is_leaf, b.is_leaf, c.is_leaf</span><br><span class="line"><span class="comment"># (True, True, False)</span></span><br></pre></td></tr></table></figure><h3 id="2-3-autograd-利用计算图计算导数"><a href="#2-3-autograd-利用计算图计算导数" class="headerlink" title="2.3 autograd 利用计算图计算导数"></a>2.3 autograd 利用计算图计算导数</h3><p>利用 autograd 计算导数，对于函数 $y=x^2e^x$，它的导函数解析式为</p><script type="math/tex; mode=display">\begin{equation}\dfrac{d\ y}{d\ x} = 2xe^x + x^2e^x\end{equation}</script><p>定义计算 y 函数和计算解析式导数结果函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># autograd 求导</span></span><br><span class="line"><span class="comment"># y = x^2 * e^x</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    y = x * x * torch.exp(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">df</span>(<span class="params">x</span>):</span><br><span class="line">    df = <span class="number">2</span> * x * torch.exp(x) + x * x * torch.exp(x)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure><ul><li>例：随机赋值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = f(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y = </span></span><br><span class="line"><span class="comment"># tensor([[0.1387, 0.4465, 0.4825],</span></span><br><span class="line"><span class="comment">#         [0.1576, 4.1902, 0.5185]], grad_fn=&lt;MulBackward0&gt;)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(gradient=torch.ones(y.size()))  <span class="comment"># 指定 dy/dx = dy/dx * 1 的 dy/dx</span></span><br><span class="line"><span class="comment"># torch.autograd.backward(y, grad_tensors=torch.ones(y.size()))  # 或者</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 反向传播后才能取到 y 关于 x 的导数（已经代入了此时 x 的值）</span></span><br><span class="line"><span class="comment"># tensor([[-0.4497,  2.1766, -0.2087],</span></span><br><span class="line"><span class="comment">#         [-0.4567, 11.4700, -0.1244]])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df(x))   <span class="comment"># 解析求出的导数值</span></span><br><span class="line"><span class="comment"># tensor([[-0.4497,  2.1766, -0.2087],</span></span><br><span class="line"><span class="comment">#         [-0.4567, 11.4700, -0.1244]], grad_fn=&lt;AddBackward0&gt;)</span></span><br></pre></td></tr></table></figure><p><code>x.grad &amp; df(x)</code> 二者是在数值上是一样的</p><h2 id="3-反向传播与计算图"><a href="#3-反向传播与计算图" class="headerlink" title="3 反向传播与计算图"></a>3 反向传播与计算图<a id='3'></a></h2><h3 id="3-1-计算图原理：链式法则"><a href="#3-1-计算图原理：链式法则" class="headerlink" title="3.1 计算图原理：链式法则"></a>3.1 计算图原理：链式法则</h3><p>根据链式法则</p><p>$dz/dy = 1,\ dz/db = 1$</p><p>$dy/dw = x,\ dy/dx = w$</p><p>$dz/dx = dz/dy \times dy/dx = 1 \times w,\ dz/dw = dz/dy \times dy/dw = 1 \times x$</p><p>只要存储结点的导数和值便可通过简单的乘法计算所有导数</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1739002830328.png" alt=""></p><p>按照上图构造</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算图</span></span><br><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">b = torch.rand(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = torch.rand(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = w * x <span class="comment"># 等价于 y = w.mul(x)</span></span><br><span class="line">z = y + b <span class="comment"># 等价于 z = y.add(b)</span></span><br><span class="line"></span><br><span class="line">x.requires_grad, b.requires_grad, w.requires_grad, y.requires_grad, z.requires_grad</span><br><span class="line"><span class="comment"># (False, True, True, True, True)</span></span><br></pre></td></tr></table></figure><h3 id="3-2-grad-fn-查看反向传播函数"><a href="#3-2-grad-fn-查看反向传播函数" class="headerlink" title="3.2 grad_fn 查看反向传播函数"></a>3.2 grad_fn 查看反向传播函数</h3><p><code>grad_fn</code> 可以查看这个结点的函数类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z.grad_fn  <span class="comment"># &lt;AddBackward0 at 0x7f96b951ba90&gt;  Add 加法，因为 z = y + b</span></span><br><span class="line">y.grad_fn  <span class="comment"># &lt;MulBackward0 at 0x7f96b951b400&gt;  Mul 乘法，因为 y = w * x</span></span><br><span class="line"></span><br><span class="line">w.grad_fn, x.grad_fn, b.grad_fn <span class="comment"># (None, None, None) 叶子结点是 grad_fn=None</span></span><br></pre></td></tr></table></figure><p><code>grad_fn.next_functions</code> 获取 grad_fn 的输入，返回上一步的反向传播函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">z.grad_fn.next_functions  <span class="comment"># z 前是 y 和 b</span></span><br><span class="line"><span class="comment"># ((&lt;MulBackward0 at 0x7f96b951b400&gt;, 0),  # y = w * x 是 mul</span></span><br><span class="line"><span class="comment">#  (&lt;AccumulateGrad at 0x7f96b95c6af0&gt;, 0))  # b 是叶子结点，需要求导 AccumulateGrad</span></span><br><span class="line"></span><br><span class="line">y.grad_fn.next_functions  <span class="comment"># y 前是 w 和 x</span></span><br><span class="line"><span class="comment"># ((&lt;AccumulateGrad at 0x7f9678466160&gt;, 0),  # w 是叶子结点，需要求导 AccumulateGrad</span></span><br><span class="line"><span class="comment">#  (None, 0)  # x 是叶子节点，x.requires_grad=False 不需要求导 None</span></span><br></pre></td></tr></table></figure><h3 id="3-3-retain-graph-的使用（仅叶子结点）"><a href="#3-3-retain-graph-的使用（仅叶子结点）" class="headerlink" title="3.3 retain_graph 的使用（仅叶子结点）"></a>3.3 retain_graph 的使用（仅叶子结点）</h3><p>如果不指定 <code>retain_graph=True</code> ，则在反向传播后，会自动清除变量值。</p><p>例如：计算 <code>w.grad</code> w 的梯度时，需要 x 的值 （$dy/dw = x$）</p><blockquote><p>注意：x.requires_grad=False 不需要求导，故 <code>x.grad</code> 报错</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="comment"># tensor([1.])  # 确实是我们之前设的 x = torch.ones(1) 相匹配</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 再次运行，梯度累加</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="comment"># tensor([1.])  # 1 + 1 = 2 累加，所以之前 grad_fn 取名为 AccumulateGrad</span></span><br></pre></td></tr></table></figure><h3 id="3-4-关闭反向传播"><a href="#3-4-关闭反向传播" class="headerlink" title="3.4 关闭反向传播"></a>3.4 关闭反向传播</h3><p>某一个节点 <code>requires_grad</code>被设置为 <code>True</code> ，那么所有依赖它的节点 <code>requires_grad</code> 都是 <code>True</code>。有时不需要对所有结点都反向传播（求导），从而来节省内存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.rand(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * w</span><br><span class="line"></span><br><span class="line">x.requires_grad, w.requires_grad, y.requires_grad  <span class="comment"># y.requires_grad = True</span></span><br><span class="line"><span class="comment"># (False, True, True)</span></span><br></pre></td></tr></table></figure><p>下面我们来关闭关于 <code>y</code> 的反向传播</p><ul><li>法一：<code>with torch.no_grad():</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    x = torch.ones(<span class="number">1</span>)</span><br><span class="line">    w = torch.rand(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = x * w</span><br><span class="line">    </span><br><span class="line">x.requires_grad, w.requires_grad, y.requires_grad  <span class="comment"># y.requires_grad = False</span></span><br><span class="line"><span class="comment"># (False, True, False)</span></span><br></pre></td></tr></table></figure><ul><li>法二：设置默认 <code>torch.set_grad_enabled(False)</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.set_grad_enabled(<span class="literal">False</span>) <span class="comment"># 更改默认设置</span></span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">1</span>)</span><br><span class="line">w = torch.rand(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x * w</span><br><span class="line"></span><br><span class="line">x.requires_grad, w.requires_grad, y.requires_grad  <span class="comment"># y.requires_grad = False</span></span><br><span class="line"><span class="comment"># (False, True, False)</span></span><br><span class="line"></span><br><span class="line">torch.set_grad_enabled(<span class="literal">True</span>) <span class="comment"># 恢复默认设置</span></span><br></pre></td></tr></table></figure><h3 id="3-5-data-从计算图取出Tensor的值"><a href="#3-5-data-从计算图取出Tensor的值" class="headerlink" title="3.5 .data 从计算图取出Tensor的值"></a>3.5 <code>.data</code> 从计算图取出Tensor的值</h3><p>修改张量的数值，又不影响计算图，使用 <code>tensor.data</code> 方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">x_clone = x.data</span><br><span class="line"></span><br><span class="line">x.requires_grad, x_clone.requires_grad  <span class="comment"># x_clone 独立于原来的计算图</span></span><br><span class="line"><span class="comment"># (True, False)</span></span><br></pre></td></tr></table></figure><h3 id="3-6-存储非叶子结点的梯度"><a href="#3-6-存储非叶子结点的梯度" class="headerlink" title="3.6 存储非叶子结点的梯度"></a>3.6 存储非叶子结点的梯度</h3><p>在计算图流程中，非叶子结点求导后其导数值便立刻被清除。可以使用 <code>autograd.grad</code> 或 <code>hook</code> 方法保留</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># autograd.grad &amp; hook</span></span><br><span class="line">x = torch.ones(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = torch.ones(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = w * x  <span class="comment"># 非叶子结点</span></span><br><span class="line">z = y.<span class="built_in">sum</span>()  <span class="comment"># 非叶子结点</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z.backward()</span><br><span class="line">x.grad, w.grad, y.grad  <span class="comment"># 非叶子结点 y.grad = None</span></span><br><span class="line"><span class="comment"># (tensor([1.]), tensor([1.]), None)</span></span><br></pre></td></tr></table></figure><blockquote><p>若为叶子结点可以采用 <code>z.backward(retain_graph=True)</code> 的方式</p></blockquote><ul><li>法一：<code>torch.autograd.grad()</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 torch.autograd.grad() 直接取梯度</span></span><br><span class="line">x = torch.ones(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = torch.ones(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = x * w</span><br><span class="line">z = y.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">torch.autograd.grad(z, y)  <span class="comment"># z.backward() 并直接取 y.grad()</span></span><br><span class="line"><span class="comment"># (tensor([1.]),)</span></span><br></pre></td></tr></table></figure><ul><li>法二：<code>hook</code></li></ul><p>标准格式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hook是一个函数，输入是梯度，不应该有返回值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">variable_hook</span>(<span class="params">grad</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;y.grad：&#x27;</span>, grad)</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">w = torch.ones(<span class="number">1</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x * w</span><br><span class="line"><span class="comment"># 注册hook</span></span><br><span class="line">hook_handle = y.register_hook(variable_hook)</span><br><span class="line"></span><br><span class="line">z = y.<span class="built_in">sum</span>()</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除非每次都要使用 hook，否则用完之后记得移除 hook</span></span><br><span class="line">hook_handle.remove()</span><br><span class="line"></span><br><span class="line"><span class="comment"># y.grad： tensor([1.])</span></span><br></pre></td></tr></table></figure><h2 id="4-案例：线性回归"><a href="#4-案例：线性回归" class="headerlink" title="4 案例：线性回归"></a>4 案例：线性回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fake_data</span>(<span class="params">batch_size=<span class="number">16</span></span>):</span><br><span class="line">    <span class="comment"># 产生随机数据：y = 2 * x + 3，加上噪声</span></span><br><span class="line">    x = torch.rand(batch_size, <span class="number">1</span>) * <span class="number">5</span>  <span class="comment"># 扩大一些，以免噪声太明显</span></span><br><span class="line">    y = x * <span class="number">2</span> + <span class="number">3</span> + torch.randn(batch_size, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子，保证结果可复现</span></span><br><span class="line">torch.manual_seed(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">x, y = get_fake_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">w = torch.rand(<span class="number">1</span>, <span class="number">1</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># w.shape = torch.Size([1, 1]) 因为 [8, 1] * [1, 1] -&gt; [batch_size, 1] 和 y 维度相同</span></span><br><span class="line">b = torch.zeros(<span class="number">1</span>, <span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">losses = np.zeros(<span class="number">200</span>)  <span class="comment"># 存储损失值</span></span><br><span class="line">lr = <span class="number">0.005</span>  <span class="comment"># 学习率</span></span><br><span class="line">EPOCHS = <span class="number">200</span>  <span class="comment"># 迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    x, y = get_fake_data(batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播 计算损失</span></span><br><span class="line">    y_pred = x.mm(w) + b.expand_as(y)  <span class="comment"># expand_as(y) 是广播机制，即将 b 复制成和 y 相同性质的张量 [1, 1] -&gt; [batch_size, 1]</span></span><br><span class="line">    loss = <span class="number">0.5</span> * (y_pred - y) ** <span class="number">2</span>  <span class="comment"># MSE 均方误差，这是对张量 y 逐元素计算</span></span><br><span class="line">    loss = loss.<span class="built_in">sum</span>()  <span class="comment"># 累和成一个数</span></span><br><span class="line">    losses[epoch] = loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; 取 .data 是因为每一轮是根据随机生成的 batch_size 个点训练，但我们希望存储的是全局参数 w, b &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; 故每次依据样本点更新全局参数，而不是改批次的参数 &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    w.data.sub_(lr * w.grad.data)  <span class="comment"># 或者 w.data = w.data - lr * w.grad.data</span></span><br><span class="line">    b.data.sub_(lr * b.grad.data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度清零</span></span><br><span class="line">    w.grad.data.zero_()  <span class="comment"># 不清零，梯度会不断累加</span></span><br><span class="line">    b.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每隔 10 次扔出当前训练情况</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch: &#123;&#125; / &#123;&#125;, Parameters: w is &#123;&#125;, b is &#123;&#125;, Loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, EPOCHS, w.item(), b.item(), losses[epoch]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Epoch: &#123;&#125; / &#123;&#125;, Parameters: w is &#123;&#125;, b is &#123;&#125;, Loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(EPOCHS, EPOCHS, w.item(), b.item(), losses[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1739010576001.png" alt=""></p>]]></content>
    
    
    <summary type="html">计算图、前向传播和反向传播是搭建神经网络的重要知识，本文从原理详细介绍了 Pytorch 中对这些流程的实现。最后提供了一个从0编程训练线性回归模型的案例。</summary>
    
    
    
    <category term="深度学习 Pytorch 完整教程" scheme="https://blog.iskage.online/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch-%E5%AE%8C%E6%95%B4%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="AI" scheme="https://blog.iskage.online/tags/AI/"/>
    
    <category term="Python" scheme="https://blog.iskage.online/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>SQL 自学笔记（4）排序与分页：ORDER BY 子句 和 LIMIT 子句</title>
    <link href="https://blog.iskage.online/posts/c8719e79.html"/>
    <id>https://blog.iskage.online/posts/c8719e79.html</id>
    <published>2025-02-08T06:43:00.000Z</published>
    <updated>2025-02-08T06:47:33.146Z</updated>
    
    <content type="html"><![CDATA[<p>本文笔记根据<a href="https://www.bilibili.com/video/BV1iq4y1u7vj/?share_source=copy_web&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2">【b站 尚硅谷-宋红康 MySQL 课程】</a>整理</p><h1>1 排序</h1><p>为了使数据按照设定的规则排序输出，可以使用 <code>ORDER BY</code></p><ul><li><p>使用 <code>ORDER BY 字段名 ASC</code> 表示按照 <code>字段</code> <strong>生序</strong> 展示</p></li><li><p>使用 <code>ORDER BY 字段名 DESC</code> 表示按照 <code>字段</code> <strong>降序</strong> 展示</p></li><li><p><code>ORDER BY</code> 位于  <code>SELECT</code> 语句结尾</p></li></ul><h2 id="1-1-按照某一列排序">1.1 按照某一列排序</h2><ul><li>例如按照 <code>hire_date</code> 升序</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> first_name, email, hire_date</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> hire_date;  # 按照 hire_date 排序输出，默认升序 <span class="keyword">ASC</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> first_name, email, hire_date</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> hire_date <span class="keyword">ASC</span>;  # 按照 hire_date 升序输出</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738994927454.png" alt=""></p><ul><li>按照 <code>hire_date</code> 降序</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> first_name, email, hire_date</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> hire_date <span class="keyword">DESC</span>;  # 按照 hire_date 降序输出</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738995009057.png" alt=""></p><ul><li>列别名同样可以使用</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> first_name, salary <span class="operator">*</span> <span class="number">12</span> <span class="keyword">AS</span> &quot;annual salary&quot;</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> &quot;annual salary&quot; <span class="keyword">DESC</span>;  # 按照别名 annual salary 降序</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738995074331.png" alt=""></p><h2 id="1-2-按照多列排序">1.2 按照多列排序</h2><p>根据多个列进行数据行排序，格式为</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列<span class="number">1</span>, 列<span class="number">2</span>, 列<span class="number">3</span></span><br><span class="line"><span class="keyword">FROM</span> 表名</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> 列<span class="number">2</span>, 列<span class="number">4</span>;</span><br></pre></td></tr></table></figure><ul><li>排序依据列可以不在展示列中：即 <code>ORDER BY</code> 后的列可以不是 <code>SELECT</code> 后的列，只要是表中的列即可</li><li>多列排序，按照从左到右的方式排序 <code>ORDER BY</code> 后面的列，只有前面列存在相同元素时，才会参考后一列进行排序：即先按照 <code>列2</code> 排，如果 <code>列2</code> 各异，则完全按照 <code>列2</code> 拍；否则参考 <code>列4</code></li></ul><p>例如：按照部门降序，聘请日期升序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> last_name, department_id, hire_date</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> department_id <span class="keyword">DESC</span>, hire_date <span class="keyword">ASC</span>;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738995708094.png" alt=""></p><h1>2 分页</h1><p>返回特定范围（行）的数据，以免占据页面过多</p><blockquote><p>也可以使用图形化工具，例如 <code>jetBrains</code> 的 <a href="https://www.jetbrains.com.cn/datagrip/"><code>DataGrip</code> 产品</a>。</p></blockquote><p>MySQL 中使用 <code>LIMIT</code> 实现分页，格式为</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LIMIT 起始行数, 行数  <span class="comment">-- 从 0 开始计数，起始行数可省略，默认为 0</span></span><br></pre></td></tr></table></figure><ul><li><code>LIMIT</code> 语句放在 <code>SELECT</code> 语句结尾</li><li>不同数据库管理系统 DBMS 的语句存在差异</li></ul><p>例如：选取前 5 行</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line">LIMIT <span class="number">5</span>;  <span class="comment">-- 前 5 行</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line">LIMIT <span class="number">0</span>, <span class="number">5</span>;  <span class="comment">-- [0, 1, 2, 3, 4] 从第 0 行开始往后查 5 行</span></span><br></pre></td></tr></table></figure><p>例如：选取第 11 到第 15 行数据（从 0 计数则是：从第 10 行到第 14 行）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line">LIMIT <span class="number">10</span>, <span class="number">10</span>;  <span class="comment">-- [10, 11, 12, 13, 14] 从 0 计数: 从第 10 行到第 14 行</span></span><br></pre></td></tr></table></figure><blockquote><p><code>MySQL 8.0</code>中可以使用 <code>LIMIT 3 OFFSET 4</code> 等价于 <code>LIMIT 4, 3</code></p></blockquote>]]></content>
    
    
    <summary type="html">本文详细介绍 数据库查询语言 SQL 的 ORDER BY 子句，使数据按照设定的规则排序输出。LIMIT 子句，返回特定范围（行）的数据，以免占据页面过多</summary>
    
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/tags/MySQL/"/>
    
    <category term="SQL" scheme="https://blog.iskage.online/tags/SQL/"/>
    
    <category term="数据库" scheme="https://blog.iskage.online/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>为搭建神经网络创建虚拟环境全流程：下载 Python，利用 conda 创建虚拟环境，激活与安装 Pytorch</title>
    <link href="https://blog.iskage.online/posts/7d8f6610.html"/>
    <id>https://blog.iskage.online/posts/7d8f6610.html</id>
    <published>2025-02-08T02:27:00.000Z</published>
    <updated>2025-02-09T02:05:33.505Z</updated>
    
    <content type="html"><![CDATA[<h1>1 下载 Python</h1><blockquote><p>注：这一部分已不需要，可以直接使用 conda 下载</p></blockquote><p>MacOS 和 Windows 下载 Python 全流程见 <a href="https://blog.iskage.online/posts/7d46a7ea.html">全平台下载安装 Python 全流程指南：Windows &amp; MacOS</a></p><h1>2 Conda 下载</h1><p>利用 Conda 更好的管理开发环境，全流程见 <a href="https://blog.iskage.online/posts/2c3265b7.html">Conda 创建虚拟环境全流程</a></p><h1>3 创建环境并激活</h1><h2 id="3-1-首先创建环境">3.1 首先创建环境</h2><p>例如我们创建一个环境名为 <code>pytorch</code> 的虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pytorch</span><br></pre></td></tr></table></figure><h2 id="3-2-激活环境">3.2 激活环境</h2><p>而后激活这个环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate pytorch</span><br></pre></td></tr></table></figure><p>激活后会在命令后/终端前显示环境名</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(pytorch) Username/@xx %   <span class="comment"># macOS</span></span><br><span class="line">(pytorch) PC C:\xx\xx&gt;     <span class="comment"># Windows</span></span><br></pre></td></tr></table></figure><h2 id="3-3-下载-python-和-pip">3.3 下载 python 和 pip</h2><p>在终端中输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pip</span><br></pre></td></tr></table></figure><blockquote><p>注：需要先激活环境。此时 conda 会自动为该环境配置 python 和 pip，为之后使用 pip 下载 torch 作准备。</p></blockquote><h1>4 官网查询下载 Pytorch 命令</h1><p>去往 <a href="https://pytorch.org/">Pytorch 官网</a>，向下滑动找到【Install PyTorch】</p><h2 id="4-1-macOS-安装指令">4.1 macOS 安装指令</h2><p>根据实际情况，选择对应配置，复制代码后在【终端】运行即可。</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738986071255.png" alt=""></p><ul><li>一定要在【步骤三：创建环境并激活】后执行</li><li>目前 Pytorch 官方已不提供 conda 下载方式，但 conda 的环境管理仍然有效，采用 pip 安装同样可行</li></ul><h2 id="4-2-Windows-安装指令">4.2 Windows 安装指令</h2><p>根据实际情况，选择对应配置，复制代码后在【PowerShell】运行即可。</p><h3 id="4-2-1-安装在-CPU">4.2.1 安装在 CPU</h3><p>选择 CPU 后复制命令</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738986244205.png" alt=""></p><h3 id="4-2-2-安装在-GPU">4.2.2 安装在 GPU</h3><p>在命令行中输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p>则会返回英伟达显卡的 <code>CUDA</code> 版本，根据版本号选择</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/22a53eeea951769ba92e14a70427c89a_720.png" alt=""></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738986564831.png" alt=""></p><h1>5 删除环境</h1><p>因为是虚拟环境，删除环境不会对其他配置造成影响，在终端或命令行中输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -n 环境名 --all</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">为了更好的对开发环境进行管理，总是需要配置虚拟环境。本文介绍了如何下载 Python，利用 conda 创建虚拟环境，激活与安装 Pytorch，管理和删除环境。</summary>
    
    
    
    <category term="深度学习 Pytorch 完整教程" scheme="https://blog.iskage.online/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch-%E5%AE%8C%E6%95%B4%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="conda" scheme="https://blog.iskage.online/tags/conda/"/>
    
    <category term="虚拟环境" scheme="https://blog.iskage.online/tags/%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"/>
    
    <category term="Python" scheme="https://blog.iskage.online/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>全平台下载安装 Python 全流程指南：Windows &amp; MacOS</title>
    <link href="https://blog.iskage.online/posts/7d46a7ea.html"/>
    <id>https://blog.iskage.online/posts/7d46a7ea.html</id>
    <published>2025-02-08T01:11:00.000Z</published>
    <updated>2025-02-09T02:05:30.425Z</updated>
    
    <content type="html"><![CDATA[<h1>1 MacOS 系统安装 Python</h1><p>一般而言，macOS 系统默认安装了 Python，可通过打开【终端】/【Terminal】输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 --version</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br></pre></td></tr></table></figure><p>此时会显示 <code>Python 3.12.8</code> 则说明已经配置且加入了环境变量。</p><p>如果没有，则需要前往官网下载。</p><h2 id="1-1-官网下载">1.1 官网下载</h2><ul><li>打开 <a href="https://www.python.org/">Python 官方网站</a></li><li>点击顶部菜单<a href="https://www.python.org/downloads/">Downloads</a> 选择 <a href="https://www.python.org/downloads/macos/">macOS</a></li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738982544046.png" alt=""></p><ul><li>向下滑动，选择版本并下载【推荐】选择 Stable Releases，更为稳定</li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738982629264.png" alt=""></p><blockquote><p>注：macOS 的安装程序以 <code>.pkg</code> 为后缀，例如如果选择图中版本，下载的文件名应该为 <code>python-3.12.9-macos11.pkg</code></p></blockquote><p>或者直接点击我的 <a href="https://cloud-iskage.oss-cn-shanghai.aliyuncs.com/packages/python-3.12.9-macos11.pkg">链接</a> 下载版本为 <code>Python 3.12.9</code></p><h2 id="1-2-运行安装包">1.2 运行安装包</h2><ul><li><p>双击下载的 <code>.pkg</code> 文件（如 <code>python-3.12.9-macos11.pkg</code>）</p></li><li><p>macOS 的下载不需要配置，会自动推荐到环境变量，故一直默认即可</p></li></ul><h2 id="1-3-验证是否安装成功">1.3 验证是否安装成功</h2><p>打开【终端】输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 --version</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br></pre></td></tr></table></figure><p>此时会显示 <code>Python 3.12.9</code> 则说明成功</p><h1>2 Windows 系统安装 Python</h1><h2 id="2-1-官网下载">2.1 官网下载</h2><ul><li>打开 <a href="https://www.python.org/">Python 官方网站</a></li><li>点击顶部菜单<a href="https://www.python.org/downloads/">Downloads</a> 选择 <a href="https://www.python.org/downloads/windows/">Windows</a></li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738983514762.png" alt=""></p><ul><li>向下滑动，选择版本并下载【推荐】选择 Stable Releases，更为稳定</li></ul><p>注意查看自己的电脑配置，选择合适的安装包，可以前往 <a href="https://support.microsoft.com/zh-cn/topic/%E7%A1%AE%E5%AE%9A%E6%82%A8%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%BF%90%E8%A1%8C%E7%9A%84%E6%98%AF-32-%E4%BD%8D%E7%89%88%E6%9C%AC%E8%BF%98%E6%98%AF-64-%E4%BD%8D%E7%89%88%E6%9C%AC%E7%9A%84-windows-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-1b03ca69-ac5e-4b04-827b-c0c47145944b">微软 Microsoft 官网</a>查询</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738983582659.png" alt=""></p><blockquote><p>Windows 的安装包为可执行文件，以 <code>.exe</code> 为文件后缀</p></blockquote><p>或者直接点击我的 <a href="https://cloud-iskage.oss-cn-shanghai.aliyuncs.com/packages/python-3.12.9-amd64.exe">链接</a> 下载，版本为 <code>Python 3.12.9</code> 架构为 <code>amd64</code></p><h2 id="2-1-运行安装包">2.1 运行安装包</h2><ul><li>双击下载的 <code>.exe</code> 文件（如 <code>python-3.12.9-amd64.exe</code>）</li></ul><blockquote><p>因为我已经下载安装了 <code>Python 3.12</code> 故后文使用 <code>Python 3.13</code> 展示安装和配置流程，二者的安装和配置没有区别</p></blockquote><ul><li>【注意】一定要勾选 【Add python.exe to PATH】以增加环境变量</li><li>可以直接点击【Install Now】</li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/9b2fd9c56f7b91c26f2c6b4c7a82472c_720.png" alt=""></p><blockquote><p>如果需要配置其他可选【Customize installation】：可以配置安装路径，选择性安装一些产品。</p></blockquote><h2 id="2-3-验证是否安装成功">2.3 验证是否安装成功</h2><p>打开【命令提示符 CMD】或 【PowerShell】输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python --version</span><br></pre></td></tr></table></figure><h1>3 在终端使用 Python</h1><p>直接在【Terminal (MacOS)】和【PowerShell (Windows)】输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3</span><br></pre></td></tr></table></figure><p>即可进入终端的 <code>Python</code> 编程环境</p><h1>4 创建虚拟环境</h1><p>利用 <code>Conda</code> 创建虚拟环境，可以更好的管理一些包。详细教程见 <a href="https://blog.iskage.online/posts/2c3265b7.html">Conda 创建虚拟环境全流程</a></p><p>或者直接使用 conda 下载，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n 环境名</span><br><span class="line">conda activate 环境名</span><br><span class="line">conda install pip</span><br></pre></td></tr></table></figure><p>其中 <code>conda install pip</code> 会自动下载 python 和 pip</p>]]></content>
    
    
    <summary type="html">下载并配置 Python，包括了在 mac 电脑和 Windows 电脑的 下载、安装、环境变量配置、使用与常见问题收录</summary>
    
    
    
    <category term="Python" scheme="https://blog.iskage.online/categories/Python/"/>
    
    
    <category term="虚拟环境" scheme="https://blog.iskage.online/tags/%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"/>
    
    <category term="Python" scheme="https://blog.iskage.online/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>深度学习框架 Pytorch 深入学习（1）：Tensor 张量数据结构</title>
    <link href="https://blog.iskage.online/posts/652f5539.html"/>
    <id>https://blog.iskage.online/posts/652f5539.html</id>
    <published>2025-02-07T12:27:00.000Z</published>
    <updated>2025-02-08T03:30:21.888Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch-搭建神经网络（1）Tensor-张量数据结构"><a href="#Pytorch-搭建神经网络（1）Tensor-张量数据结构" class="headerlink" title="Pytorch 搭建神经网络（1）Tensor 张量数据结构"></a>Pytorch 搭建神经网络（1）Tensor 张量数据结构</h1><p>基于<a href="https://book.douban.com/subject/27624483/">《深度学习框架 Pytorch 入门与实践》陈云</a> </p><p>参考 <a href="https://github.com/chenyuntc/pytorch-book">Github 的 pytorch-book 项目</a></p><p>参考 <a href="https://github.com/zergtant/pytorch-handbook">GitHub 的 pytorch-handbook 项目</a></p><p>参考 <a href="https://www.deepseek.com/">DeepSeek</a> 整理补充</p><hr><p>首先，检查 <code>Pytorch</code> 是否安装</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2.2</span><span class="number">.2</span></span><br></pre></td></tr></table></figure><p><code>Tensor</code> 是可以理解为一个类似 <code>Numpy</code> 中的高维数组。</p><h2 id="1-创建"><a href="#1-创建" class="headerlink" title="1 创建"></a>1 创建</h2><ul><li><code>torch.Tensor()</code> 分配空间</li></ul><p>生成维度2x3的张量，并未赋值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br></pre></td></tr></table></figure><ul><li><code>torch.tensor()</code> 需要具体的值进行创建</li></ul><p>输入具体的值，直接生成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.<span class="built_in">float</span>)  <span class="comment"># dtype 指定类型</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[1., 2., 3.],</span></span><br><span class="line"><span class="comment">#         [4., 5., 6.]])</span></span><br></pre></td></tr></table></figure><ul><li><code>torch.rand()</code> 使用正态分布随机初始化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">z = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.1587, 0.9499, 0.1939],</span></span><br><span class="line"><span class="comment">#         [0.9741, 0.9309, 0.7463]])</span></span><br></pre></td></tr></table></figure><h2 id="2-查看形状"><a href="#2-查看形状" class="headerlink" title="2 查看形状"></a>2 查看形状</h2><ul><li>调用方法 <code>.shape</code> 或 <code>.size()</code>查看张量形状/维度</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="comment"># torch.Size([2, 3])</span></span><br></pre></td></tr></table></figure><ul><li>产看具体某个维度数（例如列数）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.size()[<span class="number">1</span>])  <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(x.size(<span class="number">1</span>))   <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(x.shape[<span class="number">1</span>])  <span class="comment"># 3</span></span><br></pre></td></tr></table></figure><h2 id="3-加法"><a href="#3-加法" class="headerlink" title="3 加法"></a>3 加法</h2><ul><li>使用<code>+</code> 作加法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]], dtype=torch.<span class="built_in">float</span>)  <span class="comment"># 或者 torch.ones(2, 3)</span></span><br><span class="line">y = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[2., 3., 4.],</span></span><br><span class="line"><span class="comment">#         [5., 6., 7.]])</span></span><br></pre></td></tr></table></figure><blockquote><p>这种加法不改变 <code>x, y</code> 的值</p></blockquote><ul><li>使用 <code>torch.add(x, y)</code> 作加法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = torch.Tensor(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># 先分配好一个空间，不赋值</span></span><br><span class="line">torch.add(x, y, out=z)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[2., 3., 4.],</span></span><br><span class="line"><span class="comment">#         [5., 6., 7.]])</span></span><br></pre></td></tr></table></figure><blockquote><p>这种加法不改变 <code>x, y</code> 的值</p></blockquote><ul><li>调用方法 <code>.add()</code> 和 <code>.add_()</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(y.add(x))  <span class="comment"># y 不变</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y.add_(x)) <span class="comment"># y 变为 x + y</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><blockquote><p>增加了 <code>_</code> 的方法会进行替换操作</p></blockquote><h2 id="4-索引"><a href="#4-索引" class="headerlink" title="4 索引"></a>4 索引</h2><p>Tensor 的索引操作与 NumPy 类似</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 索引</span></span><br><span class="line">x = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor([[0.3479, 0.8074, 0.2170],</span></span><br><span class="line"><span class="comment">#         [0.3419, 0.9281, 0.1364]])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>])  <span class="comment"># tensor([0.8074, 0.9281]) # 行全取，列取第一列（从0计数）</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">1</span>, :])  <span class="comment"># tensor([0.3419, 0.9281, 0.1364])  # 列全取，行取第一行（从0计数）</span></span><br></pre></td></tr></table></figure><h2 id="5-和-Numpy-的转换"><a href="#5-和-Numpy-的转换" class="headerlink" title="5 和 Numpy 的转换"></a>5 和 <code>Numpy</code> 的转换</h2><ul><li><code>torch.Tensor -&gt; numpy.ndarray</code></li></ul><p>使用 <code>.numpy()</code> 方法从Tensor变为numpy.ndarray</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy 相互转换</span></span><br><span class="line">x = torch.ones(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">y = x.numpy()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x)<span class="comment"># tensor([[1., 1., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x))<span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line"><span class="built_in">print</span>(y)<span class="comment"># [[1. 1. 1.]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(y))  <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br></pre></td></tr></table></figure><ul><li><code>numpy.ndarray -&gt; torch.Tensor</code></li></ul><p>使用 <code>torch.from_numpy()</code> 函数从numpy.ndarray变为Tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">y = np.ones((<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">x = torch.from_numpy(y)</span><br><span class="line"><span class="built_in">print</span>(y)        <span class="comment"># [[1. 1. 1.]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(y))  <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line"><span class="built_in">print</span>(x)<span class="comment"># tensor([[1., 1., 1.]], dtype=torch.float64)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br></pre></td></tr></table></figure><ul><li>共享内存，通过上面方式转换后，<code>x, y</code> 是共享内存的，tenor改变，numpy.ndarray也改变</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x)  <span class="comment"># tensor([[1., 1., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(y)  <span class="comment"># [[1. 1. 1.]]</span></span><br><span class="line">temp = torch.rand(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">x.add_(temp)</span><br><span class="line"><span class="built_in">print</span>(x)  <span class="comment"># tensor([[1.5567, 1.5514, 1.0607]])</span></span><br><span class="line"><span class="built_in">print</span>(y)  <span class="comment"># [[1.5567319 1.5514015 1.0607271]]</span></span><br></pre></td></tr></table></figure><h2 id="6-零维度张量-标量"><a href="#6-零维度张量-标量" class="headerlink" title="6 零维度张量/标量"></a>6 零维度张量/标量</h2><p>Tensor 数据类型中维度为 <code>0</code> 称为标量（注意，虽然维度为0，但仍然不是 <code>int</code> 或是 <code>float</code> 这些一般 Python 数据类型）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scaler = torch.tensor(<span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(scaler)          <span class="comment"># tensor(9)</span></span><br><span class="line"><span class="built_in">print</span>(scaler.shape)   <span class="comment"># torch.Size([])</span></span><br></pre></td></tr></table></figure><p>如果想获得一般 Python 数据类型，可以使用方法 <code>.item()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(scaler.item())        <span class="comment"># 9</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(scaler.item()))   <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt; 变为 int</span></span><br></pre></td></tr></table></figure><blockquote><p>注意区分 0 维度标量和 1 维度张量</p><p>但是针对 1 维度张量，也可以使用 <code>.item()</code> 方法 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vector = torch.tensor([<span class="number">9</span>])</span><br><span class="line">scaler = torch.tensor(<span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(vector)  <span class="comment"># tensor([9])</span></span><br><span class="line"><span class="built_in">print</span>(vector.shape)  <span class="comment"># torch.Size([1])</span></span><br><span class="line"><span class="built_in">print</span>(scaler)        <span class="comment"># tensor(9)</span></span><br><span class="line"><span class="built_in">print</span>(scaler.shape)  <span class="comment"># torch.Size([])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对 1 维度张量，也可以使用 `.item()` 方法 </span></span><br><span class="line"><span class="built_in">print</span>(vector.item()) <span class="comment"># 9</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(vector.item()))  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt; 变为 int</span></span><br></pre></td></tr></table></figure><h2 id="7-张量间的复制-detach-clone"><a href="#7-张量间的复制-detach-clone" class="headerlink" title="7 张量间的复制 .detach() .clone()"></a>7 张量间的复制 <code>.detach()</code> <code>.clone()</code></h2><ul><li><code>.clone()</code> 不共享内存，二者互不影响</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">old = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">new = old.clone()  <span class="comment"># 不共享内存，二者互不影响</span></span><br><span class="line">new[<span class="number">0</span>] = <span class="number">233</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(old)</span><br><span class="line"><span class="comment"># tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#          [4, 5, 6]]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(new)</span><br><span class="line"><span class="comment"># tensor([[233, 233, 233],</span></span><br><span class="line"><span class="comment">#         [  4,   5,   6]])</span></span><br></pre></td></tr></table></figure><ul><li><code>.detach()</code> 共享内存，一者变则全变</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">old = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">new = old.detach() <span class="comment"># 共享内存，一者变则全变</span></span><br><span class="line">new[<span class="number">0</span>] = <span class="number">233</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(old)</span><br><span class="line"><span class="comment"># tensor([[233, 233, 233],</span></span><br><span class="line"><span class="comment">#         [  4,   5,   6]])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(new)</span><br><span class="line"><span class="comment"># tensor([[233, 233, 233],</span></span><br><span class="line"><span class="comment">#         [  4,   5,   6]])</span></span><br></pre></td></tr></table></figure><h2 id="8-维度转变"><a href="#8-维度转变" class="headerlink" title="8 维度转变"></a>8 维度转变</h2><p>PyTorch提供了许多维度变换方式：<code>view, reshape, permute, transpose</code></p><h3 id="8-1-维度交换-permute-transpose"><a href="#8-1-维度交换-permute-transpose" class="headerlink" title="8.1 维度交换 permute transpose"></a>8.1 维度交换 <code>permute</code> <code>transpose</code></h3><p>使用 <code>permute</code> <code>transpose</code> 对张量维度进行交换，例如维度为 2x3x4x5 ，希望变为 3x2x5x4，可以</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">previous = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)  <span class="comment"># .randn 标准正态</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># permute</span></span><br><span class="line">new1 = previous.permute((<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>))  <span class="comment"># 填入一个元组，元组里的每个数字对应原来张量的维度序号，新的维度为(第1维度,第0维度,第3维度,第2维度)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># transpose</span></span><br><span class="line">new2 = previous.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># 第0维度和第1维度交换</span></span><br><span class="line">new2 = new2.transpose(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># 第2维度和第3维度交换</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(new1.shape)  <span class="comment"># torch.Size([3, 2, 5, 4])</span></span><br><span class="line"><span class="built_in">print</span>(new2.shape)  <span class="comment"># torch.Size([3, 2, 5, 4])</span></span><br></pre></td></tr></table></figure><h3 id="8-2-维度变换-view-reshape"><a href="#8-2-维度变换-view-reshape" class="headerlink" title="8.2 维度变换 view reshape"></a>8.2 维度变换 <code>view</code> <code>reshape</code></h3><ul><li><code>reshape</code> 无要求，可直接使用</li><li><code>view</code>只能用于内存中连续存储的 Tensor</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">previous = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意总维度要正确：例如 2x3x4x5 = 120 = 6x20</span></span><br><span class="line">new1 = previous.reshape(-<span class="number">1</span>, <span class="number">10</span>)  <span class="comment"># -1 表示自动计算维度，reshape 和 view 均可使用</span></span><br><span class="line">new2 = previous.view(<span class="number">6</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(new1.shape)  <span class="comment"># torch.Size([12, 10])</span></span><br><span class="line"><span class="built_in">print</span>(new2.shape)  <span class="comment"># torch.Size([6, 20])</span></span><br></pre></td></tr></table></figure><blockquote><p>如果经过了 <code>permute</code> <code>transpose</code> 维度交换，则需要先连续化内存，使用 <code>.contiguous()</code></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">previous = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 内存不连续</span></span><br><span class="line">new = previous.permute((<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>))  <span class="comment"># torch.Size([3, 2, 5, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 连续化然后变换维度</span></span><br><span class="line">current = new.contiguous().view(<span class="number">6</span>, <span class="number">20</span>)</span><br><span class="line"><span class="comment"># current = new.reshape(6, 20)  # 或者直接 reshape</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(current.shape)  <span class="comment"># torch.Size([6, 20])</span></span><br></pre></td></tr></table></figure><h3 id="8-3-维度压缩、扩展、拼接-squeeze-unsqueeze-cat"><a href="#8-3-维度压缩、扩展、拼接-squeeze-unsqueeze-cat" class="headerlink" title="8.3 维度压缩、扩展、拼接 squeeze unsqueeze cat"></a>8.3 维度压缩、扩展、拼接 <code>squeeze</code> <code>unsqueeze</code> <code>cat</code></h3><ul><li>维度压缩 <code>torch.squeeze()</code></li></ul><p><code>torch.squeeze(input, dim=None)</code> 用于移除张量中大小为1的维度。如果不指定 <code>dim</code>，则会移除所有大小为1的维度；如果指定了 <code>dim</code>，则只会移除该维度（如果该维度大小为1）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 压缩</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># torch.Size([1, 3, 1, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除所有大小为1的维度</span></span><br><span class="line">y = torch.squeeze(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)   <span class="comment"># torch.Size([3, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只移除第2个维度（索引从0开始）</span></span><br><span class="line">z = torch.squeeze(x, dim=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(z.shape)  <span class="comment"># torch.Size([1, 3, 2])</span></span><br></pre></td></tr></table></figure><ul><li>维度扩展 <code>torch.unsqueeze()</code></li></ul><p><code>torch.unsqueeze(input, dim)</code> 用于在指定的位置插入一个大小为1的维度。<code>dim</code> 参数指定了新维度插入的位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">2</span>)    <span class="comment"># torch.Size([3, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第0维插入一个大小为1的维度</span></span><br><span class="line">y = torch.unsqueeze(x, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(y.shape)           <span class="comment"># torch.Size([1, 3, 2])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第1维插入一个大小为1的维度</span></span><br><span class="line">z = torch.unsqueeze(x, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(z.shape)           <span class="comment"># torch.Size([3, 1, 2])</span></span><br></pre></td></tr></table></figure><ul><li>维度拼接 <code>torch.cat()</code></li></ul><p><code>torch.cat(tensors, dim=d)</code> 用于在指定的维度上拼接多个张量。所有张量在除了 <code>dim=d</code> 维度之外的其它维度上<strong>必须具有相同的形状</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">4</span>)       <span class="comment"># dim=1 维度均为 3 故可以在dim=0上拼接</span></span><br><span class="line">y = torch.randn(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第0维上拼接</span></span><br><span class="line">z = torch.cat((x, y), dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(z.shape)           <span class="comment"># torch.Size([3, 4])  # 1 + 2 = 3</span></span><br></pre></td></tr></table></figure><h2 id="9-GPU-加速"><a href="#9-GPU-加速" class="headerlink" title="9 GPU 加速"></a>9 GPU 加速</h2><p>利用 GPU 的并行计算能力能加速模型的计算。Pytorch提供了2种将tensor推至GPU的方法。</p><ul><li><code>.cuda()</code> 方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = x.cuda()  <span class="comment"># 将 Tensor 转移到默认的 GPU</span></span><br></pre></td></tr></table></figure><ul><li><code>.to(device)</code> 方法【推荐】</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">x = x.to(device)  <span class="comment"># 将 Tensor 转移到指定的设备，如果失败则继续在CPU计算</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(torch.cuda.is_available)</span><br><span class="line">&lt;function is_available at <span class="number">0x0000024B63F50720</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>device</span><br><span class="line">device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.to(device)</span><br><span class="line">tensor([[-<span class="number">0.1888</span>,  <span class="number">0.0827</span>, -<span class="number">1.2929</span>],</span><br><span class="line">        [ <span class="number">2.1295</span>,  <span class="number">1.6174</span>, -<span class="number">1.4917</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Pytorch 神经网络框架，本文介绍 Tensor 张量数据结构，包括：如何创建、对张量的操作和 GPU 加速等。</summary>
    
    
    
    <category term="深度学习 Pytorch 完整教程" scheme="https://blog.iskage.online/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch-%E5%AE%8C%E6%95%B4%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="AI" scheme="https://blog.iskage.online/tags/AI/"/>
    
    <category term="Python" scheme="https://blog.iskage.online/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>SQL 自学笔记（3）运算符与 SQL 的正则表达式</title>
    <link href="https://blog.iskage.online/posts/a7d82995.html"/>
    <id>https://blog.iskage.online/posts/a7d82995.html</id>
    <published>2025-02-07T07:51:00.000Z</published>
    <updated>2025-02-07T07:52:35.342Z</updated>
    
    <content type="html"><![CDATA[<p>本文笔记根据<a href="https://www.bilibili.com/video/BV1iq4y1u7vj/?share_source=copy_web&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2">【b站 尚硅谷-宋红康 MySQL 课程】</a>整理</p><h2 id="1-算术运算符">1 算术运算符</h2><p>包括：<code>+, -, *, /, %</code> 加、减、乘、除、取模运算</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a <span class="operator">+</span> b;</span><br><span class="line"><span class="keyword">SELECT</span> a <span class="operator">-</span> b;</span><br><span class="line"><span class="keyword">SELECT</span> a <span class="operator">*</span> b;</span><br><span class="line"><span class="keyword">SELECT</span> a <span class="operator">/</span> b; <span class="comment">-- or SELECT a DIV b;</span></span><br><span class="line"><span class="keyword">SELECT</span> a <span class="operator">%</span> b; <span class="comment">-- or SELECT a MOD b;</span></span><br></pre></td></tr></table></figure><h3 id="1-1-加减运算">1.1 加减运算 <code>+ -</code></h3><ul><li><p><code>int + int = int</code> <code>float + float = float</code> 但 <code>int + float = float</code></p></li><li><p>只针对数值型计算，若为字符串，则先转化，若转换失败，则按照 <code>0</code> 计算</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">100</span> <span class="operator">+</span> <span class="number">1.2</span>, <span class="string">&#x27;3&#x27;</span> <span class="operator">+</span> <span class="number">4</span>, <span class="string">&#x27;3&#x27;</span> <span class="operator">+</span> <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;a&#x27;</span> <span class="operator">+</span> <span class="number">3</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+---------+-----------+---------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">100</span> <span class="operator">+</span> <span class="number">1.2</span> <span class="operator">|</span> <span class="string">&#x27;3&#x27;</span> <span class="operator">+</span> <span class="number">4</span> <span class="operator">|</span> <span class="string">&#x27;3&#x27;</span> <span class="operator">+</span> <span class="string">&#x27;4&#x27;</span> <span class="operator">|</span> <span class="string">&#x27;a&#x27;</span> <span class="operator">+</span> <span class="number">3</span> <span class="operator">|</span> <span class="comment">-- &#x27;a&#x27; 转换失败，按照 0 计算</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+---------+-----------+---------+</span></span><br><span class="line"><span class="operator">|</span>     <span class="number">101.2</span> <span class="operator">|</span>       <span class="number">7</span> <span class="operator">|</span>         <span class="number">7</span> <span class="operator">|</span>       <span class="number">3</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+---------+-----------+---------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span>, <span class="number">1</span> warning (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h3 id="1-2-乘除运算">1.2 乘除运算 <code>* /</code></h3><ul><li><code>int * int = int</code> <code>int * float = float</code> 而 <code>/</code> 除法最后一定是 <code>float</code> 类型结果</li><li><code>number / 0 = NULL</code> 除以 <code>0</code> 则为 <code>NULL</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">100</span> <span class="operator">*</span> <span class="number">1</span>, <span class="number">100</span> <span class="operator">*</span> <span class="number">1.0</span>, <span class="number">100</span> <span class="operator">/</span> <span class="number">1</span>, <span class="number">100</span> DIV <span class="number">0</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------+-----------+----------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">100</span> <span class="operator">*</span> <span class="number">1</span> <span class="operator">|</span> <span class="number">100</span> <span class="operator">*</span> <span class="number">1.0</span> <span class="operator">|</span> <span class="number">100</span> <span class="operator">/</span> <span class="number">1</span>  <span class="operator">|</span> <span class="number">100</span> DIV <span class="number">0</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------+-----------+----------+-----------+</span></span><br><span class="line"><span class="operator">|</span>     <span class="number">100</span> <span class="operator">|</span>     <span class="number">100.0</span> <span class="operator">|</span> <span class="number">100.0000</span> <span class="operator">|</span>      <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------+-----------+----------+-----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span>, <span class="number">1</span> warning (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h3 id="1-3-取模运算">1.3 取模运算 <code>%</code></h3><ul><li><code>A % B</code> 表示 <code>A</code> 除以 <code>B</code> 的余数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">100</span> <span class="operator">%</span> <span class="number">20</span>, <span class="number">100</span> <span class="operator">%</span> <span class="number">99</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------+----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">100</span> <span class="operator">%</span> <span class="number">20</span> <span class="operator">|</span> <span class="number">100</span> <span class="operator">%</span> <span class="number">99</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+----------+</span></span><br><span class="line"><span class="operator">|</span>        <span class="number">0</span> <span class="operator">|</span>        <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h2 id="2-比较运算符">2 比较运算符</h2><p>比较结果为<strong>真</strong>返回<code>1</code></p><p>比较结果为<strong>假</strong>返回<code>0</code></p><p><strong>其他情况</strong>返回 <code>NULL</code></p><p>包括：<code>=, &lt;=&gt;, &lt;&gt;(!=), &lt;, &lt;=, &gt;, &gt;=</code> 等号、安全等号、不等号、小于、小于等于、大于、大于等于</p><h3 id="2-1-等号比较">2.1 等号比较 <code>=</code></h3><p>判断等号两边的值、字符串或表达式是否相等，如果相等则返回 <code>1</code>，不相等则返回 <code>0</code></p><ul><li>字符串与字符串比较，值与值比较</li><li>若为字符串与值比较，则将字符串转换为值 <code>int = str -&gt; int = int(str)</code> 注意：转换失败则为 <code>0</code></li><li>有一个为 <code>NULL</code> 则返回 <code>NULL</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">2</span> <span class="operator">=</span> <span class="number">2</span>, <span class="number">2</span> <span class="operator">=</span> <span class="number">3</span>, <span class="number">2</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;bc&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;bc&#x27;</span>, <span class="number">0</span> <span class="operator">=</span> <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+---------+-------------+---------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span> <span class="operator">=</span> <span class="number">2</span> <span class="operator">|</span> <span class="number">2</span> <span class="operator">=</span> <span class="number">3</span> <span class="operator">|</span> <span class="number">2</span> <span class="operator">=</span> <span class="string">&#x27;2&#x27;</span> <span class="operator">|</span> <span class="string">&#x27;bc&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;bc&#x27;</span> <span class="operator">|</span> <span class="number">0</span> <span class="operator">=</span> <span class="string">&#x27;a&#x27;</span> <span class="operator">|</span> <span class="comment">-- &#x27;a&#x27; 转换失败则为 0</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+---------+-------------+---------+</span></span><br><span class="line"><span class="operator">|</span>     <span class="number">1</span> <span class="operator">|</span>     <span class="number">0</span> <span class="operator">|</span>       <span class="number">1</span> <span class="operator">|</span>           <span class="number">1</span> <span class="operator">|</span>       <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+---------+-------------+---------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span>, <span class="number">1</span> warning (<span class="number">0.02</span> sec)</span><br></pre></td></tr></table></figure><h3 id="2-2-安全等号">2.2 安全等号 <code>&lt;=&gt;</code></h3><p><code>&lt;=&gt;</code> 会把 <code>NULL</code> 当作一个特殊的元素，参与比较，不再返回 <code>NULL</code> 。其他返回结果与等于运算符相同。</p><ul><li>安全等号 <code>&lt;=&gt;</code> 两边的操作数的值都为<code>NULL</code> 时，返回的结果为 <code>1</code> 。</li><li>安全等号 <code>&lt;=&gt;</code> 一边为 <code>NULL</code> ，一边不为 <code>NULL</code> 时，返回 <code>0</code> 。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">1</span> <span class="operator">=</span> <span class="keyword">NULL</span>, <span class="string">&#x27;&#x27;</span> <span class="operator">=</span> <span class="keyword">NULL</span>, <span class="keyword">NULL</span> <span class="operator">=</span> <span class="keyword">NULL</span>, <span class="number">1</span> <span class="operator">&lt;=&gt;</span> <span class="keyword">NULL</span>, <span class="keyword">NULL</span> <span class="operator">&lt;=&gt;</span> <span class="keyword">NULL</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-----------+-------------+------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span> <span class="operator">=</span> <span class="keyword">NULL</span> <span class="operator">|</span> <span class="string">&#x27;&#x27;</span> <span class="operator">=</span> <span class="keyword">NULL</span> <span class="operator">|</span> <span class="keyword">NULL</span> <span class="operator">=</span> <span class="keyword">NULL</span> <span class="operator">|</span> <span class="number">1</span> <span class="operator">&lt;=&gt;</span> <span class="keyword">NULL</span> <span class="operator">|</span> <span class="keyword">NULL</span> <span class="operator">&lt;=&gt;</span> <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-----------+-------------+------------+---------------+</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">NULL</span> <span class="operator">|</span>      <span class="keyword">NULL</span> <span class="operator">|</span>        <span class="keyword">NULL</span> <span class="operator">|</span>          <span class="number">0</span> <span class="operator">|</span>             <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-----------+-------------+------------+---------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h3 id="2-3-不等于比较-or">2.3 不等于比较 <code>!= or &lt;&gt;</code></h3><p>判断等号两边的值、字符串或表达式是否不相等，如果不相等则返回 <code>1</code>，相等则返回 <code>0</code></p><ul><li>与等号比较类似，面对 <code>NULL</code> 无论比较结果如何，最后均返回 <code>NULL</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">2</span> <span class="operator">!=</span> <span class="number">2</span>, <span class="number">2</span> <span class="operator">!=</span> <span class="number">3</span>, <span class="number">2</span> <span class="operator">!=</span> <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;bc&#x27;</span> <span class="operator">!=</span> <span class="string">&#x27;bc&#x27;</span>, <span class="number">0</span> <span class="operator">!=</span><span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;&#x27;</span> <span class="operator">!=</span> <span class="keyword">NULL</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+----------+--------------+---------+------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span> <span class="operator">!=</span> <span class="number">2</span> <span class="operator">|</span> <span class="number">2</span> <span class="operator">!=</span> <span class="number">3</span> <span class="operator">|</span> <span class="number">2</span> <span class="operator">!=</span> <span class="string">&#x27;2&#x27;</span> <span class="operator">|</span> <span class="string">&#x27;bc&#x27;</span> <span class="operator">!=</span> <span class="string">&#x27;bc&#x27;</span> <span class="operator">|</span> <span class="number">0</span> <span class="operator">!=</span><span class="string">&#x27;a&#x27;</span> <span class="operator">|</span> <span class="string">&#x27;&#x27;</span> <span class="operator">!=</span> <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+----------+--------------+---------+------------+</span></span><br><span class="line"><span class="operator">|</span>      <span class="number">0</span> <span class="operator">|</span>      <span class="number">1</span> <span class="operator">|</span>        <span class="number">0</span> <span class="operator">|</span>            <span class="number">0</span> <span class="operator">|</span>       <span class="number">0</span> <span class="operator">|</span>       <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+----------+--------------+---------+------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span>, <span class="number">1</span> warning (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><h3 id="2-4-大于小于">2.4 大于小于 <code>&gt;, &lt;, &gt;=, &lt;=</code></h3><p>性质和等于 <code>=</code>，不等于 <code>!= or &lt;&gt;</code> 相同</p><ul><li>比较结果为<strong>真</strong>返回<code>1</code> 比较结果为<strong>假</strong>返回<code>0</code> <strong>其他情况</strong>返回 <code>NULL</code></li><li>面对 <code>NULL</code> 无论比较结果如何，最后均返回 <code>NULL</code></li></ul><h2 id="3-其他类型运算符">3 其他类型运算符</h2><h3 id="3-1-空运算符-ISNULL-IS-NULL">3.1 空运算符 <code>ISNULL, IS NULL</code></h3><p>是否为 <code>NULL</code> ，如果为 <code>NULL</code> 则返回 <code>1</code> ，否则返回 <code>0</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">IS</span> <span class="keyword">NULL</span>, ISNULL(<span class="keyword">NULL</span>), ISNULL(<span class="string">&#x27;a&#x27;</span>), <span class="number">1</span> <span class="keyword">IS</span> <span class="keyword">NULL</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+--------------+-------------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span> <span class="keyword">IS</span> <span class="keyword">NULL</span> <span class="operator">|</span> ISNULL(<span class="keyword">NULL</span>) <span class="operator">|</span> ISNULL(<span class="string">&#x27;a&#x27;</span>) <span class="operator">|</span> <span class="number">1</span> <span class="keyword">IS</span> <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+--------------+-------------+-----------+</span></span><br><span class="line"><span class="operator">|</span>            <span class="number">1</span> <span class="operator">|</span>            <span class="number">1</span> <span class="operator">|</span>           <span class="number">0</span> <span class="operator">|</span>         <span class="number">0</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+--------------+-------------+-----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><blockquote><p>结合之前的安全等号，从表中寻找 <code>NULL</code> 有如下方法 （注：等号，不等号，大小于符号无法处理空值问题）</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> email, commission_pct <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> commission_pct <span class="keyword">IS</span> <span class="keyword">NULL</span>;</span><br><span class="line"><span class="keyword">SELECT</span> email, commission_pct <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> commission_pct <span class="operator">&lt;=&gt;</span> <span class="keyword">NULL</span>;</span><br><span class="line"><span class="keyword">SELECT</span> email, commission_pct <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> ISNULL(commission_pct);</span><br><span class="line"><span class="keyword">SELECT</span> email, commission_pct <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> commission_pct <span class="operator">=</span> <span class="keyword">NULL</span>; <span class="comment">-- 查询失败</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738900575560.png" alt=""></p><h3 id="3-2-非空运算符-IS-NOT-NULL">3.2 非空运算符 <code>IS NOT NULL</code></h3><p>是否为 <code>NULL</code> ，如果为 <code>NULL</code> 则返回 <code>0</code> ，否则返回 <code>1</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">IS</span> <span class="keyword">NOT NULL</span>, <span class="string">&#x27;a&#x27;</span> <span class="keyword">IS</span> <span class="keyword">NOT NULL</span>, <span class="number">1</span> <span class="keyword">IS</span> <span class="keyword">NOT NULL</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span> <span class="keyword">IS</span> <span class="keyword">NOT NULL</span> <span class="operator">|</span> <span class="string">&#x27;a&#x27;</span> <span class="keyword">IS</span> <span class="keyword">NOT NULL</span> <span class="operator">|</span> <span class="number">1</span> <span class="keyword">IS</span> <span class="keyword">NOT NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span>                <span class="number">0</span> <span class="operator">|</span>               <span class="number">1</span> <span class="operator">|</span>             <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+-----------------+---------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h3 id="3-3-最小值运算符-LEAST">3.3 最小值运算符 <code>LEAST()</code></h3><p>返回 <code>LEAST(a1, a2, ..., ai, ..., an)</code> 中的最小值</p><ul><li>当 <code>ai</code> 是 <code>int</code> 或者 <code>float</code> 时，<code>LEAST</code> 将返回其中最小的值</li><li>当 <code>ai</code> 为 <code>str</code> 时，返回字母表中顺序最靠前的字符</li><li>当列表中有 <code>NULL</code> 时，返回值为 <code>NULL</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> LEAST(<span class="number">1</span>, <span class="number">2</span>), LEAST(<span class="number">1.2</span>, <span class="number">3</span>), LEAST(<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;ac&#x27;</span>), LEAST(<span class="number">1</span>, <span class="keyword">NULL</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+---------------+------------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> LEAST(<span class="number">1</span>, <span class="number">2</span>) <span class="operator">|</span> LEAST(<span class="number">1.2</span>, <span class="number">3</span>) <span class="operator">|</span> LEAST(<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;ac&#x27;</span>) <span class="operator">|</span> LEAST(<span class="number">1</span>, <span class="keyword">NULL</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+---------------+------------------+----------------+</span></span><br><span class="line"><span class="operator">|</span>           <span class="number">1</span> <span class="operator">|</span>           <span class="number">1.2</span> <span class="operator">|</span> ac               <span class="operator">|</span>           <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+---------------+------------------+----------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h3 id="3-4-最大值运算符-GREATEST">3.4 最大值运算符 <code>GREATEST()</code></h3><p>返回 <code>GREATEST(a1, a2, ..., ai, ..., an)</code> 中的最大值</p><ul><li>当 <code>ai</code> 是 <code>int</code> 或者 <code>float</code> 时，<code>GREATEST</code> 将返回其中最大的值</li><li>当 <code>ai</code> 为 <code>str</code> 时，返回字母表中顺序最靠后的字符</li><li>当列表中有 <code>NULL</code> 时，返回值为 <code>NULL</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> GREATEST(<span class="number">1</span>, <span class="number">2</span>), GREATEST(<span class="number">1.2</span>, <span class="number">3</span>), GREATEST(<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;ac&#x27;</span>), GREATEST(<span class="number">1</span>, <span class="keyword">NULL</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+------------------+---------------------+-------------------+</span></span><br><span class="line"><span class="operator">|</span> GREATEST(<span class="number">1</span>, <span class="number">2</span>) <span class="operator">|</span> GREATEST(<span class="number">1.2</span>, <span class="number">3</span>) <span class="operator">|</span> GREATEST(<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;ac&#x27;</span>) <span class="operator">|</span> GREATEST(<span class="number">1</span>, <span class="keyword">NULL</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+------------------+---------------------+-------------------+</span></span><br><span class="line"><span class="operator">|</span>              <span class="number">2</span> <span class="operator">|</span>              <span class="number">3.0</span> <span class="operator">|</span> b                   <span class="operator">|</span>              <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+------------------+---------------------+-------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><h3 id="3-5-判断是否区间中的值-BETWEEN-AND">3.5 判断是否区间中的值 <code>BETWEEN ... AND ...</code></h3><p>寻找位于 <code>[a, b]</code> 中与否，即大于等于 <code>a</code> 小于等于 <code>b</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列名<span class="number">1</span>, 列名<span class="number">2</span></span><br><span class="line"><span class="keyword">FROM</span> 表名</span><br><span class="line"><span class="keyword">WHERE</span> 列名 <span class="keyword">BETWEEN</span> a <span class="keyword">AND</span> b;</span><br></pre></td></tr></table></figure><p>例如，是则为 <code>1</code> 否则返回 <code>0</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">5</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">AND</span> <span class="number">10</span>, <span class="number">0.4</span> <span class="keyword">BETWEEN</span> <span class="number">3</span> <span class="keyword">AND</span> <span class="number">6</span>, <span class="string">&#x27;x&#x27;</span> <span class="keyword">BETWEEN</span> <span class="string">&#x27;a&#x27;</span> <span class="keyword">AND</span> <span class="string">&#x27;z&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+---------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span> <span class="keyword">BETWEEN</span> <span class="number">1</span> <span class="keyword">AND</span> <span class="number">10</span> <span class="operator">|</span> <span class="number">0.4</span> <span class="keyword">BETWEEN</span> <span class="number">3</span> <span class="keyword">AND</span> <span class="number">6</span> <span class="operator">|</span> <span class="string">&#x27;x&#x27;</span> <span class="keyword">BETWEEN</span> <span class="string">&#x27;a&#x27;</span> <span class="keyword">AND</span> <span class="string">&#x27;z&#x27;</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+---------------------+-------------------------+</span></span><br><span class="line"><span class="operator">|</span>                  <span class="number">1</span> <span class="operator">|</span>                   <span class="number">0</span> <span class="operator">|</span>                       <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+---------------------+-------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>完整的查表运用</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> last_name, salary</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">WHERE</span> salary <span class="keyword">BETWEEN</span> <span class="number">2500</span> <span class="keyword">AND</span> <span class="number">3500</span>;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738901321018.png" alt=""></p><h3 id="3-6-属于运算符-IN">3.6 属于运算符 <code>IN</code></h3><p>判断元素是否属于列表中，类似的，只要存在 <code>NULL</code> 则返回值为 <code>NULL</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">1</span> <span class="keyword">IN</span> (<span class="number">2</span>, <span class="number">3</span>), <span class="string">&#x27;a&#x27;</span> <span class="keyword">IN</span> (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>), <span class="keyword">NULL</span> <span class="keyword">IN</span> (<span class="string">&#x27;&#x27;</span>, <span class="number">2</span>), <span class="string">&#x27;a&#x27;</span> <span class="keyword">IN</span> (<span class="string">&#x27;a&#x27;</span>, <span class="keyword">NULL</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+-------------------+-----------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span> <span class="keyword">IN</span> (<span class="number">2</span>, <span class="number">3</span>) <span class="operator">|</span> <span class="string">&#x27;a&#x27;</span> <span class="keyword">IN</span> (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>) <span class="operator">|</span> <span class="keyword">NULL</span> <span class="keyword">IN</span> (<span class="string">&#x27;&#x27;</span>, <span class="number">2</span>) <span class="operator">|</span> <span class="string">&#x27;a&#x27;</span> <span class="keyword">IN</span> (<span class="string">&#x27;a&#x27;</span>, <span class="keyword">NULL</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+-------------------+-----------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span>           <span class="number">0</span> <span class="operator">|</span>                 <span class="number">1</span> <span class="operator">|</span>            <span class="keyword">NULL</span> <span class="operator">|</span>                  <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+-------------------+-----------------+--------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h3 id="3-7-不属于运算符-NOT-IN">3.7 不属于运算符 <code>NOT IN</code></h3><p>判断元素是否不属于列表，类似的，只要存在 <code>NULL</code> 则返回值为 <code>NULL</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">1</span> <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="number">2</span>, <span class="number">3</span>), <span class="keyword">NULL</span> <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="number">1</span>, <span class="number">2</span>), <span class="string">&#x27;a&#x27;</span> <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="number">1</span>, <span class="keyword">NULL</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+--------------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span> <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="number">2</span>, <span class="number">3</span>) <span class="operator">|</span> <span class="keyword">NULL</span> <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="number">1</span>, <span class="number">2</span>) <span class="operator">|</span> <span class="string">&#x27;a&#x27;</span> <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="number">1</span>, <span class="keyword">NULL</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+--------------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span>               <span class="number">1</span> <span class="operator">|</span>               <span class="keyword">NULL</span> <span class="operator">|</span>                 <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+--------------------+----------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span>, <span class="number">1</span> warning (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><h3 id="3-8-模糊匹配-LIKE">3.8 模糊匹配 <code>LIKE</code></h3><p>模糊匹配<u>字符串</u>，如果满足条件则返回 <code>1</code> ，否则返回 <code>0</code> 。同样，遇 <code>NULL</code> 则 <code>NULL</code></p><ul><li>通配符：</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;%&#x27;</span> <span class="comment">-- 匹配 0 个或多个字符。</span></span><br><span class="line"><span class="string">&#x27;_&#x27;</span> <span class="comment">-- 只能匹配一个字符。</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> first_name</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">WHERE</span> first_name <span class="keyword">LIKE</span> <span class="string">&#x27;S%&#x27;</span>;  <span class="comment">-- 匹配 first_name 字段以 S 开头的字符串</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738903197390.png" alt=""></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> last_name</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">WHERE</span> last_name <span class="keyword">LIKE</span> <span class="string">&#x27;_o%&#x27;</span>;  <span class="comment">-- 匹配 last_name 字段形如 &#x27;一个字母 + o&#x27; 开头的字符串</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738903166457.png" alt=""></p><h3 id="3-9-利用-ESCAPE-处理特殊字符">3.9 利用 <code>ESCAPE</code> 处理特殊字符</h3><p>当需要查询的元素中包含特殊字符，如 <code>%, _, $</code> 等。可以使用 <code>\</code> 或其他字符（除 <code>\</code> 其他字符需要 <code>ESCAPE</code> 指明为转义字符）处理。</p><ul><li>例如：我们需要查询形如 <code>增长50%左右</code> ，即匹配以任意字符开头，接着是 <code>50%</code>，最后是任意字符的字符串。所以可以采取 <code>'%50\%%'</code> 或 <code>'%50$%%' ESCAPE '$'</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> job_id</span><br><span class="line"><span class="keyword">FROM</span> jobs</span><br><span class="line"><span class="keyword">WHERE</span> job_id <span class="keyword">LIKE</span> <span class="string">&#x27;IT$_%&#x27;</span> <span class="keyword">ESCAPE</span> <span class="string">&#x27;$&#x27;</span>;  <span class="comment">-- 指定 &#x27;$&#x27; 为转义字符，&#x27;$&#x27; 后的 &#x27;_&#x27; 为普通字符</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> job_id</span><br><span class="line"><span class="keyword">FROM</span> jobs</span><br><span class="line"><span class="keyword">WHERE</span> job_id <span class="keyword">LIKE</span> <span class="string">&#x27;IT@_%&#x27;</span> <span class="keyword">ESCAPE</span> <span class="string">&#x27;@&#x27;</span>;  <span class="comment">-- 指定 &#x27;@&#x27; 为转义字符，&#x27;@&#x27; 后的 &#x27;_&#x27; 为普通字符</span></span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">---------+</span></span><br><span class="line"><span class="operator">|</span> job_id  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------+</span></span><br><span class="line"><span class="operator">|</span> IT_PROG <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h2 id="4-逻辑运算符">4 逻辑运算符</h2><p>逻辑运算符的返回结果为 <code>1, 0, NULL</code></p><p>逻辑运算符大多符合直觉，并不复杂</p><h3 id="4-1-非-运算符-NOT-或">4.1 非 运算符 <code>NOT</code> 或 <code>!</code></h3><ul><li><code>!0 -&gt; 1</code> 值为0时返回1</li><li><code>!非0 -&gt; 0</code> 值为非0值时返回0</li><li><code>!NULL -&gt; NULL</code> 值为NULL时，返回NULL</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="keyword">NOT</span> <span class="number">1</span>, <span class="keyword">NOT</span> <span class="number">0</span>, <span class="keyword">NOT</span>(<span class="number">1</span><span class="operator">+</span><span class="number">1</span>), <span class="keyword">NOT</span> <span class="operator">!</span><span class="number">1</span>, <span class="keyword">NOT NULL</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+----------+--------+----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NOT</span> <span class="number">1</span> <span class="operator">|</span> <span class="keyword">NOT</span> <span class="number">0</span> <span class="operator">|</span> <span class="keyword">NOT</span>(<span class="number">1</span><span class="operator">+</span><span class="number">1</span>) <span class="operator">|</span> <span class="keyword">NOT</span> <span class="operator">!</span><span class="number">1</span> <span class="operator">|</span> <span class="keyword">NOT NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+----------+--------+----------+</span></span><br><span class="line"><span class="operator">|</span>     <span class="number">0</span> <span class="operator">|</span>     <span class="number">1</span> <span class="operator">|</span>        <span class="number">0</span> <span class="operator">|</span>      <span class="number">1</span> <span class="operator">|</span>     <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+----------+--------+----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span>, <span class="number">1</span> warning (<span class="number">0.02</span> sec)</span><br></pre></td></tr></table></figure><h3 id="4-2-与-运算符-AND-或">4.2 与 运算符 <code>AND</code> 或 <code>&amp;&amp;</code></h3><ul><li><p><code>非0非NULL &amp;&amp; 非0非NULL -&gt; 1</code> 所有值均为非0值，并且都不为NULL时，返回1</p></li><li><p><code>0 &amp;&amp; ... -&gt; 0</code>一个值或者多个值为0时则返回0</p></li><li><p><code>NULL &amp;&amp; ... -&gt; NULL</code>否则返回NULL</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">2</span> <span class="keyword">AND</span> <span class="number">3</span>, <span class="number">0</span> <span class="keyword">AND</span> <span class="number">1</span>, <span class="number">0</span> <span class="keyword">AND</span> <span class="keyword">NULL</span>, <span class="number">1</span> <span class="keyword">AND</span> <span class="keyword">NULL</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------+---------+------------+------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span> <span class="keyword">AND</span> <span class="number">3</span> <span class="operator">|</span> <span class="number">0</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="operator">|</span> <span class="number">0</span> <span class="keyword">AND</span> <span class="keyword">NULL</span> <span class="operator">|</span> <span class="number">1</span> <span class="keyword">AND</span> <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------+---------+------------+------------+</span></span><br><span class="line"><span class="operator">|</span>       <span class="number">1</span> <span class="operator">|</span>       <span class="number">0</span> <span class="operator">|</span>          <span class="number">0</span> <span class="operator">|</span>       <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------+---------+------------+------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><p>例如：查表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, job_id, salary</span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">WHERE</span> salary <span class="operator">&gt;=</span><span class="number">10000</span></span><br><span class="line">  <span class="keyword">AND</span> job_id <span class="keyword">LIKE</span> <span class="string">&#x27;%MAN%&#x27;</span>;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738906355775.png" alt=""></p><h3 id="4-3-或-运算符-OR-或">4.3 或 运算符 <code>OR</code> 或 <code>||</code></h3><ul><li><code>非0非NULL || 非NULL -&gt; 1</code> 值都不为NULL，并且任何一个值为非0值时，则返回1</li><li><code>0 || 0 -&gt; 0</code> 否则返回0；</li><li><code>非0非NULL || NULL -&gt; 1</code> 当一个值为NULL，并且另一个值为非0值时，返回1，否则为NULL</li><li><code>NULL || NULL</code> 当两个值都为NULL时，返回NULL</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="number">2</span> <span class="keyword">OR</span> <span class="number">3</span>, <span class="number">2</span> <span class="keyword">OR</span> <span class="number">0</span>, <span class="number">3</span> <span class="keyword">OR</span> <span class="keyword">NULL</span>, <span class="number">0</span> <span class="operator">||</span> <span class="keyword">NULL</span>, <span class="keyword">NULL</span> <span class="operator">||</span> <span class="keyword">NULL</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+-----------+-----------+--------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span> <span class="keyword">OR</span> <span class="number">3</span> <span class="operator">|</span> <span class="number">2</span> <span class="keyword">OR</span> <span class="number">0</span> <span class="operator">|</span> <span class="number">3</span> <span class="keyword">OR</span> <span class="keyword">NULL</span> <span class="operator">|</span> <span class="number">0</span> <span class="operator">||</span> <span class="keyword">NULL</span> <span class="operator">|</span> <span class="keyword">NULL</span> <span class="operator">||</span> <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+-----------+-----------+--------------+</span></span><br><span class="line"><span class="operator">|</span>      <span class="number">1</span> <span class="operator">|</span>      <span class="number">1</span> <span class="operator">|</span>         <span class="number">1</span> <span class="operator">|</span>      <span class="keyword">NULL</span> <span class="operator">|</span>         <span class="keyword">NULL</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+-----------+-----------+--------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span>, <span class="number">2</span> warnings (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><blockquote><p>注：<code>AND</code> 的优先级高于 <code>OR</code> ，先处理与逻辑，再考虑或逻辑</p></blockquote><h3 id="4-4-异或-运算符-XOR">4.4 异或 运算符 <code>XOR</code></h3><ul><li>存在 <code>NULL</code>时，则返回 <code>NULL</code></li><li>都<code>非0非NULL</code>，则返回 <code>0</code></li><li><code>0 XOR 0 -&gt; 0</code> 都 <code>0</code> 则返回 <code>0</code></li><li>一个为 <code>0</code>，另一个 <code>非0非NULL</code> ，则返回 <code>1</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="keyword">NULL</span> XOR <span class="number">1</span>, <span class="number">2</span> XOR <span class="number">3</span>, <span class="number">0</span> XOR <span class="number">0</span>, <span class="number">0</span> XOR <span class="number">4</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------+---------+---------+---------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span> XOR <span class="number">1</span> <span class="operator">|</span> <span class="number">2</span> XOR <span class="number">3</span> <span class="operator">|</span> <span class="number">0</span> XOR <span class="number">0</span> <span class="operator">|</span> <span class="number">0</span> XOR <span class="number">4</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+---------+---------+---------+</span></span><br><span class="line"><span class="operator">|</span>       <span class="keyword">NULL</span> <span class="operator">|</span>       <span class="number">0</span> <span class="operator">|</span>       <span class="number">0</span> <span class="operator">|</span>       <span class="number">1</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+---------+---------+---------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h2 id="5-位运算符">5 位运算符</h2><p>位运算符会先将操作数变成<strong>二进制数</strong>，然后进行位运算，最后将计算结果从二进制变回<strong>十进制数</strong></p><p>位运算在所有语言中均类似，不过多介绍。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a <span class="operator">&amp;</span> b;   <span class="comment">-- 按位与</span></span><br><span class="line"><span class="keyword">SELECT</span> a <span class="operator">|</span> b;   <span class="comment">-- 按位或</span></span><br><span class="line"><span class="keyword">SELECT</span> a <span class="operator">^</span> b;   <span class="comment">-- 按位异或</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">~</span> a;     <span class="comment">-- 按位取反</span></span><br><span class="line"><span class="keyword">SELECT</span> a <span class="operator">&gt;&gt;</span> <span class="number">2</span>;  <span class="comment">-- 按位右移</span></span><br><span class="line"><span class="keyword">SELECT</span> a <span class="operator">&lt;&lt;</span> <span class="number">2</span>;  <span class="comment">-- 按位左移</span></span><br></pre></td></tr></table></figure><h2 id="6-优先级">6 优先级</h2><p>优先级越高，越先运算</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738911612091.png" alt="优先级由高到低 1-&gt;8"></p><blockquote><p>建议使用多个计算式，均使用 <code>()</code></p></blockquote><h2 id="7-正则表达式">7 正则表达式</h2><p>正则表达式在各种语言中均类似，下面介绍 <code>SQL</code> 语言中 <code>REGEXP</code> 的使用</p><h3 id="7-1-REGEXP-的使用">7.1 <code>REGEXP</code> 的使用</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列名<span class="number">1</span>, 列名<span class="number">2</span>, 列名<span class="number">3</span>, ...</span><br><span class="line"><span class="keyword">FROM</span> 表名</span><br><span class="line"><span class="keyword">WHERE</span> 列名 REGEXP 匹配条件(正则表达式);</span><br></pre></td></tr></table></figure><h3 id="7-2-常见正则表达式">7.2 常见正则表达式</h3><h4 id="7-2-1-匹配起始字符">7.2.1 匹配起始字符 <code>^</code></h4><ul><li>表达式 <code>^K</code> 以 <code>K</code> 起始的字符</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> last_name REGEXP <span class="string">&#x27;^K&#x27;</span>;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738912114205.png" alt=""></p><h4 id="7-2-2-匹配结尾字符">7.2.2 匹配结尾字符 <code>$</code></h4><ul><li>表达式 <code>t$</code> 以 <code>t</code> 结尾的字符</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> first_name REGEXP <span class="string">&#x27;t$&#x27;</span>;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738912256452.png" alt=""></p><h4 id="7-2-3-匹配单个字符">7.2.3 匹配单个字符 <code>.</code></h4><ul><li>表达式 <code>a.b</code> 包含字母 <code>a</code> 和 <code>b</code> 且 <code>a</code> 和 <code>b</code> 之间只有一个字符</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> last_name REGEXP <span class="string">&#x27;a.b&#x27;</span>;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738912435878.png" alt=""></p><h4 id="7-2-4-匹配多个字符">7.2.4 匹配多个字符 <code>+</code> <code>*</code></h4><ul><li>表达式 <code>ba+</code> 包含字符串 <code>...ba...</code> 其中 <code>+</code> 要求多个 <code>a</code></li></ul><blockquote><p><code>+</code> 要求 <strong>1次或多次</strong></p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> last_name REGEXP <span class="string">&#x27;^ba+&#x27;</span>; <span class="comment">-- 包含 ba... ^b 要求必须以 b 开头</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738912757500.png" alt=""></p><ul><li>表达式 <code>b*an</code> 包含 <code>...b...an</code> 其中 <code>*</code> 要求多个 <code>b</code></li></ul><blockquote><p><code>*</code> 要求 <strong>0次或多次</strong></p></blockquote><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738913126980.png" alt=""></p><h4 id="7-2-5-匹配特定字符串">7.2.5 匹配特定字符串 <code>''</code></h4><ul><li>表达式 <code>'un'</code> 包含 <code>un</code> 即可</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> last_name REGEXP <span class="string">&#x27;un&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>注：可以使用逻辑运算符</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> last_name REGEXP <span class="string">&#x27;un|am&#x27;</span>;  <span class="comment">-- 包含 un 或 am</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738913338905.png" alt=""></p><h4 id="7-2-6-匹配集合中任一元素">7.2.6 匹配集合中任一元素 <code>[]</code></h4><ul><li>表达式 <code>[orz]</code> 包含<code>o</code> <code>r</code> <code>z</code> 任一一个即可</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> last_name REGEXP <span class="string">&#x27;[orz]&#x27;</span>;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738913543623.png" alt=""></p><h4 id="7-2-7-匹配完全由集合外的元素构成">7.2.7 匹配完全由集合外的元素构成 <code>[^]</code></h4><ul><li>表达式 <code>[^A-Y]</code> 表示只有当元素是有A-Y以外的元素构成时才被查询到。注意：大小写敏感。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> email REGEXP <span class="string">&#x27;[^A-Y]&#x27;</span>; <span class="comment">-- 查询存在 小写字母 或 Z</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738914108381.png" alt=""></p><h4 id="7-2-8-使用-n-或者-n-m-来指定字符串连续出现的次数">7.2.8 使用 <code>&#123;n,&#125;</code> 或者 <code>&#123;n,m&#125;</code> 来指定字符串连续出现的次数</h4><ul><li>表达式 <code>a&#123;2,&#125;</code> 表示字母 <code>a</code> 连续出现至少2次</li><li>表达式 <code>a&#123;2,4&#125;</code> 表示字母 <code>a</code> 连续出现最少2次，最多不能超过4次</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> salary <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> salary REGEXP <span class="string">&#x27;0&#123;2,3&#125;&#x27;</span>; <span class="comment">-- 连续 2-3 个 0</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738914541747.png" alt=""></p>]]></content>
    
    
    <summary type="html">本文详细介绍 数据库查询语言 SQL 的各种运算符：算术、比较、逻辑、位，以及正则表达式在查询语言中的应用</summary>
    
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/tags/MySQL/"/>
    
    <category term="SQL" scheme="https://blog.iskage.online/tags/SQL/"/>
    
    <category term="数据库" scheme="https://blog.iskage.online/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>SQL 自学笔记（2）SELECT 语句</title>
    <link href="https://blog.iskage.online/posts/151eef00.html"/>
    <id>https://blog.iskage.online/posts/151eef00.html</id>
    <published>2025-02-06T09:52:00.000Z</published>
    <updated>2025-02-06T09:54:37.408Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SELECT语句"><a href="#SELECT语句" class="headerlink" title="SELECT语句"></a>SELECT语句</h2><p>本文笔记根据<a href="https://www.bilibili.com/video/BV1iq4y1u7vj/?share_source=copy_web&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2">【b站 尚硅谷-宋红康 MySQL 课程】</a>整理</p><h2 id="1-SQL-语言"><a href="#1-SQL-语言" class="headerlink" title="1 SQL 语言"></a>1 SQL 语言</h2><h3 id="1-1-书写规则"><a href="#1-1-书写规则" class="headerlink" title="1.1 书写规则"></a>1.1 书写规则</h3><ul><li>一行或多行</li><li>以 <code>;</code> 分隔</li><li>字符串型 和 日期时间类型 的数据使用 <code>&#39;&#39;</code> 表示</li><li>列的别名使用 <code>&quot;&quot;</code> 表示</li><li>建议关键字、函数名大写</li></ul><h3 id="1-2-注释"><a href="#1-2-注释" class="headerlink" title="1.2 注释"></a>1.2 注释</h3><p>单行注释</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 注释文字 (MySQL特有)</span><br><span class="line"><span class="comment">-- 注释文字</span></span><br></pre></td></tr></table></figure><p>多行注释</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 注释文字 */</span></span><br></pre></td></tr></table></figure><h3 id="1-3-数据导入-运行-sql文件"><a href="#1-3-数据导入-运行-sql文件" class="headerlink" title="1.3 数据导入 (运行.sql文件)"></a>1.3 数据导入 (运行<code>.sql</code>文件)</h3><p>登陆 <code>MySQL</code> 后，输入指令</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> source xx.sql  <span class="comment">-- 输入路径</span></span><br></pre></td></tr></table></figure><p>在学习语句前，先下载 <a href="https://cloud-iskage.oss-cn-shanghai.aliyuncs.com/docs/mydb.sql">mydb.sql 文件</a> ，之后使用数据导入的方式，运行 <code>mydb.sql</code> 创建数据库 <code>atguigudb</code> 以便后续操作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> source .<span class="operator">/</span>xxx<span class="operator">/</span>mydb.sql  <span class="comment">-- mac 电脑</span></span><br><span class="line">mysql<span class="operator">&gt;</span> source C:\Users\Username\xxx\mydb.sql  <span class="comment">-- windows 电脑</span></span><br></pre></td></tr></table></figure><blockquote><p>该文件来自 <a href="https://www.bilibili.com/video/BV1iq4y1u7vj/?share_source=copy_web&amp;vd_source=67ce2d561f3b6dc9d7cff375959101a2">【b站 尚硅谷-宋红康 MySQL 课程】</a></p></blockquote><h2 id="2-基本-SELECT-语句"><a href="#2-基本-SELECT-语句" class="headerlink" title="2 基本 SELECT 语句"></a>2 基本 SELECT 语句</h2><blockquote><p>首先先进入刚刚创建的数据库 <code>atguigudb</code> </p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> USE atguigudb;</span><br></pre></td></tr></table></figure><h3 id="2-1-SELECT-…-FROM-…"><a href="#2-1-SELECT-…-FROM-…" class="headerlink" title="2.1 SELECT … FROM …"></a>2.1 SELECT … FROM …</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 选择哪些列</span><br><span class="line"><span class="keyword">FROM</span> 从哪个表中选择</span><br></pre></td></tr></table></figure><ul><li>选择所有列</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> employees;</span><br></pre></td></tr></table></figure><ul><li>选择表中特定的列</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> employee_id, last_name, salary</span><br><span class="line"><span class="keyword">FROM</span> employees;</span><br></pre></td></tr></table></figure><h3 id="2-2-列的别名"><a href="#2-2-列的别名" class="headerlink" title="2.2 列的别名"></a>2.2 列的别名</h3><p>可以在列名后紧跟别名，也可以使用关键字 <code>AS</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    employee_id emp_id,</span><br><span class="line">    last_name <span class="keyword">AS</span> lname,</span><br><span class="line">    department_id &quot;dept_id&quot;</span><br><span class="line"><span class="keyword">FROM</span> employees;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738834180268.png" alt=""></p><p>也可以对整列进行计算</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    employee_id emp_id,</span><br><span class="line">    salary <span class="operator">*</span> <span class="number">12</span> &quot;annual salary&quot;  <span class="comment">-- 对 salary 整列计算</span></span><br><span class="line"><span class="keyword">FROM</span> employees;</span><br></pre></td></tr></table></figure><h3 id="2-3-去除重复行"><a href="#2-3-去除重复行" class="headerlink" title="2.3 去除重复行"></a>2.3 去除重复行</h3><p>默认情况不会去除重复行</p><p>在 <code>SELECT</code> 语句中增加关键字 <code>DISTINCT</code> 即可返回去除重复行后的结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> department_id</span><br><span class="line"><span class="keyword">FROM</span> employees;</span><br><span class="line"><span class="comment">-- 12 rows</span></span><br></pre></td></tr></table></figure><blockquote><p>注：若 <code>DISTINCT</code> 后存在多个字段，只要两行之间有某个字段不同，则不认为重复</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> department_id, salary</span><br><span class="line"><span class="keyword">FROM</span> employees;</span><br><span class="line"><span class="comment">-- 74 rows</span></span><br></pre></td></tr></table></figure><p>例如：对 <code>department_id, salary</code> 去重，虽然部门只有 <code>12</code> 个，但因为 <code>salary</code> 仍有一些不同，故认为并不重复。</p><h3 id="2-4-空值-null-参与计算"><a href="#2-4-空值-null-参与计算" class="headerlink" title="2.4 空值 null 参与计算"></a>2.4 空值 <code>null</code> 参与计算</h3><p>遇到 <code>null</code> 值，运算的结果都为 <code>null</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> employee_id, salary &quot;月工资&quot;, salary <span class="operator">*</span> (<span class="number">1</span> <span class="operator">+</span> commission_pct) <span class="operator">*</span> <span class="number">12</span> &quot;年工资&quot;, commission_pct</span><br><span class="line"><span class="keyword">FROM</span> employees;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738834695370.png" alt=""></p><h3 id="2-5-着重号-解决关键字冲突"><a href="#2-5-着重号-解决关键字冲突" class="headerlink" title="2.5 着重号 ``  解决关键字冲突"></a>2.5 着重号 <code>`` </code> 解决关键字冲突</h3><p>如果不得不使用关键字命名，则可以使用 <code>`` </code> </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">order</span>;</span><br><span class="line"><span class="comment">-- 报错</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> `<span class="keyword">order</span>`;</span><br><span class="line"><span class="comment">-- 正确</span></span><br></pre></td></tr></table></figure><h3 id="2-6-增加常值列"><a href="#2-6-增加常值列" class="headerlink" title="2.6 增加常值列"></a>2.6 增加常值列</h3><p>增加一列命名为 <code>temp_const</code> （字符串用 <code>&#39;&#39;</code>）用数字 <code>123</code> 填充</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="number">123</span> <span class="keyword">AS</span> <span class="string">&#x27;temp_const&#x27;</span>, employee_id, last_name, salary</span><br><span class="line"><span class="keyword">FROM</span> employees;</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738835171143.png" alt=""></p><h2 id="3-查询表结构"><a href="#3-查询表结构" class="headerlink" title="3 查询表结构"></a>3 查询表结构</h2><p>使用 <code>DESCRIBE</code> 或 <code>DESC</code> 命令，查询表结构</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DESCRIBE</span> employees;</span><br><span class="line"><span class="keyword">DESC</span> employees;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">+</span><span class="comment">----------------+-------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field          <span class="operator">|</span> Type        <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span> <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+-------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> employee_id    <span class="operator">|</span> <span class="type">int</span>         <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span> <span class="number">0</span>       <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> first_name     <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">20</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> last_name      <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">25</span>) <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> email          <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">25</span>) <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> UNI <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> phone_number   <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">20</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hire_date      <span class="operator">|</span> <span class="type">date</span>        <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> job_id         <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">10</span>) <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> salary         <span class="operator">|</span> <span class="keyword">double</span>(<span class="number">8</span>,<span class="number">2</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> commission_pct <span class="operator">|</span> <span class="keyword">double</span>(<span class="number">2</span>,<span class="number">2</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> manager_id     <span class="operator">|</span> <span class="type">int</span>         <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> department_id  <span class="operator">|</span> <span class="type">int</span>         <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+-------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="number">11</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br></pre></td></tr></table></figure><h2 id="4-WHERE-过滤条件"><a href="#4-WHERE-过滤条件" class="headerlink" title="4 WHERE 过滤条件"></a>4 <code>WHERE</code> 过滤条件</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列名<span class="number">1</span>, 列名<span class="number">2</span>, 列名<span class="number">3</span></span><br><span class="line"><span class="keyword">FROM</span> 表名</span><br><span class="line"><span class="keyword">WHERE</span> 过滤条件</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">where</span> last_name <span class="operator">=</span> <span class="string">&#x27;King&#x27;</span> <span class="keyword">and</span> first_name <span class="operator">=</span> <span class="string">&#x27;Steven&#x27;</span>; <span class="comment">-- 过滤条件</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">WHERE</span> department_id <span class="operator">=</span> <span class="number">90</span>; <span class="comment">-- 过滤条件</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本文详细介绍 SELECT 语句的基本使用</summary>
    
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/tags/MySQL/"/>
    
    <category term="SQL" scheme="https://blog.iskage.online/tags/SQL/"/>
    
    <category term="数据库" scheme="https://blog.iskage.online/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Conda 创建虚拟环境全流程</title>
    <link href="https://blog.iskage.online/posts/2c3265b7.html"/>
    <id>https://blog.iskage.online/posts/2c3265b7.html</id>
    <published>2025-02-06T04:11:00.000Z</published>
    <updated>2025-02-08T03:33:48.593Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-下载-Miniconda">1 下载 Miniconda</h2><p>前往<a href="https://www.anaconda.com/download">官网 https://www.anaconda.com/download</a>下载，需要提交邮箱，验证下载。</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738812320257.png" alt=""></p><p>提交成功后，滑至底部，选择 [Miniconda Installers] ，因为它相比 [Anaconda] 更为精简。</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/976615dc8f3b26708fe5dd4d932c3b4d_720.png" alt=""></p><blockquote><p>注：需要提前下载 <code>Python</code> ，可前去 <a href="https://www.python.org/downloads/">Python 官网</a> 下载</p></blockquote><h2 id="2-安装-Miniconda">2 安装 Miniconda</h2><p>先点击 [Next] ，而后点击 [I Agree] ，而后选择 [All Users] ，这将为电脑的所有用户安装【推荐】，然后选择 [Next]</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/91d3b0a80dc3f5f623c41fb89a44a4f7.png" alt=""></p><p>之后，选择安装路径，可以根据自己的情况来，但要记住安装路径，因为之后要添加【环境变量】</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/12b49c1abc490ecd4ca7a9fe63c6436f_720.png" alt=""></p><p>之后，三项【全部勾选】后安装即可。</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/f6f67e69c09729405b20ae42d4cbf2c2_720.png" alt=""></p><h2 id="3-配置环境变量">3 配置环境变量</h2><p>右键 [此电脑] ，选择 [属性] ，打开 [系统] 窗口，点击 [高级系统设置]</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/4ba1d54ca6830505fae9a8bc721ad076_720.png" alt=""></p><p>打开 [系统属性] ，选择 [高级] 选项，点击 [环境变量] 在 [环境变量] 窗口，选择 [path] 点击 [编辑] ，在 [编辑环境变量] 窗口中，将 Miniconda 如图所示配置。</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/bf764dcc6b2bd4b546cf7fa43530c07e.png" alt=""></p><p>配置【三个路径】一般默认情况为，根据之前安装时的具体路径选择</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">C:\ProgramData\miniconda3</span><br><span class="line">C:\ProgramData\miniconda3\Scripts</span><br><span class="line">C:\ProgramData\miniconda3\Library\bin</span><br></pre></td></tr></table></figure><blockquote><p>注：一定要点击确认，进行保存</p></blockquote><h2 id="4-Conda-的使用">4 Conda 的使用</h2><p>首先检查是否已经成功配置，在命令行/终端中输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; conda -V</span><br><span class="line">conda <span class="number">24.11</span><span class="number">.1</span></span><br></pre></td></tr></table></figure><p>如果输出版本号，则说明环境配置成功，可正常使用</p><h2 id="5-创建环境">5 创建环境</h2><p>创建虚拟环境</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n 环境名 python=<span class="number">3.12</span></span><br></pre></td></tr></table></figure><p><code>环境名</code> 可自己定义，<code>python=x.x</code> 可省略</p><blockquote><p>注：必须在全局已经下载安装了 <code>Python</code></p></blockquote><h2 id="6-激活环境">6 激活环境</h2><p>激活已经创建的虚拟环境</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate 环境名</span><br></pre></td></tr></table></figure><p>退出激活的虚拟环境</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure><h2 id="7-常见问题：Run-‘conda-init’-before-‘conda-activate’">7 常见问题：Run ‘conda init’ before ‘conda activate’</h2><p>当执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate 环境名</span><br></pre></td></tr></table></figure><p>时会遇到报错</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CondaError: Run <span class="string">&#x27;conda init&#x27;</span> before <span class="string">&#x27;conda activate&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>[!NOTE]</p><p>解决方法</p></blockquote><p>使用【管理员身份】打开【终端】，执行命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda init</span><br></pre></td></tr></table></figure><p>然后回到重新执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate 环境名</span><br></pre></td></tr></table></figure><blockquote><p>如若仍然失败，则检查是否为下文的常见问题：无法加载文件 profile.ps1</p></blockquote><h2 id="8-常见问题：无法加载文件-profile-ps1">8 常见问题：无法加载文件 profile.ps1</h2><p>如果【已经使用管理员身份执行完】命令 <code>conda init</code> ，在重新打开终端时出现以下报错</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">. : 无法加载文件 C:\Users\Username\Documents\WindowsPowerShell\profile.ps1，因为在此系统上禁止运行脚本。有关详细信息，请参阅 https:/go.microsoft.com/fwlink/?LinkID=<span class="number">135170</span> 中的 about_Execution_Policies。</span><br><span class="line">所在位置 行:<span class="number">1</span> 字符: <span class="number">3</span></span><br><span class="line">+ . <span class="string">&#x27;C:\Users\15056\Documents\WindowsPowerShell\profile.ps1&#x27;</span></span><br><span class="line">+   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">    + CategoryInfo          : SecurityError: (:) []，PSSecurityException</span><br><span class="line">    + FullyQualifiedErrorId : UnauthorizedAccess</span><br></pre></td></tr></table></figure><p>则可以【使用管理员身份】打开终端，执行以下命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Set</span>-ExecutionPolicy RemoteSigned -Scope CurrentUser</span><br></pre></td></tr></table></figure><p>而后关闭终端，重新打开</p><p>如果此时命令行前显示 <code>(base)</code> 则说明配置完成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(base) PS C:\...</span><br></pre></td></tr></table></figure><p>此时使用，即可成功激活虚拟环境</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate 环境名</span><br></pre></td></tr></table></figure><h2 id="9-其他-conda-命令">9 其他 conda 命令</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda info --envs  <span class="comment"># 显示所有环境信息</span></span><br><span class="line">conda remove -n 环境名 --<span class="built_in">all</span>  <span class="comment"># 根据环境名删除虚拟环境，删除环境和所有该环境里安装的包</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">利用 Miniconda 更为轻便的搭建虚拟环境，包括了 下载、安装、环境变量配置、使用与常见问题收录</summary>
    
    
    
    <category term="conda" scheme="https://blog.iskage.online/categories/conda/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="conda" scheme="https://blog.iskage.online/tags/conda/"/>
    
    <category term="虚拟环境" scheme="https://blog.iskage.online/tags/%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"/>
    
    <category term="Python" scheme="https://blog.iskage.online/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>SQL 自学笔记（1）如何安装 MySQL</title>
    <link href="https://blog.iskage.online/posts/8b09a768.html"/>
    <id>https://blog.iskage.online/posts/8b09a768.html</id>
    <published>2025-02-05T07:42:00.000Z</published>
    <updated>2025-02-06T09:53:21.759Z</updated>
    
    <content type="html"><![CDATA[<h1>详细教程：如何安装 MySQL</h1><p>MySQL 是数据库管理系统 DSMS 之一，本文介绍如何 <strong>下载、安装、配置、登陆、使用</strong> MySQL</p><h2 id="1-下载">1 下载</h2><ol><li>前往官网：<a href="https://www.mysql.com">https://www.mysql.com</a></li><li>点击 - <a href="https://www.mysql.com/downloads/">Downloads</a></li></ol><p>点击 - <a href="https://dev.mysql.com/downloads/">MySQL Community (GPL) Downloads</a></p><p>社区版免费，且功能近似，适合学习使用。</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738735763767.png" alt=""></p><ol start="3"><li>点击 - <a href="https://dev.mysql.com/downloads/mysql/">MySQL Community Server</a></li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738735912017.png" alt=""></p><ol start="4"><li>选择版本</li></ol><p>推荐使用 <code>8.0.x</code> 版本，相比旧版本 <code>5.0.x</code> 更为完善。点击 - <a href="https://dev.mysql.com/downloads/windows/installer/8.0.html">Go to Download Page</a></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738736158523.png" alt=""></p><p>选择 <code>.msi</code> 的安装程序。选择图中的程序下载，配有图形化安装流程，支持离线安装。</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738736235034.png" alt=""></p><blockquote><p>注：下载需要注册 ORACLE 甲骨文公司账户</p></blockquote><h2 id="2-安装">2 安装</h2><ol><li>找到文件 <code>mysql-installer-community-8.0.41.0.msi</code> ，双击进入安装。</li><li>在 [Choosing a Setup Type] - 窗口中选择 自定义安装 [Custom]</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/6fb510a37d115e06be126a4e7bde91eb.png" alt=""></p><ol start="3"><li>在 [Select Products] 窗口中，选择产品。从左边选择想要安装的产品，而后显示在右边。</li></ol><blockquote><p>注：如想自定义安装路径，见后。</p></blockquote><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/0dddb331420bff1755778e44b880e51b_720.png" alt=""></p><p>自定义安装路径，则需要选择产品，点击 [Advanced Options]</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/ba921bd680af66e88223a0b7e3720285_720.png" alt=""></p><ol start="4"><li>在 [Installation] 窗口，点击 [Execute] 开始安装</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/f32f54e134c74322b24711806c2996ce_720.png" alt=""></p><p>安装完成点击 [Next]</p><ol start="5"><li>在 [Product Configuration] 窗口，开始配置，点击 [Next]</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/29078465616e9aff29a15b4119d4f07a_720.png" alt=""></p><p>保持默认即可，点击 [Next]</p><p>[Config Type] 为主机类型，[Port: 3306] 为端口号</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/9bcda347f6bbca58df8843f38c178a78_720.png" alt=""></p><ol start="6"><li>在 [Authentication Method] 窗口，可以设置授权方式。默认，直接点击 [Next]</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/f1754354d7e0302717a9c1fc4c528c02_720.png" alt=""></p><ol start="7"><li>在 [Accounts and Roles] 界面设置管理员密码。用于登陆 MySQL 服务</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/dfb3117e767af332e6b2822ff27a0654_720.png" alt=""></p><ol start="8"><li>在 [Windows Service] 界面设置 [服务] ，保持默认即可。</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/d3129dd2ba331425f9e7892a0b81b087_720.png" alt=""></p><ol start="9"><li>在 [Server File Permissions] 中授权，保持默认即可</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/33bcd45652a5178ddd35788211bbfc93_720.png" alt=""></p><ol start="10"><li>在 [Apply Configuration] 点击 [Execute] 执行之前的配置</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/79ce3831a4322e41691869175c54a604_720.png" alt=""></p><p>点击 [Finish] 完成配置</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/e7357753070eaa02ec50024aa28556a2_720.png" alt=""></p><ol start="11"><li>完成安装</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/40579a68dcdadf9f09e6c3c517e83525_720.png" alt=""></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/494e644125931e21cf86124bb5e9d67c.png" alt=""></p><h2 id="3-环境变量">3 环境变量</h2><p>没有配置 MySQL 到环境变量中，终端无法运行 MySQL 命令。</p><ol><li>右键 [此电脑] ，选择 [属性]</li><li>打开 [系统] 窗口，点击 [高级系统设置]</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/4ba1d54ca6830505fae9a8bc721ad076_720.png" alt=""></p><ol start="3"><li>打开 [系统属性] ，选择 [高级] 选项，点击 [环境变量]</li><li>在 [环境变量] 窗口，选择 [path]</li><li>点击 [编辑] ，在 [编辑环境变量] 窗口中，将 MySQL 的 bin 目录添加进去，使用分号 <code>;</code> 划分</li></ol><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/3648923a3ca1b07903f3e18b967dd1f9.png" alt=""></p><p>输入 MySQL 的 bin 目录 (根据自己的目录输入)，如果没有更改默认下载路径，则一般为</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files\MySQL\MySQL Server 8.0\bin</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/e1c8475465d768bc72da4b2b085ae4a1.png" alt=""></p><ol start="6"><li>点击 [确认] ，回到终端尝试使用 MySQL 指令</li></ol><p>检查是否成功，输入</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> mysql <span class="operator">-</span>V</span><br></pre></td></tr></table></figure><p>返回版本信息则代表成功</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files\MySQL\MySQL Server <span class="number">8.0</span>\bin\mysql.exe  Ver <span class="number">8.0</span><span class="number">.41</span> <span class="keyword">for</span> Win64 <span class="keyword">on</span> x86_64 (MySQL Community Server <span class="operator">-</span> GPL)</span><br></pre></td></tr></table></figure><h2 id="4-查看服务">4 查看服务</h2><p>需要启动服务，才能登陆 MySQL 并连接数据库。在之前配置时，已经默认服务启动。</p><p>可以检查一下：右键 [此电脑] 点击 [管理] ，双击 [服务和应用程序] ，双击 [服务] ，找到 MySQL 的服务 （默认情况下为 <code>MySQL80</code>）</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/c94326af2e1953a05b2a0a087f366e8c_720.png" alt=""></p><p>也可以使用终端实现。右键 [终端]/[命令行工具] 使用 <strong>以管理员身份运行</strong> ，而后使用如下命令</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 启动服务 MySQL80 为 MySQL 的服务名</span><br><span class="line"><span class="operator">&gt;</span> net <span class="keyword">start</span> MySQL80</span><br><span class="line"></span><br><span class="line"># 停止服务 MySQL80 为 MySQL 的服务名</span><br><span class="line"><span class="operator">&gt;</span> net stop MySQL80</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/4639db1363ba088fad5f91904369a021_720.png" alt=""></p><h2 id="5-登陆-MySQL">5 登陆 MySQL</h2><p>打开 [终端] 输入</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql <span class="operator">-</span>h 主机名 <span class="operator">-</span>P 端口名 <span class="operator">-</span>u 用户名 <span class="operator">-</span>p密码</span><br></pre></td></tr></table></figure><p>一般而言 主机名为 <code>localhost</code> ，端口名为 <code>3306</code> ，用户名为 <code>root</code> ，密码和 <code>-p</code> 之间不能有空格。</p><p>推荐使用</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql <span class="operator">-</span>u root <span class="operator">-</span>p</span><br></pre></td></tr></table></figure><p>进行登陆，之后输入密码即可登陆。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span> mysql <span class="operator">-</span>u root <span class="operator">-</span>p</span><br><span class="line">Enter password: <span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span><span class="operator">*</span></span><br></pre></td></tr></table></figure><p>登陆成功后为</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">8</span></span><br><span class="line">Server version: <span class="number">8.0</span><span class="number">.41</span> MySQL Community Server <span class="operator">-</span> GPL</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2025</span>, Oracle <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its affiliates.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span><span class="operator">/</span><span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> help. Type <span class="string">&#x27;\c&#x27;</span> <span class="keyword">to</span> clear the <span class="keyword">current</span> input statement.</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure><h2 id="6-简单尝试">6 简单尝试</h2><ol><li>查看已有数据库:</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> sys                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> test               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br></pre></td></tr></table></figure><ol start="2"><li>创建数据库:</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database 数据库名 <span class="keyword">DEFAULT</span> CHARSET utf8 <span class="keyword">COLLATE</span> utf8_general_ci;  </span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database 数据库名</span><br></pre></td></tr></table></figure><ol start="3"><li>删除数据库:</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> database 数据库名;</span><br></pre></td></tr></table></figure><ol start="4"><li>进入数据库:</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">use 数据库名;</span><br></pre></td></tr></table></figure><ol start="5"><li>查看数据库下所有数据表:</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> tables;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> Tables_in_test <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> tb1            <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br></pre></td></tr></table></figure><ol start="6"><li>创建表:</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create table</span> 表名(</span><br><span class="line">  列名 类型,</span><br><span class="line">  列名 类型,</span><br><span class="line">  列名 类型</span><br><span class="line">) <span class="keyword">default</span> charset<span class="operator">=</span>utf8;</span><br></pre></td></tr></table></figure><ul><li>e.g.</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create table</span> tb1(</span><br><span class="line">    id <span class="type">int</span>,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">16</span>),</span><br><span class="line">    age <span class="type">int</span></span><br><span class="line">) <span class="keyword">default</span> charset<span class="operator">=</span>utf8;</span><br></pre></td></tr></table></figure><ul><li>不能为空/允许为空</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create table</span> tb1(</span><br><span class="line">    id <span class="type">int</span>,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">16</span>) <span class="keyword">not null</span>, <span class="comment">-- 不能为空</span></span><br><span class="line">    age <span class="type">int</span> <span class="keyword">null</span>         <span class="comment">-- 可以为空</span></span><br><span class="line">) <span class="keyword">default</span> charset<span class="operator">=</span>utf8;</span><br></pre></td></tr></table></figure><ul><li>默认值</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create table</span> tb1(</span><br><span class="line">    id <span class="type">int</span>,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">16</span>) <span class="keyword">not null</span>,</span><br><span class="line">    age <span class="type">int</span> <span class="keyword">default</span> <span class="number">3</span>        <span class="comment">-- 设置默认为3</span></span><br><span class="line">) <span class="keyword">default</span> charset<span class="operator">=</span>utf8;</span><br></pre></td></tr></table></figure><ul><li>主键</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create table</span> tb1(</span><br><span class="line">    id <span class="type">int</span> <span class="keyword">primary key</span>,       <span class="comment">-- 不能为空，不能重复</span></span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">16</span>),</span><br><span class="line">    age <span class="type">int</span></span><br><span class="line">) <span class="keyword">default</span> charset<span class="operator">=</span>utf8;</span><br></pre></td></tr></table></figure><ul><li>一般设置，主键+自增 <code>标准</code></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create table</span> tb1(</span><br><span class="line">    id <span class="type">int</span> auto_increment <span class="keyword">primary key</span>,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">16</span>),</span><br><span class="line">    age <span class="type">int</span></span><br><span class="line">) <span class="keyword">default</span> charset<span class="operator">=</span>utf8;</span><br></pre></td></tr></table></figure><ol start="7"><li>删除表</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> 表名;</span><br></pre></td></tr></table></figure><ol start="8"><li>展示表信息</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">desc</span> 表名;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> tb1;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------------+------+-----+---------+----------------+</span></span><br><span class="line"><span class="operator">|</span> Field <span class="operator">|</span> Type        <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span> <span class="operator">|</span> Extra          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------------+------+-----+---------+----------------+</span></span><br><span class="line"><span class="operator">|</span> id    <span class="operator">|</span> <span class="type">int</span>         <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span> auto_increment <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">16</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> age   <span class="operator">|</span> <span class="type">int</span>         <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>                <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------------+------+-----+---------+----------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">本文详细介绍如何 下载、安装、配置、登陆、使用 MySQL</summary>
    
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/categories/MySQL/"/>
    
    
    <category term="MySQL" scheme="https://blog.iskage.online/tags/MySQL/"/>
    
    <category term="SQL" scheme="https://blog.iskage.online/tags/SQL/"/>
    
    <category term="数据库" scheme="https://blog.iskage.online/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 搭建神经网络 目录</title>
    <link href="https://blog.iskage.online/posts/8a1b9ca7.html"/>
    <id>https://blog.iskage.online/posts/8a1b9ca7.html</id>
    <published>2025-02-05T07:30:00.000Z</published>
    <updated>2025-02-07T12:18:28.829Z</updated>
    
    <content type="html"><![CDATA[<div class="note info flat"><p><strong>文章为博主自学 Pytorch 整理的笔记。点击链接即可前往对应文章查看。更多信息，可前往本人的<a href="https://github.com/isKage/iskage.github.io">Github库</a>中查询。</strong></p></div><div class="note danger flat"><p><strong>转载请注明出处，要求见文末</strong></p></div><p>Pytorch 官方文档 中文 <a href="https://www.pytorchtutorial.com/docs/">https://www.pytorchtutorial.com/docs/</a></p><p>Pytorch 官方文档 <a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><div class="note no-icon flat"><p><strong>笔记目录</strong></p><p><a href="./3f7272cd.html">1 下载 Pytorch 和数据处理</a></p><p><a href="./ae1c954d.html">2 网络搭建</a></p><p><a href="./119f3166.html">3 损失函数 反向传播 优化器</a></p><p><a href="./bd02f045.html">4 网络调用和保存</a></p><p><a href="./2d229fe6.html">5 完整模型训练</a></p></div>]]></content>
    
    
    <summary type="html">利用 Pytorch 搭建神经网络，已更新完全</summary>
    
    
    
    <category term="Pytorch 入门" scheme="https://blog.iskage.online/categories/Pytorch-%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="AI" scheme="https://blog.iskage.online/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 搭建神经网络（5）完整模型训练</title>
    <link href="https://blog.iskage.online/posts/2d229fe6.html"/>
    <id>https://blog.iskage.online/posts/2d229fe6.html</id>
    <published>2025-02-05T07:27:10.000Z</published>
    <updated>2025-02-07T12:18:29.504Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch-CIFAR10"><a href="#Pytorch-CIFAR10" class="headerlink" title="Pytorch (CIFAR10)"></a>Pytorch (CIFAR10)</h1><p>官方文档 中文 <a href="https://www.pytorchtutorial.com/docs/">https://www.pytorchtutorial.com/docs/</a></p><p>官方文档 <a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><h2 id="1-Structure"><a href="#1-Structure" class="headerlink" title="1. Structure"></a>1. Structure</h2><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240806%E4%B8%8B%E5%8D%8882617716.png" alt=""></p><h2 id="2-Code"><a href="#2-Code" class="headerlink" title="2 Code"></a>2 Code</h2><h3 id="train-model-py"><a href="#train-model-py" class="headerlink" title="train_model.py"></a>train_model.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建神经网络(output_features=10)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Classification10Class</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Classification10Class, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.module = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(in_features=<span class="number">64</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">64</span>),</span><br><span class="line">            nn.Linear(in_features=<span class="number">64</span>, out_features=<span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.module(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证网络正确性</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    classification = Classification10Class()</span><br><span class="line">    <span class="comment"># 按照batch_size=64，channel=3，size=32*32输入，即64张图片，RGB颜色通道，32*32的大小输入</span></span><br><span class="line">    inputs = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    ouputs = classification(inputs)</span><br><span class="line">    <span class="built_in">print</span>(ouputs.shape)</span><br></pre></td></tr></table></figure><h3 id="train-py"><a href="#train-py" class="headerlink" title="train.py"></a>train.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; CIFAR Dataset &quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train_model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&quot;./dataset&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&quot;./dataset&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 获取数据集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># print(&quot;训练数据集长度为 &#123;&#125;&quot;.format(train_data_size))</span></span><br><span class="line"><span class="comment"># print(&quot;测试数据集长度为 &#123;&#125;&quot;.format(test_data_size))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 利用DataLoader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    dataset=train_data,</span><br><span class="line">    batch_size=<span class="number">64</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_dataloader = DataLoader(</span><br><span class="line">    dataset=test_data,</span><br><span class="line">    batch_size=<span class="number">64</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 搭建神经网络 (from train_model.py import *)</span></span><br><span class="line">classification = Classification10Class()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 优化器</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">optimizer = torch.optim.SGD(</span><br><span class="line">    params=classification.parameters(),</span><br><span class="line">    lr=learning_rate,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 设置训练网络的参数</span></span><br><span class="line">total_train_step = <span class="number">0</span>  <span class="comment"># 训练次数</span></span><br><span class="line">total_test_step = <span class="number">0</span>  <span class="comment"># 测试次数</span></span><br><span class="line">epochs = <span class="number">10</span>  <span class="comment"># 训练迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------第 &#123;&#125; 轮训练开始----------&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练步骤</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="comment"># 输入输出</span></span><br><span class="line">        images, targets = data</span><br><span class="line">        outputs = classification(images)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 损失函数</span></span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清零梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数: &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤(不更新参数)</span></span><br><span class="line">    total_test_loss = <span class="number">0</span>  <span class="comment"># 测试集损失累积</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            images, targets = data</span><br><span class="line">            outputs = classification(images)</span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line">            total_test_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;测试集loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br></pre></td></tr></table></figure><h3 id="output"><a href="#output" class="headerlink" title="output"></a>output</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">----------第 1 轮训练开始----------</span><br><span class="line">训练次数: 100, loss: 2.2914345264434814</span><br><span class="line">训练次数: 200, loss: 2.2848589420318604</span><br><span class="line">训练次数: 300, loss: 2.2572102546691895</span><br><span class="line">训练次数: 400, loss: 2.1692259311676025</span><br><span class="line">训练次数: 500, loss: 2.0409679412841797</span><br><span class="line">训练次数: 600, loss: 2.0187602043151855</span><br><span class="line">训练次数: 700, loss: 2.009617567062378</span><br><span class="line">测试集loss: 313.149169921875</span><br><span class="line">----------第 2 轮训练开始----------</span><br><span class="line">训练次数: 800, loss: 1.878823161125183</span><br><span class="line">训练次数: 900, loss: 1.8439174890518188</span><br><span class="line">训练次数: 1000, loss: 1.9330165386199951</span><br><span class="line">训练次数: 1100, loss: 1.9703041315078735</span><br><span class="line">训练次数: 1200, loss: 1.7066203355789185</span><br><span class="line">训练次数: 1300, loss: 1.668871521949768</span><br><span class="line">训练次数: 1400, loss: 1.7355754375457764</span><br><span class="line">训练次数: 1500, loss: 1.7841742038726807</span><br><span class="line">测试集loss: 309.877685546875</span><br><span class="line">----------第 3 轮训练开始----------</span><br><span class="line">训练次数: 1600, loss: 1.7344536781311035</span><br><span class="line">训练次数: 1700, loss: 1.621435284614563</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="3-Visualization-amp-Save"><a href="#3-Visualization-amp-Save" class="headerlink" title="3 Visualization &amp; Save"></a>3 Visualization &amp; Save</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; CIFAR Dataset &quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train_model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&quot;./dataset&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&quot;./dataset&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 获取数据集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># print(&quot;训练数据集长度为 &#123;&#125;&quot;.format(train_data_size))</span></span><br><span class="line"><span class="comment"># print(&quot;测试数据集长度为 &#123;&#125;&quot;.format(test_data_size))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 利用DataLoader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    dataset=train_data,</span><br><span class="line">    batch_size=<span class="number">64</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_dataloader = DataLoader(</span><br><span class="line">    dataset=test_data,</span><br><span class="line">    batch_size=<span class="number">64</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 搭建神经网络 (from train_model.py import *)</span></span><br><span class="line">classification = Classification10Class()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 优化器</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">optimizer = torch.optim.SGD(</span><br><span class="line">    params=classification.parameters(),</span><br><span class="line">    lr=learning_rate,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 设置训练网络的参数</span></span><br><span class="line">total_train_step = <span class="number">0</span>  <span class="comment"># 训练次数</span></span><br><span class="line">total_test_step = <span class="number">0</span>  <span class="comment"># 测试次数</span></span><br><span class="line">epochs = <span class="number">10</span>  <span class="comment"># 训练迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard可视化</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------------- 第 &#123;&#125; 轮训练开始 -------------&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练步骤</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="comment"># 输入输出</span></span><br><span class="line">        images, targets = data</span><br><span class="line">        outputs = classification(images)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 损失函数</span></span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清零梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数: &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line">            writer.add_scalar(</span><br><span class="line">                tag=<span class="string">&quot;train_loss (every 100): &quot;</span>,</span><br><span class="line">                scalar_value=loss.item(),</span><br><span class="line">                global_step=total_train_step,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤(不更新参数)</span></span><br><span class="line">    total_test_loss = <span class="number">0</span>  <span class="comment"># 测试集损失累积</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            images, targets = data</span><br><span class="line">            outputs = classification(images)</span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line">            total_test_loss += loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;##### 测试集loss: &#123;&#125; #####&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    writer.add_scalar(</span><br><span class="line">        tag=<span class="string">&quot;test_loss (every epoch): &quot;</span>,</span><br><span class="line">        scalar_value=total_test_loss,</span><br><span class="line">        global_step=epoch,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存每次训练的模型</span></span><br><span class="line">    torch.save(classification, <span class="string">&quot;./models_cifar/classification_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;##### 模型成功保存 #####&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240808%E4%B8%8A%E5%8D%88103240716.png" alt=""></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240808%E4%B8%8A%E5%8D%88103319943.png" style="zoom:80%;"/><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240808%E4%B8%8A%E5%8D%88103407633.png" style="zoom:80%;" /><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240808%E4%B8%8A%E5%8D%88103503420.png" style="zoom:80%;" /></p><h2 id="4-Full-Code"><a href="#4-Full-Code" class="headerlink" title="4 Full Code"></a>4 Full Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; CIFAR Classification &quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> train_model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&quot;./dataset&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&quot;./dataset&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor(),</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 获取数据集长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># print(&quot;训练数据集长度为 &#123;&#125;&quot;.format(train_data_size))</span></span><br><span class="line"><span class="comment"># print(&quot;测试数据集长度为 &#123;&#125;&quot;.format(test_data_size))</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 利用DataLoader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    dataset=train_data,</span><br><span class="line">    batch_size=<span class="number">64</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_dataloader = DataLoader(</span><br><span class="line">    dataset=test_data,</span><br><span class="line">    batch_size=<span class="number">64</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 搭建神经网络 (from train_model.py import *)</span></span><br><span class="line">classification = Classification10Class()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 优化器</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">optimizer = torch.optim.SGD(</span><br><span class="line">    params=classification.parameters(),</span><br><span class="line">    lr=learning_rate,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 设置训练网络的参数</span></span><br><span class="line">total_train_step = <span class="number">0</span>  <span class="comment"># 训练次数</span></span><br><span class="line">total_test_step = <span class="number">0</span>  <span class="comment"># 测试次数 == epoch</span></span><br><span class="line">epochs = <span class="number">10</span>  <span class="comment"># 训练迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard可视化</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;------------- 第 &#123;&#125; 轮训练开始 -------------&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练步骤</span></span><br><span class="line">    classification.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        <span class="comment"># 输入输出</span></span><br><span class="line">        images, targets = data</span><br><span class="line">        outputs = classification(images)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 损失函数</span></span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 清零梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数: &#123;&#125;, loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line">            writer.add_scalar(</span><br><span class="line">                tag=<span class="string">&quot;train_loss (every 100 steps)&quot;</span>,</span><br><span class="line">                scalar_value=loss.item(),</span><br><span class="line">                global_step=total_train_step,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤(不更新参数)</span></span><br><span class="line">    classification.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span>  <span class="comment"># 测试集损失累积</span></span><br><span class="line">    total_accuracy = <span class="number">0</span>  <span class="comment"># 分类问题正确率</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            images, targets = data</span><br><span class="line">            outputs = classification(images)</span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line">            total_test_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 正确率</span></span><br><span class="line">            accuracy = (outputs.argmax(axis=<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy += accuracy</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在测试集上的损失</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;##### 在测试集上的loss: &#123;&#125; #####&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    writer.add_scalar(</span><br><span class="line">        tag=<span class="string">&quot;test_loss (every epoch)&quot;</span>,</span><br><span class="line">        scalar_value=total_test_loss,</span><br><span class="line">        global_step=epoch,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在测试集上的正确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;##### 在测试集上的正确率: &#123;&#125; #####&quot;</span>.<span class="built_in">format</span>(total_accuracy / test_data_size))</span><br><span class="line">    writer.add_scalar(</span><br><span class="line">        tag=<span class="string">&quot;test_accuracy (every epoch)&quot;</span>,</span><br><span class="line">        scalar_value=total_accuracy / test_data_size,</span><br><span class="line">        global_step=epoch,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存每次训练的模型</span></span><br><span class="line">    torch.save(classification, <span class="string">&quot;./models_cifar/classification_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">    <span class="comment"># torch.save(classification.state_dict(), &quot;./models_cifar/classification_&#123;&#125;.pth&quot;.format(epoch)) # 推荐</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;##### 模型成功保存 #####&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="5-GPU"><a href="#5-GPU" class="headerlink" title="5 GPU"></a>5 GPU</h2><h3 id="1-Google-Colab"><a href="#1-Google-Colab" class="headerlink" title="1. Google Colab"></a>1. Google Colab</h3><ul><li>Google Colab: <a href="https://colab.research.google.com/">https://colab.research.google.com/</a></li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240808%E4%B8%8A%E5%8D%88110119717.png" alt=""></p><ul><li>配置</li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240808%E4%B8%8A%E5%8D%88110154640.png" alt=""></p><ul><li>cpu 大约42s    gpu 大约8s</li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240808%E4%B8%8A%E5%8D%88110318142.png" alt=""></p><h3 id="2-Method-1-cuda"><a href="#2-Method-1-cuda" class="headerlink" title="2. Method 1: .cuda()"></a>2. Method 1: .cuda()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27; GPU &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model_name = model_name.cuda()  <span class="comment"># GPU</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn = loss_fn.cuda()  <span class="comment"># GPU</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    images = images.cuda()</span><br><span class="line">    targets = targets.cuda()</span><br></pre></td></tr></table></figure><h3 id="3-Method-2-to-device"><a href="#3-Method-2-to-device" class="headerlink" title="3. Method 2: .to(device)"></a>3. Method 2: .to(device)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_name = model_name.to(device)</span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line">images = images.to(device)</span><br><span class="line">targets = targets.to(device)</span><br></pre></td></tr></table></figure><h2 id="6-Validation-and-Test"><a href="#6-Validation-and-Test" class="headerlink" title="6 Validation and Test"></a>6 Validation and Test</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">image_path = <span class="string">&quot;./image/dog.png&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载图片并转化类型</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line">image = image.convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line">image = transform(image)  <span class="comment"># torch.Size([3, 32, 32])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 加载神经网络(因为是方式1, 所以要申明网络模型)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Classification10Class</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Classification10Class, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.module = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(in_features=<span class="number">64</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">64</span>),</span><br><span class="line">            nn.Linear(in_features=<span class="number">64</span>, out_features=<span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.module(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. load参数</span></span><br><span class="line">model = torch.load(<span class="string">&quot;./models_cifar/classification_gpu_29.pth&quot;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 测试</span></span><br><span class="line">image = torch.reshape(image, (<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(image)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(outputs)</span><br><span class="line"><span class="built_in">print</span>(outputs.argmax(axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240808%E4%B8%8A%E5%8D%88114418117.png" alt=""></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240808%E4%B8%8A%E5%8D%88114552561.png" alt=""></p>]]></content>
    
    
    <summary type="html">利用 Pytorch 搭建神经网络：完整的搭建，训练过程，以 CIFAR10 数据集为例</summary>
    
    
    
    <category term="Pytorch 入门" scheme="https://blog.iskage.online/categories/Pytorch-%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="AI" scheme="https://blog.iskage.online/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 搭建神经网络（4）网络调用和保存</title>
    <link href="https://blog.iskage.online/posts/bd02f045.html"/>
    <id>https://blog.iskage.online/posts/bd02f045.html</id>
    <published>2025-02-05T07:27:06.000Z</published>
    <updated>2025-02-07T12:18:30.058Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><p>官方文档 中文 <a href="https://www.pytorchtutorial.com/docs/">https://www.pytorchtutorial.com/docs/</a></p><p>官方文档 <a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><h2 id="1-torchvision-models"><a href="#1-torchvision-models" class="headerlink" title="1 torchvision.models"></a>1 torchvision.models</h2><p>有关图像处理的模型</p><p>torchvision.models: <a href="https://pytorch.org/vision/0.9/models.html">https://pytorch.org/vision/0.9/models.html</a></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240806%E4%B8%8B%E5%8D%8860250011.png" alt=""></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240806%E4%B8%8B%E5%8D%8861010016.png" alt=""></p><h2 id="2-Example-Classification-VGG"><a href="#2-Example-Classification-VGG" class="headerlink" title="2 Example: Classification VGG"></a>2 Example: Classification VGG</h2><h3 id="1-Download-ImageNet"><a href="#1-Download-ImageNet" class="headerlink" title="1. Download ImageNet"></a>1. Download ImageNet</h3><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240806%E4%B8%8B%E5%8D%8861138161.png" alt=""></p><p>too large</p><h3 id="2-Download-the-model"><a href="#2-Download-the-model" class="headerlink" title="2. Download the model"></a>2. Download the model</h3><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240806%E4%B8%8B%E5%8D%8862514836.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vgg16_pretrained_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16_pretrained_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_pretrained_true)</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240806%E4%B8%8B%E5%8D%8862722432.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model: out_features=<span class="number">1000</span></span><br></pre></td></tr></table></figure><h3 id="3-Add-Layers"><a href="#3-Add-Layers" class="headerlink" title="3. Add Layers"></a>3. Add Layers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; 增加层 &quot;&quot;&quot;</span></span><br><span class="line">vgg16_pretrained_true.classifier.add_module(</span><br><span class="line">    name=<span class="string">&quot;add_linear&quot;</span>,  <span class="comment"># 增加新的层名</span></span><br><span class="line">    module=nn.Linear(</span><br><span class="line">        in_features=<span class="number">1000</span>,</span><br><span class="line">        out_features=<span class="number">10</span>,</span><br><span class="line">    ),  <span class="comment"># 增加新的层</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(vgg16_pretrained_true)  <span class="comment"># out_features=10</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240806%E4%B8%8B%E5%8D%8863539832.png" alt=""></p><h3 id="4-Modify-Layers"><a href="#4-Modify-Layers" class="headerlink" title="4. Modify Layers"></a>4. Modify Layers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; 修改层 &quot;&quot;&quot;</span></span><br><span class="line">vgg16_pretrained_false.classifier[<span class="number">6</span>] = nn.Linear(</span><br><span class="line">    in_features=<span class="number">1000</span>,</span><br><span class="line">    out_features=<span class="number">10</span>,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(vgg16_pretrained_false)  <span class="comment"># out_features=10</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240806%E4%B8%8B%E5%8D%8863840763.png" alt=""></p><h2 id="3-Save-amp-Load-Models"><a href="#3-Save-amp-Load-Models" class="headerlink" title="3 Save &amp; Load Models"></a>3 Save &amp; Load Models</h2><h3 id="1-Save"><a href="#1-Save" class="headerlink" title="1. Save"></a>1. Save</h3><h4 id="Method-1"><a href="#Method-1" class="headerlink" title="Method 1"></a>Method 1</h4><ul><li>save()</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1. save method 1: save -&gt; structure + parameters</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">torch.save(vgg16, <span class="string">&quot;./models/vgg16_method1.pth&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="Method-2-Recommend"><a href="#Method-2-Recommend" class="headerlink" title="Method 2 (Recommend)"></a>Method 2 (Recommend)</h4><ul><li>save(model.state_dict())</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">2. save method 2: save as dict -&gt; parameters (better)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">torch.save(vgg16.state_dict(), <span class="string">&quot;./models/vgg16_method2.pth&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="2-Load"><a href="#2-Load" class="headerlink" title="2. Load"></a>2. Load</h3><h4 id="Method-1-1"><a href="#Method-1-1" class="headerlink" title="Method 1"></a>Method 1</h4><ul><li>load()</li></ul><p>structure + parameters</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># load method 1:</span></span><br><span class="line">model1 = torch.load(<span class="string">&quot;./models/vgg16_method1.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model1)</span><br></pre></td></tr></table></figure><h4 id="Method-2-Recommend-1"><a href="#Method-2-Recommend-1" class="headerlink" title="Method 2 (Recommend)"></a>Method 2 (Recommend)</h4><ul><li>load_state_dict()</li></ul><p>parameters -&gt; models</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># load method 2:</span></span><br><span class="line">vgg16 = torchvision.models.vgg16()</span><br><span class="line">model2_param_dict = torch.load(<span class="string">&quot;./models/vgg16_method2.pth&quot;</span>)  <span class="comment"># parameters&#x27; dict</span></span><br><span class="line">vgg16.load_state_dict(model2_param_dict)</span><br><span class="line"><span class="built_in">print</span>(vgg16)</span><br></pre></td></tr></table></figure><h3 id="3-Some-Errors"><a href="#3-Some-Errors" class="headerlink" title="3. Some Errors"></a>3. Some Errors</h3><ul><li>save the model with method 1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27; python_file_name: model_save.py &#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">      <span class="built_in">super</span>(MyNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">      x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">my_net = MyNet()</span><br><span class="line">torch.save(my_net, <span class="string">&quot;my_net_method1.pth&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>load the model in another python file</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">error</span></span><br><span class="line"><span class="string">model = torch.load(&quot;my_net_method1.pth&quot;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model_save <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&quot;my_net_method1.pth&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">利用 Pytorch 搭建神经网络：调用其他网络，保存自己的网络</summary>
    
    
    
    <category term="Pytorch 入门" scheme="https://blog.iskage.online/categories/Pytorch-%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="AI" scheme="https://blog.iskage.online/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 搭建神经网络（3）损失函数 反向传播 优化器</title>
    <link href="https://blog.iskage.online/posts/119f3166.html"/>
    <id>https://blog.iskage.online/posts/119f3166.html</id>
    <published>2025-02-05T07:27:03.000Z</published>
    <updated>2025-02-07T12:18:30.571Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Loss-Backward-Optimizer-损失函数-反向传播-优化器"><a href="#Loss-Backward-Optimizer-损失函数-反向传播-优化器" class="headerlink" title="Loss Backward Optimizer 损失函数 反向传播 优化器"></a>Loss Backward Optimizer 损失函数 反向传播 优化器</h1><p>官方文档 中文 <a href="https://www.pytorchtutorial.com/docs/">https://www.pytorchtutorial.com/docs/</a></p><p>官方文档 <a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><h2 id="1-Loss-Function"><a href="#1-Loss-Function" class="headerlink" title="1 Loss Function"></a>1 Loss Function</h2><h3 id="1-Loss-Functions"><a href="#1-Loss-Functions" class="headerlink" title="1. Loss Functions"></a>1. Loss Functions</h3><p><a href="https://pytorch.org/docs/1.8.1/nn.html#loss-functions">https://pytorch.org/docs/1.8.1/nn.html#loss-functions</a></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240805%E4%B8%8B%E5%8D%8834542085.png" alt=""></p><h3 id="2-Some-Loss-Functions"><a href="#2-Some-Loss-Functions" class="headerlink" title="2. Some Loss Functions"></a>2. Some Loss Functions</h3><h4 id="L1Loss"><a href="#L1Loss" class="headerlink" title="L1Loss"></a>L1Loss</h4><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240805%E4%B8%8B%E5%8D%8835511919.png" alt=""></p><ul><li>examples</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">target = torch.reshape(target, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">loss_sum = L1Loss(reduction=<span class="string">&#x27;sum&#x27;</span>)  <span class="comment"># tensor(2.)</span></span><br><span class="line">loss_mean = L1Loss(reduction=<span class="string">&#x27;mean&#x27;</span>)  <span class="comment"># tensor(0.6667)</span></span><br><span class="line">res_sum = loss_sum(<span class="built_in">input</span>, target)</span><br><span class="line">res_mean = loss_mean(<span class="built_in">input</span>, target)</span><br><span class="line"><span class="built_in">print</span>(res_sum)</span><br><span class="line"><span class="built_in">print</span>(res_mean)</span><br></pre></td></tr></table></figure><ul><li>input &amp; target</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; (N, *)</span><br><span class="line">target -&gt; (N, *)</span><br></pre></td></tr></table></figure><h4 id="MSELoss"><a href="#MSELoss" class="headerlink" title="MSELoss"></a>MSELoss</h4><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240805%E4%B8%8B%E5%8D%8835620468.png" alt=""></p><ul><li>example</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MSELoss</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">target = torch.reshape(target, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">loss = MSELoss()</span><br><span class="line">res = loss(<span class="built_in">input</span>, target)  <span class="comment"># tensor(1.3333)</span></span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure><ul><li>input &amp; target</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; (N, *)</span><br><span class="line">target -&gt; (N, *)</span><br></pre></td></tr></table></figure><h4 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h4><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240805%E4%B8%8B%E5%8D%8840106382.png" alt=""></p><ul><li>example</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> CrossEntropyLoss</span><br><span class="line"></span><br><span class="line">input_predict = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>], dtype=torch.<span class="built_in">float</span>)  <span class="comment"># 模型返回了对3个种类的概率预测</span></span><br><span class="line">target = torch.tensor([<span class="number">1</span>])   <span class="comment"># 实际属于第1+1种，返回1</span></span><br><span class="line"></span><br><span class="line">input_predict = torch.reshape(input_predict, (<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">loss = CrossEntropyLoss()</span><br><span class="line">res = loss(input_predict, target)  <span class="comment"># tensor(1.1019)</span></span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure><ul><li>input &amp; target</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input  -&gt; (N, C)    最后得到的各个种类的概率</span><br><span class="line">target -&gt; (N)      实际属于哪一种</span><br><span class="line"></span><br><span class="line">output -&gt; 返回标量: If reduction is &#x27;none&#x27;, then the same size as the target: (N) </span><br><span class="line"></span><br><span class="line"># C = number of classes</span><br></pre></td></tr></table></figure><h3 id="3-Loss-of-CIFAR10"><a href="#3-Loss-of-CIFAR10" class="headerlink" title="3. Loss of CIFAR10"></a>3. Loss of CIFAR10</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cifar</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Cifar, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model1 = Sequential(</span><br><span class="line">            Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(in_features=<span class="number">64</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">64</span>),</span><br><span class="line">            Linear(in_features=<span class="number">64</span>, out_features=<span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;测试&#x27;&#x27;&#x27;</span></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&quot;./dataset&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">False</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">my_net = Cifar()</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = my_net(imgs)</span><br><span class="line">    <span class="comment"># print(outputs)  # [p0, p1, ..., p9]  每张图片的类的预测的概率</span></span><br><span class="line">    <span class="comment"># print(targets)  # [t0, t1, ..., t63] 真实每张图片对应的类标号</span></span><br><span class="line"></span><br><span class="line">    res_loss = loss(outputs, targets)</span><br><span class="line">    <span class="built_in">print</span>(res_loss)   <span class="comment"># tensor(2.2994, grad_fn=&lt;NllLossBackward0&gt;) 误差</span></span><br></pre></td></tr></table></figure><h2 id="2-Backward"><a href="#2-Backward" class="headerlink" title="2 Backward"></a>2 Backward</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">my_net = Cifar()</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; network &#x27;&#x27;&#x27;</span></span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = my_net(imgs)</span><br><span class="line">    <span class="comment"># print(outputs)  # [p0, p1, ..., p9]  每张图片的类的预测的概率</span></span><br><span class="line">    <span class="comment"># print(targets)  # [t0, t1, ..., t63] 真实每张图片对应的类标号</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; loss function &#x27;&#x27;&#x27;</span></span><br><span class="line">    res_loss = loss(outputs, targets)</span><br><span class="line">    <span class="comment"># print(res_loss)   # tensor(2.2994, grad_fn=&lt;NllLossBackward0&gt;) 误差</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; backward &#x27;&#x27;&#x27;</span></span><br><span class="line">    res_loss.backward()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ok&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240805%E4%B8%8B%E5%8D%8843712009.png" alt=""></p><h2 id="3-Optimizer"><a href="#3-Optimizer" class="headerlink" title="3 Optimizer"></a>3 Optimizer</h2><p>Pytorch Docs: <a href="https://pytorch.org/docs/1.8.1/optim.html">https://pytorch.org/docs/1.8.1/optim.html</a></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240805%E4%B8%8B%E5%8D%8844003825.png" alt=""></p><h3 id="1-Basical-Usage"><a href="#1-Basical-Usage" class="headerlink" title="1. Basical Usage"></a>1. Basical Usage</h3><ol><li>Constructing it</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27; 参数parameters和学习率lr(learning rate)是必须传入的，其他由特定算法决定 &#x27;&#x27;&#x27;</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">optimizer = optim.Adam([var1, var2], lr=<span class="number">0.0001</span>)</span><br></pre></td></tr></table></figure><ol><li>Taking an optimization step</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">  <span class="comment"># 清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化器更新参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="4-完整网络代码-Full-Example-for-NN"><a href="#4-完整网络代码-Full-Example-for-NN" class="headerlink" title="4 完整网络代码 Full  Example for NN"></a>4 完整网络代码 Full  Example for NN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; 加载数据 &#x27;&#x27;&#x27;</span></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&quot;./dataset&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">False</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cifar</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; 创建网络 &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Cifar, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model1 = Sequential(</span><br><span class="line">            Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(in_features=<span class="number">64</span> * <span class="number">4</span> * <span class="number">4</span>, out_features=<span class="number">64</span>),</span><br><span class="line">            Linear(in_features=<span class="number">64</span>, out_features=<span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; 实例化网络 选择损失函数 优化器 &#x27;&#x27;&#x27;</span></span><br><span class="line">my_net = Cifar()</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">optim = torch.optim.SGD(my_net.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 随机梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; 开始训练 &#x27;&#x27;&#x27;</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 每次训练起始损失</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; network &#x27;&#x27;&#x27;</span></span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = my_net(imgs)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; loss function &#x27;&#x27;&#x27;</span></span><br><span class="line">        res_loss = loss(outputs, targets)</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; zero_grad &#x27;&#x27;&#x27;</span></span><br><span class="line">        optim.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; backward &#x27;&#x27;&#x27;</span></span><br><span class="line">        res_loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; optim_step updated &#x27;&#x27;&#x27;</span></span><br><span class="line">        optim.step()</span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; each epoch loss sum &#x27;&#x27;&#x27;</span></span><br><span class="line">        running_loss += res_loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;every epoch, the loss is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(running_loss))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>every epoch, the loss <span class="keyword">is</span>: <span class="number">360.25927734375</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>every epoch, the loss <span class="keyword">is</span>: <span class="number">354.6569519042969</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>every epoch, the loss <span class="keyword">is</span>: <span class="number">336.74420166015625</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>every epoch, the loss <span class="keyword">is</span>: <span class="number">319.4207458496094</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>every epoch, the loss <span class="keyword">is</span>: <span class="number">310.6980285644531</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>every epoch, the loss <span class="keyword">is</span>: <span class="number">302.37701416015625</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>every epoch, the loss <span class="keyword">is</span>: <span class="number">292.7424011230469</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>every epoch, the loss <span class="keyword">is</span>: <span class="number">284.93902587890625</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">利用 Pytorch 搭建神经网络：完整的网络</summary>
    
    
    
    <category term="Pytorch 入门" scheme="https://blog.iskage.online/categories/Pytorch-%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="AI" scheme="https://blog.iskage.online/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 搭建神经网络（2）网络搭建</title>
    <link href="https://blog.iskage.online/posts/ae1c954d.html"/>
    <id>https://blog.iskage.online/posts/ae1c954d.html</id>
    <published>2025-02-05T07:27:01.000Z</published>
    <updated>2025-02-07T12:18:31.067Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch-搭建网络"><a href="#Pytorch-搭建网络" class="headerlink" title="Pytorch 搭建网络"></a>Pytorch 搭建网络</h1><p>官方文档 中文 <a href="https://www.pytorchtutorial.com/docs/">https://www.pytorchtutorial.com/docs/</a></p><p>官方文档 <a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a></p><h2 id="1-神经网络"><a href="#1-神经网络" class="headerlink" title="1 神经网络"></a>1 神经网络</h2><p>官方文档：<a href="https://pytorch.org/docs/1.8.1/nn.html">https://pytorch.org/docs/1.8.1/nn.html</a></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%8893656667.png" alt=""></p><h2 id="2-Containers框架"><a href="#2-Containers框架" class="headerlink" title="2 Containers框架"></a>2 Containers框架</h2><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%8893741302.png" alt=""></p><ul><li>官方示例</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))<span class="comment"># 卷积conv1、非线性relu</span></span><br><span class="line">        <span class="keyword">return</span> F.relu(<span class="variable language_">self</span>.conv2(x))<span class="comment"># 卷积conv2、非线性relu</span></span><br></pre></td></tr></table></figure><ul><li>简单尝试</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_model = MyModel()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">output = my_model(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h2 id="3-stride-amp-padding"><a href="#3-stride-amp-padding" class="headerlink" title="3 stride &amp; padding"></a>3 stride &amp; padding</h2><p><strong>torch.nn.functional.conv2d</strong></p><h3 id="1-stride"><a href="#1-stride" class="headerlink" title="1. stride"></a>1. stride</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入图像 5x5 每个数字表示颜色</span></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积核 3x3</span></span><br><span class="line">kernel = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为torch.nn.functional.conv2d默认输入为4维的，所以转化</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="built_in">input</span>.shape[<span class="number">0</span>], <span class="built_in">input</span>.shape[<span class="number">1</span>]))</span><br><span class="line">kernel = torch.reshape(kernel, (<span class="number">1</span>, <span class="number">1</span>, kernel.shape[<span class="number">0</span>], kernel.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor</span></span><br><span class="line"><span class="string">input: 就是输入数据集</span></span><br><span class="line"><span class="string">weight: 卷积核</span></span><br><span class="line"><span class="string">stride: 卷积核每次移动的步数</span></span><br><span class="line"><span class="string">padding: 对输入图像的填充</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">output_stride1 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output_stride1)</span><br><span class="line"></span><br><span class="line">output_stride2 = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output_stride2)</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88101819072.png" alt=""></p><h3 id="2-padding"><a href="#2-padding" class="headerlink" title="2. padding"></a>2. padding</h3><p>对输入图像的填充</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output_padding1 = F.conv2d(<span class="built_in">input</span>, kernel, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output_padding1)</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88102514146.png" alt=""></p><h2 id="4-卷积层"><a href="#4-卷积层" class="headerlink" title="4 卷积层"></a>4 卷积层</h2><h3 id="1-Convolution"><a href="#1-Convolution" class="headerlink" title="1. Convolution"></a>1. Convolution</h3><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88102701065.png" alt=""></p><h3 id="2-调用和参数"><a href="#2-调用和参数" class="headerlink" title="2. 调用和参数"></a>2. 调用和参数</h3><ul><li>调用</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li>参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in_channels: 输入</span></span><br><span class="line">- in_channels (<span class="built_in">int</span>) – Number of channels <span class="keyword">in</span> the <span class="built_in">input</span> image</span><br><span class="line"></span><br><span class="line"><span class="comment"># out_channels: 输出</span></span><br><span class="line">- out_channels (<span class="built_in">int</span>) – Number of channels produced by the convolution</span><br><span class="line"></span><br><span class="line"><span class="comment"># kernel_size: 卷积核的大小</span></span><br><span class="line">- kernel_size (<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>) – Size of the convolving kernel</span><br><span class="line"></span><br><span class="line"><span class="comment"># stride: 步数</span></span><br><span class="line">- stride (<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>, optional) – Stride of the convolution. Default: <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># padding: 填充层数</span></span><br><span class="line">- padding (<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>, optional) – Zero-padding added to both sides of the <span class="built_in">input</span>. Default: <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># padding_mode: 填充方式</span></span><br><span class="line">- padding_mode (string, optional) – <span class="string">&#x27;zeros&#x27;</span>, <span class="string">&#x27;reflect&#x27;</span>, <span class="string">&#x27;replicate&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;circular&#x27;</span>. Default: <span class="string">&#x27;zeros&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dilation: 卷积核中元素的对应位置</span></span><br><span class="line">- dilation (<span class="built_in">int</span> <span class="keyword">or</span> <span class="built_in">tuple</span>, optional) – Spacing between kernel elements. Default: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">- groups (<span class="built_in">int</span>, optional) – Number of blocked connections <span class="keyword">from</span> <span class="built_in">input</span> channels to output channels. Default: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">- bias (<span class="built_in">bool</span>, optional) – If <span class="literal">True</span>, adds a learnable bias to the output. Default: <span class="literal">True</span></span><br></pre></td></tr></table></figure><ul><li>dilation</li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/QQ_1738725222605.png" alt=""></p><h3 id="3-简单原理"><a href="#3-简单原理" class="headerlink" title="3. 简单原理"></a>3. 简单原理</h3><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88103543577.png" alt=""></p><ul><li>in_channels &amp; out_channels</li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88104218535.png" alt=""></p><h3 id="4-示例代码"><a href="#4-示例代码" class="headerlink" title="4. 示例代码"></a>4. 示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">my_net = Net()</span><br><span class="line"><span class="built_in">print</span>(my_net)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 网络结构</span></span><br><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(<span class="number">3</span>, <span class="number">6</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在tensorboard展示</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs_conv&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line">my_net = Net()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = my_net(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)</span><br><span class="line">    <span class="comment"># writer.add_images(&quot;output&quot;, output, step)  # torch.Size([64, 6, 30, 30]) output的channel=6，报错</span></span><br><span class="line">    output = torch.reshape(output, (-<span class="number">1</span>, <span class="number">3</span>, <span class="number">30</span>, <span class="number">30</span>))</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step)</span><br><span class="line">    step += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>如果希望卷积后，通道变多，但尺寸不变，则需要填充padding，公式</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88110036714.png" alt=""></p><h2 id="5-池化层"><a href="#5-池化层" class="headerlink" title="5 池化层"></a>5 池化层</h2><h3 id="1-Pooling"><a href="#1-Pooling" class="headerlink" title="1. Pooling"></a>1. Pooling</h3><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240804%E4%B8%8B%E5%8D%8830453528.png" alt=""></p><h3 id="2-调用和参数-1"><a href="#2-调用和参数-1" class="headerlink" title="2. 调用和参数"></a>2. 调用和参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MaxPool2d(kernel_size, stride=<span class="literal">None</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=<span class="literal">False</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取最大值的窗口</span></span><br><span class="line">- kernel_size – the size of the window to take a <span class="built_in">max</span> over</span><br><span class="line"></span><br><span class="line"><span class="comment"># 横向纵向的步长，default = kernel_size</span></span><br><span class="line">- stride – the stride of the window. Default value <span class="keyword">is</span> kernel_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 补充图像边缘</span></span><br><span class="line">- padding – implicit zero padding to be added on both sides</span><br><span class="line"></span><br><span class="line"><span class="comment"># 空洞</span></span><br><span class="line">- dilation – a parameter that controls the stride of elements <span class="keyword">in</span> the window</span><br><span class="line"></span><br><span class="line">- return_indices – <span class="keyword">if</span> <span class="literal">True</span>, will <span class="keyword">return</span> the <span class="built_in">max</span> indices along <span class="keyword">with</span> the outputs. Useful <span class="keyword">for</span> torch.nn.MaxUnpool2d later</span><br><span class="line"></span><br><span class="line"><span class="comment"># floor向下取整 ceil向上取整，例如ceil_mode = True，保留超出部分</span></span><br><span class="line">- ceil_mode – when <span class="literal">True</span>, will use ceil instead of floor to compute the output shape</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240804%E4%B8%8B%E5%8D%8832249522.png" alt=""></p><ul><li>注意输入的input和输出output均为</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(N, C, H, W)  <span class="comment"># (batch_size层数, Channel通道数, Height高, Width宽)</span></span><br></pre></td></tr></table></figure><ul><li>注意池化不可对long操作，故</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, ],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, ],</span><br><span class="line">], dtype=torch.<span class="built_in">float</span>)  <span class="comment"># 转为float</span></span><br></pre></td></tr></table></figure><h3 id="3-示例代码"><a href="#3-示例代码" class="headerlink" title="3. 示例代码"></a>3. 示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, ],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, ],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, ],</span><br><span class="line">], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1层，1通道，5x5大小，-1表示自动计算</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaxPoolNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MaxPoolNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.maxpool1 = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="variable language_">self</span>.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_net = MaxPoolNet()</span><br><span class="line">output = my_net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">&gt;&gt; tensor([[[[<span class="number">3.</span>, <span class="number">2.</span>],</span><br><span class="line">             [<span class="number">5.</span>, <span class="number">1.</span>]]]])</span><br></pre></td></tr></table></figure><h2 id="6-非线性激活（激活函数）"><a href="#6-非线性激活（激活函数）" class="headerlink" title="6 非线性激活（激活函数）"></a>6 非线性激活（激活函数）</h2><h3 id="1-Non-linear-Activations"><a href="#1-Non-linear-Activations" class="headerlink" title="1. Non-linear Activations"></a>1. Non-linear Activations</h3><p>relu, sigmoid…</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240804%E4%B8%8B%E5%8D%8835228634.png" alt=""></p><h3 id="2-调用和参数-2"><a href="#2-调用和参数-2" class="headerlink" title="2. 调用和参数"></a>2. 调用和参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否内存拷贝</span></span><br><span class="line">inplace – can optionally do the operation <span class="keyword">in</span>-place. Default: <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">ReLu(<span class="built_in">input</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">input</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">output = ReLu(<span class="built_in">input</span>, inplace = <span class="literal">False</span>)</span><br><span class="line">&gt;&gt; <span class="built_in">input</span> = -<span class="number">1</span></span><br><span class="line">&gt;&gt; output = <span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="3-示例代码-1"><a href="#3-示例代码-1" class="headerlink" title="3. 示例代码"></a>3. 示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ReLU</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([</span><br><span class="line">    [<span class="number">1</span>, -<span class="number">0.5</span>],</span><br><span class="line">    [-<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NonLinearActivationsRelu</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NonLinearActivationsRelu, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.relu1 = ReLU(inplace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="variable language_">self</span>.relu1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_net = NonLinearActivationsRelu()</span><br><span class="line">output = my_net(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line">&gt;&gt; tensor([[ <span class="number">1.0000</span>, -<span class="number">0.5000</span>],</span><br><span class="line">           [-<span class="number">1.0000</span>,  <span class="number">3.0000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">&gt;&gt; tensor([[<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">           [<span class="number">0.</span>, <span class="number">3.</span>]])</span><br></pre></td></tr></table></figure><h2 id="7-正则化层"><a href="#7-正则化层" class="headerlink" title="7 正则化层"></a>7 正则化层</h2><h3 id="1-Normalization"><a href="#1-Normalization" class="headerlink" title="1. Normalization"></a>1. Normalization</h3><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240804%E4%B8%8B%E5%8D%8840656515.png" alt=""></p><h3 id="2-调用和参数-3"><a href="#2-调用和参数-3" class="headerlink" title="2. 调用和参数"></a>2. 调用和参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.BatchNorm2d(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>, track_running_stats=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">- num_features – C <span class="keyword">from</span> an expected <span class="built_in">input</span> of size (N,C,H,W)</span><br><span class="line"></span><br><span class="line">- eps – a value added to the denominator <span class="keyword">for</span> numerical stability. Default: <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">- momentum – the value used <span class="keyword">for</span> the running_mean <span class="keyword">and</span> running_var computation. Can be <span class="built_in">set</span> to <span class="literal">None</span> <span class="keyword">for</span> cumulative moving average (i.e. simple average). Default: <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">- affine – a boolean value that when <span class="built_in">set</span> to <span class="literal">True</span>, this module has learnable affine parameters. Default: <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">- track_running_stats – a boolean value that when <span class="built_in">set</span> to <span class="literal">True</span>, this module tracks the running mean <span class="keyword">and</span> variance, <span class="keyword">and</span> when <span class="built_in">set</span> to <span class="literal">False</span>, this module does <span class="keyword">not</span> track such statistics, <span class="keyword">and</span> initializes statistics buffers running_mean <span class="keyword">and</span> running_var <span class="keyword">as</span> <span class="literal">None</span>. When these buffers are <span class="literal">None</span>, this module always uses batch statistics. <span class="keyword">in</span> both training <span class="keyword">and</span> <span class="built_in">eval</span> modes. Default: <span class="literal">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- <span class="built_in">input</span> - (N, C, H, W)</span><br><span class="line">- output - (N, C, H, W)</span><br></pre></td></tr></table></figure><h3 id="3-示例代码-2"><a href="#3-示例代码-2" class="headerlink" title="3. 示例代码"></a>3. 示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># With Learnable Parameters</span></span><br><span class="line">m = nn.BatchNorm2d(<span class="number">100</span>)</span><br><span class="line"><span class="comment"># Without Learnable Parameters</span></span><br><span class="line">m = nn.BatchNorm2d(<span class="number">100</span>, affine=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">100</span>, <span class="number">35</span>, <span class="number">45</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure><h2 id="8-线性层"><a href="#8-线性层" class="headerlink" title="8 线性层"></a>8 线性层</h2><h3 id="1-Linear"><a href="#1-Linear" class="headerlink" title="1. Linear"></a>1. Linear</h3><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240804%E4%B8%8B%E5%8D%8841333231.png" alt=""></p><h3 id="2-调用和参数-4"><a href="#2-调用和参数-4" class="headerlink" title="2. 调用和参数"></a>2. 调用和参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Linear(in_features, out_features, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入层的神经元个数</span></span><br><span class="line">- in_features – size of each <span class="built_in">input</span> sample</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出层的神经元个数</span></span><br><span class="line">- out_features – size of each output sample</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否 w_i*x_i 后加上 b_i</span></span><br><span class="line">- bias – If <span class="built_in">set</span> to <span class="literal">False</span>, the layer will <span class="keyword">not</span> learn an additive bias. Default: <span class="literal">True</span></span><br></pre></td></tr></table></figure><h3 id="3-示例代码-3"><a href="#3-示例代码-3" class="headerlink" title="3. 示例代码"></a>3. 示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">False</span>,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearLayers</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearLayers, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear1 = Linear(in_features=<span class="number">64</span> * <span class="number">3</span> * <span class="number">32</span> * <span class="number">32</span>, out_features=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = <span class="variable language_">self</span>.linear1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_net = LinearLayers()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="comment"># print(imgs.shape)  # torch.Size([64, 3, 32, 32])  64一组，3个通道，32x32图片</span></span><br><span class="line">    <span class="comment"># output = torch.reshape(imgs, (1, 1, 1, -1))  # 每64个图片平铺为一个一维向量</span></span><br><span class="line">    <span class="comment"># output = my_net(output)  # torch.Size([1, 1, 1, 10])</span></span><br><span class="line"></span><br><span class="line">    output = torch.flatten(imgs)  <span class="comment"># torch.Size([196608])</span></span><br><span class="line">    output = my_net(output)  <span class="comment"># torch.Size([10])</span></span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><h2 id="9-pytorch提供的模型"><a href="#9-pytorch提供的模型" class="headerlink" title="9 pytorch提供的模型"></a>9 pytorch提供的模型</h2><p><a href="https://pytorch.org/docs/1.8.1/nn.html">https://pytorch.org/docs/1.8.1/nn.html</a></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240804%E4%B8%8B%E5%8D%8843352338.png" alt=""></p><h2 id="10-Sequential"><a href="#10-Sequential" class="headerlink" title="10 Sequential"></a>10 Sequential</h2><h3 id="1-Sequential简化"><a href="#1-Sequential简化" class="headerlink" title="1. Sequential简化"></a>1. Sequential简化</h3><p>torch.nn -&gt; container -&gt; Sequential</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Sequential(*args)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure><h3 id="2-案例：CIFAR分类"><a href="#2-案例：CIFAR分类" class="headerlink" title="2. 案例：CIFAR分类"></a>2. 案例：CIFAR分类</h3><ul><li>CIFAR Model 结构</li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/Structure-of-CIFAR10-quick-model.png" alt="Structure-of-CIFAR10-quick-model"></p><ul><li>Note: 计算padding和stride</li></ul><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240731%E4%B8%8A%E5%8D%88110036714.png" alt=""></p><ul><li>tensorboard可视化结构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_test = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs_cifar&quot;</span>)</span><br><span class="line">writer.add_graph(cifar_net, x_test)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240804%E4%B8%8B%E5%8D%8850621811.png" alt=""></p>]]></content>
    
    
    <summary type="html">利用 Pytorch 搭建神经网络：搭建网络和基础知识</summary>
    
    
    
    <category term="Pytorch 入门" scheme="https://blog.iskage.online/categories/Pytorch-%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="AI" scheme="https://blog.iskage.online/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch 搭建神经网络（1）下载 Pytorch 和数据处理</title>
    <link href="https://blog.iskage.online/posts/3f7272cd.html"/>
    <id>https://blog.iskage.online/posts/3f7272cd.html</id>
    <published>2025-02-05T07:27:00.000Z</published>
    <updated>2025-02-07T12:18:31.607Z</updated>
    
    <content type="html"><![CDATA[<h1 id="下载Pytorch和数据处理"><a href="#下载Pytorch和数据处理" class="headerlink" title="下载Pytorch和数据处理"></a>下载Pytorch和数据处理</h1><h2 id="0-创建环境并下载Pytorch"><a href="#0-创建环境并下载Pytorch" class="headerlink" title="0 创建环境并下载Pytorch"></a>0 创建环境并下载Pytorch</h2><p>官网：<a href="https://pytorch.org">https://pytorch.org</a></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240729%E4%B8%8A%E5%8D%88110530197.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch::pytorch torchvision torchaudio -c pytorch</span><br></pre></td></tr></table></figure><h2 id="1-加载数据"><a href="#1-加载数据" class="headerlink" title="1 加载数据"></a>1 加载数据</h2><h3 id="1-Dataset"><a href="#1-Dataset" class="headerlink" title="1. Dataset"></a>1. Dataset</h3><p>提取数据并获取label</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):</span><br><span class="line">        <span class="variable language_">self</span>.root_dir = root_dir</span><br><span class="line">        <span class="variable language_">self</span>.label_dir = label_dir</span><br><span class="line">        <span class="variable language_">self</span>.path = os.path.join(<span class="variable language_">self</span>.root_dir, <span class="variable language_">self</span>.label_dir)</span><br><span class="line">        <span class="variable language_">self</span>.img_path = os.listdir(<span class="variable language_">self</span>.path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        img_name = <span class="variable language_">self</span>.img_path[index]</span><br><span class="line">        img_item_path = os.path.join(<span class="variable language_">self</span>.root_dir, <span class="variable language_">self</span>.label_dir, img_name)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)</span><br><span class="line">        label = <span class="variable language_">self</span>.label_dir</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&#x27;dataset/train&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 蚂蚁数据集</span></span><br><span class="line">ants_label_dir = <span class="string">&#x27;ants_image&#x27;</span></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 蜜蜂数据集</span></span><br><span class="line">bees_label_dir = <span class="string">&#x27;bees_image&#x27;</span></span><br><span class="line">bees_dataset = MyData(root_dir, bees_label_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 整合 简单的拼接，按照谁在前整合后仍然在前的原则</span></span><br><span class="line">train_dataset = ants_dataset + bees_dataset</span><br></pre></td></tr></table></figure><p>常见数据集形式</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240729%E4%B8%8B%E5%8D%88122404428.png" alt=""></p><h3 id="2-Dataloader"><a href="#2-Dataloader" class="headerlink" title="2. Dataloader"></a>2. Dataloader</h3><p>为后面的网络提供不同的数据形式（打包）</p><h2 id="2-TensorBoard"><a href="#2-TensorBoard" class="headerlink" title="2 TensorBoard"></a>2 TensorBoard</h2><h3 id="1-尝试"><a href="#1-尝试" class="headerlink" title="1. 尝试"></a>1. 尝试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y=x&#x27;</span>, i, i)  <span class="comment"># 第一个是标题，第二个是y轴，第三个是x轴</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=文件夹名</span><br><span class="line">tensorboard --logdir=logs</span><br><span class="line"><span class="comment"># 改端口</span></span><br><span class="line">tensorboard --logdir=logs --port=<span class="number">6007</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240729%E4%B8%8B%E5%8D%88123602383.png" alt=""></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240729%E4%B8%8B%E5%8D%88123851459.png" alt=""></p><h3 id="2-训练集练习"><a href="#2-训练集练习" class="headerlink" title="2. 训练集练习"></a>2. 训练集练习</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">image_path1 = <span class="string">&#x27;dataset/train/ants_image/0013035.jpg&#x27;</span></span><br><span class="line">image_PIL1 = Image.<span class="built_in">open</span>(image_path1)</span><br><span class="line">image_array1 = np.array(image_PIL1)</span><br><span class="line"></span><br><span class="line">image_path2 = <span class="string">&#x27;dataset/train/ants_image/5650366_e22b7e1065.jpg&#x27;</span></span><br><span class="line">image_PIL2 = Image.<span class="built_in">open</span>(image_path2)</span><br><span class="line">image_array2 = np.array(image_PIL2)</span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;test&quot;</span>, image_array1, <span class="number">1</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br><span class="line">writer.add_image(<span class="string">&quot;test&quot;</span>, image_array2, <span class="number">2</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240729%E4%B8%8B%E5%8D%88125438974.png" alt=""></p><h2 id="3-Transforms"><a href="#3-Transforms" class="headerlink" title="3 Transforms"></a>3 Transforms</h2><blockquote><p>[!NOTE]</p><p>torchvision的模块之一：Transforms</p></blockquote><h3 id="1-ToTensor"><a href="#1-ToTensor" class="headerlink" title="1. ToTensor"></a>1. ToTensor</h3><ol><li>transforms.ToTensor将”PIL Image”和”numpy.ndarray”转化为tensor类型</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&#x27;dataset/train/ants_image/5650366_e22b7e1065.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># PIL Image 类型</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line">&gt;&gt; &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at <span class="number">0x13B2D7970</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy.ndarray 类型</span></span><br><span class="line">cv_img = cv2.imread(img_path)</span><br><span class="line"><span class="built_in">print</span>(cv_img)</span><br><span class="line">&gt;&gt; [[[<span class="number">106</span> <span class="number">119</span>  <span class="number">97</span>]</span><br><span class="line">    [<span class="number">106</span> <span class="number">119</span>  <span class="number">97</span>]</span><br><span class="line">    [<span class="number">107</span> <span class="number">120</span>  <span class="number">98</span>]</span><br><span class="line">    ...</span><br><span class="line">    [<span class="number">110</span> <span class="number">115</span> <span class="number">116</span>]</span><br><span class="line">    [<span class="number">110</span> <span class="number">115</span> <span class="number">116</span>]</span><br><span class="line">    [<span class="number">110</span> <span class="number">115</span> <span class="number">116</span>]]]</span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line">tensor_img = tensor_trans(img)</span><br><span class="line"><span class="built_in">print</span>(tensor_img)</span><br><span class="line">&gt;&gt; tensor([[[<span class="number">0.3804</span>, <span class="number">0.3804</span>, <span class="number">0.3843</span>,  ..., <span class="number">0.3412</span>, <span class="number">0.3373</span>, <span class="number">0.3333</span>],</span><br><span class="line">           [<span class="number">0.3765</span>, <span class="number">0.3804</span>, <span class="number">0.3843</span>,  ..., <span class="number">0.3529</span>, <span class="number">0.3490</span>, <span class="number">0.3451</span>],</span><br><span class="line">           [<span class="number">0.3804</span>, <span class="number">0.3804</span>, <span class="number">0.3843</span>,  ..., <span class="number">0.3725</span>, <span class="number">0.3686</span>, <span class="number">0.3647</span>],</span><br><span class="line">           ...,</span><br><span class="line">           [<span class="number">0.5608</span>, <span class="number">0.5608</span>, <span class="number">0.5647</span>,  ..., <span class="number">0.4392</span>, <span class="number">0.4392</span>, <span class="number">0.4392</span>],</span><br><span class="line">           [<span class="number">0.5412</span>, <span class="number">0.5529</span>, <span class="number">0.5608</span>,  ..., <span class="number">0.4353</span>, <span class="number">0.4353</span>, <span class="number">0.4353</span>],</span><br><span class="line">           [<span class="number">0.5333</span>, <span class="number">0.5412</span>, <span class="number">0.5608</span>,  ..., <span class="number">0.4314</span>, <span class="number">0.4314</span>, <span class="number">0.4314</span>]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-transforms使用"><a href="#2-transforms使用" class="headerlink" title="2. transforms使用"></a>2. transforms使用</h3><p>Note：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__call__函数的作用</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Person</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, use</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;__call__函数：&quot;</span> + use)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">self, use</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;一般函数：&quot;</span> + use)</span><br><span class="line"></span><br><span class="line">person = Person()</span><br><span class="line">person(<span class="string">&quot;call能直接利用类名括号调用&quot;</span>)</span><br><span class="line">person.func(<span class="string">&quot;必须使用.func方式&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="4-torchvision数据集"><a href="#4-torchvision数据集" class="headerlink" title="4 torchvision数据集"></a>4 torchvision数据集</h2><p>torchvision.dataset</p><p>查看官方文档：<a href="https://pytorch.org/">https://pytorch.org/</a></p><p>0.9版本：<a href="https://pytorch.org/vision/0.9/">https://pytorch.org/vision/0.9/</a></p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240729%E4%B8%8B%E5%8D%8850535594.png" alt=""></p><h3 id="1-尝试使用"><a href="#1-尝试使用" class="headerlink" title="1. 尝试使用"></a>1. 尝试使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset_transforms = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor(),  <span class="comment"># 转为tensor类型</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, transform=dataset_transforms, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=dataset_transforms, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">img, target = test_set[0]</span></span><br><span class="line"><span class="string">print(img)  # &lt;PIL.Image.Image image mode=RGB size=32x32 at 0x14433FCA0&gt;</span></span><br><span class="line"><span class="string">print(target)  # 3</span></span><br><span class="line"><span class="string">print(test_set.classes[target])  # cat</span></span><br><span class="line"><span class="string">img.show()</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(log_dir=<span class="string">&#x27;./logs_cifar&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img, target = test_set[i]</span><br><span class="line">    writer.add_image(<span class="string">&#x27;test_set&#x27;</span>, img, i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h3 id="2-数据集的下载"><a href="#2-数据集的下载" class="headerlink" title="2. 数据集的下载"></a>2. 数据集的下载</h3><p>进入数据集的源码（CIFAR10），查看url即为下载链接</p><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240729%E4%B8%8B%E5%8D%8853613373.png" alt=""></p><h2 id="5-Dataloader"><a href="#5-Dataloader" class="headerlink" title="5 Dataloader"></a>5 Dataloader</h2><p>官方文档：<a href="https://pytorch.org/docs/1.8.1/data.html?highlight=dataloader#torch.utils.data.DataLoader">https://pytorch.org/docs/1.8.1/data.html?highlight=dataloader#torch.utils.data.DataLoader</a></p><h3 id="1-batch-size"><a href="#1-batch-size" class="headerlink" title="1. batch_size"></a>1. batch_size</h3><p><img src="https://blog-iskage.oss-cn-hangzhou.aliyuncs.com/images/image-20240730%E4%B8%8A%E5%8D%8894027575.png" alt=""></p><h3 id="2-shuffle"><a href="#2-shuffle" class="headerlink" title="2. shuffle"></a>2. shuffle</h3><p>shuffle打乱顺序</p><h3 id="3-代码"><a href="#3-代码" class="headerlink" title="3. 代码"></a>3. 代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset_transforms = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor(),  <span class="comment"># 转为tensor类型</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=dataset_transforms, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = DataLoader(dataset=test_set, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1. batch_size=64 每次取4个数据进行打包：test_set[0-63]=dataset[0-63]打包</span></span><br><span class="line"><span class="string">2. shuffle打乱顺序</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;dataloader&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        writer.add_images(<span class="string">&#x27;Epoch: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch), imgs, step)</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">利用 Pytorch 搭建神经网络：数据处理和下载</summary>
    
    
    
    <category term="Pytorch 入门" scheme="https://blog.iskage.online/categories/Pytorch-%E5%85%A5%E9%97%A8/"/>
    
    
    <category term="Pytorch" scheme="https://blog.iskage.online/tags/Pytorch/"/>
    
    <category term="神经网络" scheme="https://blog.iskage.online/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="深度学习" scheme="https://blog.iskage.online/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="AI" scheme="https://blog.iskage.online/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>微观经济学 笔记目录</title>
    <link href="https://blog.iskage.online/posts/73f9c55d.html"/>
    <id>https://blog.iskage.online/posts/73f9c55d.html</id>
    <published>2025-01-21T06:18:00.000Z</published>
    <updated>2025-01-20T08:10:36.076Z</updated>
    
    <content type="html"><![CDATA[<div class="note info flat"><p><strong>文章为博主根据复旦大学本科课程《微观经济学》整理的课程笔记。点击链接即可前往对应文章查看，笔记以pdf的格式展示。更多信息，可前往本人的<a href="https://github.com/isKage/iskage.github.io">Github库</a>中查询。或者直接右键下载。</strong></p></div><div class="note danger flat"><p><strong>转载请注明出处，要求见文末</strong></p></div><p>课程书籍为罗伯特·S·平狄克/鲁宾菲尔德编写，中国人民大学出版社出版的<a href="https://book.douban.com/subject/4039368/">《微观经济学》</a></p><div class="note no-icon flat"><p><strong>笔记目录</strong></p><p><a href="./b1fd752e.html">1 绪论</a></p><p><a href="./ac04a21d.html">2 供给和需求的基本原理</a></p><p><a href="./d039b3c8.html">3 消费者行为</a></p><p><a href="./cd431d80.html">4 个别需求和市场需求</a></p><p><a href="./7bee8e99.html">5 生产</a></p><p><a href="./c4281815.html">6 生产成本</a></p><p><a href="./cfb40b0.html">7 利润的最大化和竞争性供给</a></p><p><a href="./3aba38be.html">8 竞争性市场分析</a></p><p><a href="./c5b9c11f.html">9 市场势力：垄断和买方垄断</a></p><p><a href="./43074da4.html">10 有市场势力的定价</a></p><p><a href="./66ca6135.html">11 垄断竞争和寡头垄断</a></p></div>]]></content>
    
    
    <summary type="html">复旦大学《微观经济学》课程笔记，已更新完全</summary>
    
    
    
    <category term="微观经济学" scheme="https://blog.iskage.online/categories/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
    
    <category term="微观经济学" scheme="https://blog.iskage.online/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>微观经济学(11)垄断竞争和寡头垄断 (完)</title>
    <link href="https://blog.iskage.online/posts/66ca6135.html"/>
    <id>https://blog.iskage.online/posts/66ca6135.html</id>
    <published>2025-01-20T07:27:40.000Z</published>
    <updated>2025-01-20T08:10:45.160Z</updated>
    
    <content type="html"><![CDATA[<div class="note success flat"><p><strong>返回<a href="./73f9c55d.html">笔记目录</a></strong></p></div><div class="note info flat"><p>因为文件数据存储在<strong>中国香港地区</strong>，所以加载需要少量时间。如长时间无法加载，可考虑使用<s>VPN代理</s>。</p></div>  <div id="pdf-container" style="position: relative;">    <iframe id="pdf-frame" src="https://oss.iskage.online/pdfs/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6%E7%AC%94%E8%AE%B0/Chapter%2012%20Monopolistic%20Competition%20and%20Oligopoly.pdf#toolbar=0" width="100%" height="600px" allowfullscreen></iframe>    <button id="fullscreen-btn" class="fullscreen-btn" style="position: absolute; top: 10px; right: 10px; z-index: 10;">全屏模式</button>  </div>  <script src="/js/pdf-viewer.js"></script>  ]]></content>
    
    
    <summary type="html">《微观经济学》（平狄克）课程第11讲：垄断竞争和寡头垄断 Chapter 12 Monopolistic Competition and Oligopoly</summary>
    
    
    
    <category term="微观经济学" scheme="https://blog.iskage.online/categories/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
    
    <category term="微观经济学" scheme="https://blog.iskage.online/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>微观经济学(10)有市场势力的定价</title>
    <link href="https://blog.iskage.online/posts/43074da4.html"/>
    <id>https://blog.iskage.online/posts/43074da4.html</id>
    <published>2025-01-20T07:27:31.000Z</published>
    <updated>2025-01-20T08:09:18.936Z</updated>
    
    <content type="html"><![CDATA[<div class="note success flat"><p><strong>返回<a href="./73f9c55d.html">笔记目录</a></strong></p></div><div class="note info flat"><p>因为文件数据存储在<strong>中国香港地区</strong>，所以加载需要少量时间。如长时间无法加载，可考虑使用<s>VPN代理</s>。</p></div>  <div id="pdf-container" style="position: relative;">    <iframe id="pdf-frame" src="https://oss.iskage.online/pdfs/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6%E7%AC%94%E8%AE%B0/Chapter%2011%20Pricing%20with%20Market%20Power.pdf#toolbar=0" width="100%" height="600px" allowfullscreen></iframe>    <button id="fullscreen-btn" class="fullscreen-btn" style="position: absolute; top: 10px; right: 10px; z-index: 10;">全屏模式</button>  </div>  <script src="/js/pdf-viewer.js"></script>  ]]></content>
    
    
    <summary type="html">《微观经济学》（平狄克）课程第10讲：有市场势力的定价 Chapter 11 Pricing with Market Power</summary>
    
    
    
    <category term="微观经济学" scheme="https://blog.iskage.online/categories/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
    
    <category term="微观经济学" scheme="https://blog.iskage.online/tags/%E5%BE%AE%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
  </entry>
  
</feed>
